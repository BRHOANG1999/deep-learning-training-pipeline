{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐍 Python & NumPy Fundamentals for Neuroscience\n",
    "\n",
    "Welcome back, neural network explorer! 🧠⚡\n",
    "\n",
    "Before we dive into the exciting world of brain signal analysis, let's make sure you're comfortable with the core tools you'll be using every day. This notebook is designed as a **refresher** - not a complete introduction to programming.\n",
    "\n",
    "## What We'll Cover Today 🎯\n",
    "\n",
    "1. **Python Essentials**: The language constructs you'll use constantly\n",
    "2. **NumPy Fundamentals**: The backbone of scientific computing\n",
    "3. **Data Structures**: Arrays, lists, and when to use each\n",
    "4. **Mathematical Operations**: Linear algebra for neural networks\n",
    "5. **Brain Signal Examples**: Real-world applications of what you're learning\n",
    "\n",
    "## 💡 Why This Matters\n",
    "\n",
    "In neuroscience, you'll constantly work with:\n",
    "- **Multi-dimensional arrays** (time × channels × trials)\n",
    "- **Mathematical operations** (filtering, Fourier transforms, matrix multiplication)\n",
    "- **Data manipulation** (selecting time windows, averaging across trials)\n",
    "- **Efficient computation** (vectorized operations for speed)\n",
    "\n",
    "Master these fundamentals, and you'll breeze through the advanced topics!\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to refresh your Python skills? Let's go! 🚀*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ Setting Up Our Environment\n",
    "\n",
    "First, let's import the libraries we'll be using and set up some nice defaults for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for today's session\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for nice plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# NumPy settings for cleaner output\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"✅ Environment ready!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐍 Python Essentials: Quick Review\n",
    "\n",
    "Let's quickly review the Python constructs you'll use most often in neuroscience computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions: Your Secret Weapon 🔧\n",
    "\n",
    "List comprehensions are incredibly useful for data processing. In neuroscience, you'll often need to apply operations across multiple files, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing multiple EEG channels\n",
    "channel_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']\n",
    "\n",
    "# Traditional approach\n",
    "frontal_channels = []\n",
    "for channel in channel_names:\n",
    "    if channel.startswith('F'):\n",
    "        frontal_channels.append(channel)\n",
    "        \n",
    "print(\"Traditional approach:\", frontal_channels)\n",
    "\n",
    "# List comprehension (much cleaner!)\n",
    "frontal_channels_lc = [ch for ch in channel_names if ch.startswith('F')]\n",
    "print(\"List comprehension:\", frontal_channels_lc)\n",
    "\n",
    "# More complex example: Extract channel indices\n",
    "frontal_indices = [i for i, ch in enumerate(channel_names) if ch.startswith('F')]\n",
    "print(\"Frontal channel indices:\", frontal_indices)\n",
    "\n",
    "# With transformation\n",
    "channel_pairs = [(ch, f\"{ch}_processed\") for ch in channel_names[:3]]\n",
    "print(\"Channel pairs:\", channel_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Building Reusable Analysis Tools 🔨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_spectrum(signal_data, sampling_rate=250, freq_bands=None):\n",
    "    \"\"\"\n",
    "    Calculate power spectrum for EEG-like data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal_data : array-like\n",
    "        Time series data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency in Hz\n",
    "    freq_bands : dict, optional\n",
    "        Dictionary of frequency bands to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    power : array\n",
    "        Power spectral density\n",
    "    \"\"\"\n",
    "    if freq_bands is None:\n",
    "        freq_bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30)\n",
    "        }\n",
    "    \n",
    "    # Calculate power spectrum using Welch's method\n",
    "    frequencies, power = signal.welch(signal_data, sampling_rate, nperseg=sampling_rate*2)\n",
    "    \n",
    "    # Calculate band powers\n",
    "    band_powers = {}\n",
    "    for band, (low, high) in freq_bands.items():\n",
    "        band_mask = (frequencies >= low) & (frequencies <= high)\n",
    "        band_powers[band] = np.trapz(power[band_mask], frequencies[band_mask])\n",
    "    \n",
    "    return frequencies, power, band_powers\n",
    "\n",
    "# Test our function\n",
    "test_signal = np.random.randn(1000) + 2*np.sin(2*np.pi*10*np.linspace(0, 4, 1000))\n",
    "freqs, psd, bands = calculate_power_spectrum(test_signal)\n",
    "\n",
    "print(\"Band powers:\")\n",
    "for band, power in bands.items():\n",
    "    print(f\"  {band}: {power:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes: Organizing Complex Analysis Pipelines 🏗️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGProcessor:\n",
    "    \"\"\"A simple EEG processing class to demonstrate object-oriented concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_rate=250, channels=None):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.channels = channels or ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4']\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        \"\"\"Load EEG data (channels × time_points).\"\"\"\n",
    "        self.raw_data = np.array(data)\n",
    "        print(f\"Data loaded: {self.raw_data.shape} (channels × time_points)\")\n",
    "        \n",
    "    def apply_bandpass_filter(self, low_freq=1, high_freq=40):\n",
    "        \"\"\"Apply a bandpass filter to remove noise.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded! Use load_data() first.\")\n",
    "            \n",
    "        # Design butterworth filter\n",
    "        sos = signal.butter(4, [low_freq, high_freq], \n",
    "                           btype='bandpass', fs=self.sampling_rate, output='sos')\n",
    "        \n",
    "        # Apply filter to each channel\n",
    "        self.processed_data = np.zeros_like(self.raw_data)\n",
    "        for i, channel_data in enumerate(self.raw_data):\n",
    "            self.processed_data[i] = signal.sosfilt(sos, channel_data)\n",
    "            \n",
    "        print(f\"✅ Bandpass filter applied ({low_freq}-{high_freq} Hz)\")\n",
    "        \n",
    "    def plot_channel(self, channel_idx=0, duration=2.0):\n",
    "        \"\"\"Plot raw vs processed data for a specific channel.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded!\")\n",
    "            \n",
    "        # Time axis\n",
    "        n_samples = int(duration * self.sampling_rate)\n",
    "        time_axis = np.linspace(0, duration, n_samples)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot raw data\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(time_axis, self.raw_data[channel_idx, :n_samples], 'b-', alpha=0.7)\n",
    "        plt.title(f'Raw EEG - Channel {self.channels[channel_idx]}')\n",
    "        plt.ylabel('Amplitude (μV)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot processed data (if available)\n",
    "        if self.processed_data is not None:\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(time_axis, self.processed_data[channel_idx, :n_samples], 'r-', alpha=0.7)\n",
    "            plt.title(f'Filtered EEG - Channel {self.channels[channel_idx]}')\n",
    "            plt.ylabel('Amplitude (μV)')\n",
    "            plt.xlabel('Time (seconds)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demo the class\n",
    "processor = EEGProcessor(sampling_rate=250)\n",
    "\n",
    "# Simulate some EEG data with noise\n",
    "time_points = np.linspace(0, 10, 2500)  # 10 seconds at 250 Hz\n",
    "n_channels = len(processor.channels)\n",
    "\n",
    "# Create realistic EEG-like signals\n",
    "fake_eeg = np.zeros((n_channels, len(time_points)))\n",
    "for i in range(n_channels):\n",
    "    # Alpha waves (10 Hz) + noise + some artifacts\n",
    "    alpha_wave = 2 * np.sin(2 * np.pi * 10 * time_points + i * 0.5)\n",
    "    theta_wave = 1 * np.sin(2 * np.pi * 6 * time_points + i * 0.3)\n",
    "    noise = 0.5 * np.random.randn(len(time_points))\n",
    "    artifacts = 10 * np.sin(2 * np.pi * 60 * time_points)  # 60 Hz line noise\n",
    "    \n",
    "    fake_eeg[i] = alpha_wave + theta_wave + noise + artifacts\n",
    "\n",
    "# Process the data\n",
    "processor.load_data(fake_eeg)\n",
    "processor.apply_bandpass_filter(low_freq=1, high_freq=40)\n",
    "processor.plot_channel(channel_idx=0, duration=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔢 NumPy Fundamentals: The Heart of Scientific Computing\n",
    "\n",
    "NumPy is the foundation of scientific computing in Python. In neuroscience, you'll use it for everything from basic array operations to complex signal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Creation and Basic Operations 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to create arrays (common in neuroscience)\n",
    "\n",
    "# 1. Time axis for signals\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 2.0  # seconds\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "print(f\"Time axis: {time[:5]}...{time[-5:]}\")\n",
    "print(f\"Shape: {time.shape}\")\n",
    "\n",
    "# 2. Initialize arrays for data storage\n",
    "n_channels = 64\n",
    "n_timepoints = 1000\n",
    "n_trials = 100\n",
    "\n",
    "# Empty array for EEG data (channels × time × trials)\n",
    "eeg_data = np.zeros((n_channels, n_timepoints, n_trials))\n",
    "print(f\"\\nEEG data array shape: {eeg_data.shape}\")\n",
    "\n",
    "# Random data (useful for testing)\n",
    "random_signals = np.random.randn(n_channels, n_timepoints)\n",
    "print(f\"Random signals shape: {random_signals.shape}\")\n",
    "\n",
    "# 3. Structured arrays for experiments\n",
    "trial_conditions = np.array(['rest', 'task', 'rest', 'task'] * 25)\n",
    "print(f\"\\nTrial conditions (first 10): {trial_conditions[:10]}\")\n",
    "\n",
    "# 4. Frequency arrays for spectral analysis\n",
    "frequencies = np.fft.fftfreq(n_timepoints, 1/sampling_rate)\n",
    "positive_freqs = frequencies[:n_timepoints//2]\n",
    "print(f\"\\nFrequency range: {positive_freqs[0]:.1f} to {positive_freqs[-1]:.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Indexing and Slicing: Extracting What You Need 🎯\n",
    "\n",
    "In neuroscience, you'll constantly need to extract specific time windows, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample EEG data (channels × time × trials)\n",
    "np.random.seed(42)  # For reproducible results\n",
    "n_channels, n_timepoints, n_trials = 10, 1000, 50\n",
    "eeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n",
    "\n",
    "# Add some structure to make it more realistic\n",
    "time_axis = np.linspace(0, 4, n_timepoints)  # 4 seconds\n",
    "for ch in range(n_channels):\n",
    "    for trial in range(n_trials):\n",
    "        # Add alpha waves (10 Hz) with some variability\n",
    "        alpha_freq = 10 + np.random.normal(0, 1)\n",
    "        eeg_data[ch, :, trial] += 2 * np.sin(2 * np.pi * alpha_freq * time_axis)\n",
    "\n",
    "print(f\"EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Data type: {eeg_data.dtype}\")\n",
    "print(f\"Memory usage: {eeg_data.nbytes / 1024:.1f} KB\")\n",
    "\n",
    "# Common indexing operations\n",
    "print(\"\\n=== Common Indexing Operations ===\")\n",
    "\n",
    "# 1. Select specific channels\n",
    "frontal_channels = [0, 1, 2]  # Assuming these are frontal\n",
    "frontal_data = eeg_data[frontal_channels, :, :]\n",
    "print(f\"1. Frontal channels data shape: {frontal_data.shape}\")\n",
    "\n",
    "# 2. Select time window (e.g., 1-3 seconds)\n",
    "start_time, end_time = 1.0, 3.0\n",
    "start_idx = int(start_time * (n_timepoints / 4))  # 4 seconds total\n",
    "end_idx = int(end_time * (n_timepoints / 4))\n",
    "time_window = eeg_data[:, start_idx:end_idx, :]\n",
    "print(f\"2. Time window (1-3s) shape: {time_window.shape}\")\n",
    "\n",
    "# 3. Select specific trials\n",
    "task_trials = [1, 3, 5, 7, 9]  # Assuming these are task trials\n",
    "task_data = eeg_data[:, :, task_trials]\n",
    "print(f\"3. Task trials data shape: {task_data.shape}\")\n",
    "\n",
    "# 4. Complex indexing: frontal channels, specific time window, task trials\n",
    "subset = eeg_data[frontal_channels, start_idx:end_idx, task_trials]\n",
    "print(f\"4. Complex subset shape: {subset.shape}\")\n",
    "\n",
    "# 5. Boolean indexing (very powerful!)\n",
    "# Find time points where signal is above threshold\n",
    "channel_0_trial_0 = eeg_data[0, :, 0]\n",
    "high_amplitude_mask = np.abs(channel_0_trial_0) > 2\n",
    "high_amplitude_points = channel_0_trial_0[high_amplitude_mask]\n",
    "print(f\"5. High amplitude points: {len(high_amplitude_points)} out of {n_timepoints}\")\n",
    "\n",
    "# 6. Advanced: condition-based selection\n",
    "# Get trials where the mean amplitude is above median\n",
    "trial_means = np.mean(eeg_data, axis=(0, 1))  # Mean across channels and time\n",
    "high_activity_trials = trial_means > np.median(trial_means)\n",
    "high_activity_data = eeg_data[:, :, high_activity_trials]\n",
    "print(f\"6. High activity trials: {high_activity_data.shape[2]} out of {n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Operations: The Power of Vectorization ⚡\n",
    "\n",
    "Vectorized operations are much faster than Python loops. This is crucial when processing large neuroscience datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare vectorized vs loop operations\n",
    "n_samples = 100000\n",
    "data = np.random.randn(n_samples)\n",
    "\n",
    "# Method 1: Python loop (slow)\n",
    "start_time = time.time()\n",
    "result_loop = []\n",
    "for value in data:\n",
    "    result_loop.append(value ** 2 + 2 * value + 1)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Vectorized operations (fast)\n",
    "start_time = time.time()\n",
    "result_vectorized = data ** 2 + 2 * data + 1\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop method: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized method: {vectorized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {loop_time/vectorized_time:.1f}x faster!\")\n",
    "\n",
    "# Verify they give the same result\n",
    "print(f\"Results match: {np.allclose(result_loop, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Operations for Neuroscience 🧮\n",
    "\n",
    "Let's explore the mathematical operations you'll use most frequently in brain signal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample multi-channel EEG data\n",
    "np.random.seed(123)\n",
    "n_channels, n_timepoints, n_trials = 8, 500, 20\n",
    "eeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n",
    "\n",
    "# Add realistic signal structure\n",
    "time = np.linspace(0, 2, n_timepoints)  # 2 seconds\n",
    "for ch in range(n_channels):\n",
    "    for trial in range(n_trials):\n",
    "        # Add alpha rhythm (10 Hz) and some theta (6 Hz)\n",
    "        alpha = 3 * np.sin(2 * np.pi * 10 * time + ch * 0.2)\n",
    "        theta = 1.5 * np.sin(2 * np.pi * 6 * time + ch * 0.1)\n",
    "        eeg_data[ch, :, trial] += alpha + theta\n",
    "\n",
    "print(f\"Sample EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Data range: {eeg_data.min():.2f} to {eeg_data.max():.2f}\")\n",
    "\n",
    "# === Statistical Operations ===\n",
    "print(\"\\n=== Statistical Operations ===\")\n",
    "\n",
    "# 1. Basic statistics across different axes\n",
    "trial_averages = np.mean(eeg_data, axis=2)  # Average across trials\n",
    "channel_averages = np.mean(eeg_data, axis=0)  # Average across channels\n",
    "time_averages = np.mean(eeg_data, axis=1)  # Average across time\n",
    "\n",
    "print(f\"Trial averages shape: {trial_averages.shape}\")\n",
    "print(f\"Channel averages shape: {channel_averages.shape}\")\n",
    "print(f\"Time averages shape: {time_averages.shape}\")\n",
    "\n",
    "# 2. Standard deviation and variance\n",
    "trial_std = np.std(eeg_data, axis=2)\n",
    "print(f\"\\nStandard deviation across trials: {trial_std.mean():.3f}\")\n",
    "\n",
    "# 3. Percentiles (useful for outlier detection)\n",
    "percentiles = np.percentile(eeg_data, [5, 25, 50, 75, 95])\n",
    "print(f\"Data percentiles: {percentiles}\")\n",
    "\n",
    "# === Signal Processing Operations ===\n",
    "print(\"\\n=== Signal Processing Operations ===\")\n",
    "\n",
    "# 1. Z-score normalization (common preprocessing step)\n",
    "eeg_normalized = (eeg_data - np.mean(eeg_data, axis=1, keepdims=True)) / np.std(eeg_data, axis=1, keepdims=True)\n",
    "print(f\"Normalized data mean: {np.mean(eeg_normalized):.6f}\")\n",
    "print(f\"Normalized data std: {np.std(eeg_normalized):.6f}\")\n",
    "\n",
    "# 2. Baseline correction (subtract pre-stimulus period)\n",
    "baseline_period = slice(0, 50)  # First 50 time points\n",
    "baseline_mean = np.mean(eeg_data[:, baseline_period, :], axis=1, keepdims=True)\n",
    "eeg_baseline_corrected = eeg_data - baseline_mean\n",
    "print(f\"Baseline corrected data range: {eeg_baseline_corrected.min():.2f} to {eeg_baseline_corrected.max():.2f}\")\n",
    "\n",
    "# 3. Root Mean Square (RMS) - measure of signal power\n",
    "rms_values = np.sqrt(np.mean(eeg_data**2, axis=1))\n",
    "print(f\"RMS values shape: {rms_values.shape}\")\n",
    "print(f\"Average RMS across channels: {np.mean(rms_values):.3f}\")\n",
    "\n",
    "# === Advanced Operations ===\n",
    "print(\"\\n=== Advanced Operations ===\")\n",
    "\n",
    "# 1. Correlation between channels\n",
    "# Take first trial, correlate channels across time\n",
    "channel_correlations = np.corrcoef(eeg_data[:, :, 0])\n",
    "print(f\"Channel correlation matrix shape: {channel_correlations.shape}\")\n",
    "print(f\"Average correlation: {np.mean(channel_correlations[np.triu_indices(n_channels, k=1)]):.3f}\")\n",
    "\n",
    "# 2. Covariance matrix\n",
    "cov_matrix = np.cov(eeg_data[:, :, 0])\n",
    "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
    "\n",
    "# 3. Principal Component Analysis (PCA) preparation\n",
    "# Center the data\n",
    "data_centered = eeg_data - np.mean(eeg_data, axis=1, keepdims=True)\n",
    "# Compute covariance matrix\n",
    "cov_temporal = np.cov(data_centered[:, :, 0].T)  # Time x Time covariance\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_temporal)\n",
    "print(f\"Top 5 eigenvalues: {np.sort(eigenvalues)[-5:][::-1]}\")\n",
    "\n",
    "# 4. Sliding window analysis\n",
    "window_size = 50\n",
    "step_size = 10\n",
    "windows = []\n",
    "for start in range(0, n_timepoints - window_size, step_size):\n",
    "    window_data = eeg_data[:, start:start+window_size, :]\n",
    "    window_mean = np.mean(window_data, axis=1)  # Average across time in window\n",
    "    windows.append(window_mean)\n",
    "\n",
    "windowed_analysis = np.array(windows)\n",
    "print(f\"Windowed analysis shape: {windowed_analysis.shape}\")\n",
    "print(f\"Number of windows: {len(windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra for Neural Networks 🧠\n",
    "\n",
    "Understanding matrix operations is crucial for neural networks and many signal processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Matrix Operations Fundamental to Neural Networks ===\n",
    "print(\"=== Matrix Operations for Neural Networks ===\")\n",
    "\n",
    "# 1. Basic matrix multiplication (the heart of neural networks)\n",
    "# Simulate a simple neural network layer\n",
    "input_features = 64  # e.g., 64 EEG channels\n",
    "hidden_units = 32\n",
    "batch_size = 10\n",
    "\n",
    "# Input data (batch_size × input_features)\n",
    "X = np.random.randn(batch_size, input_features)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "\n",
    "# Weight matrix (input_features × hidden_units)\n",
    "W = np.random.randn(input_features, hidden_units) * 0.1  # Small random weights\n",
    "print(f\"Weight W shape: {W.shape}\")\n",
    "\n",
    "# Bias vector (hidden_units,)\n",
    "b = np.zeros(hidden_units)\n",
    "print(f\"Bias b shape: {b.shape}\")\n",
    "\n",
    "# Forward pass: linear transformation\n",
    "z = np.dot(X, W) + b  # or X @ W + b\n",
    "print(f\"Linear output z shape: {z.shape}\")\n",
    "\n",
    "# Activation function (ReLU)\n",
    "a = np.maximum(0, z)\n",
    "print(f\"Activated output a shape: {a.shape}\")\n",
    "print(f\"Activated units: {np.sum(a > 0)} out of {a.size}\")\n",
    "\n",
    "# 2. Batch operations (processing multiple samples simultaneously)\n",
    "print(\"\\n=== Batch Operations ===\")\n",
    "\n",
    "# Simulate processing multiple EEG epochs\n",
    "n_epochs = 100\n",
    "n_channels = 10\n",
    "n_timepoints = 250\n",
    "\n",
    "# Create batch of EEG data\n",
    "eeg_batch = np.random.randn(n_epochs, n_channels, n_timepoints)\n",
    "print(f\"EEG batch shape: {eeg_batch.shape}\")\n",
    "\n",
    "# Flatten each epoch for neural network input\n",
    "eeg_flattened = eeg_batch.reshape(n_epochs, -1)\n",
    "print(f\"Flattened EEG shape: {eeg_flattened.shape}\")\n",
    "\n",
    "# Apply transformation to entire batch\n",
    "feature_weights = np.random.randn(n_channels * n_timepoints, 50)\n",
    "features = np.dot(eeg_flattened, feature_weights)\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "# 3. Covariance and correlation matrices (important for connectivity analysis)\n",
    "print(\"\\n=== Covariance and Correlation ===\")\n",
    "\n",
    "# Simulate multi-channel EEG data\n",
    "n_channels = 5\n",
    "n_timepoints = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create correlated channels (simulate brain connectivity)\n",
    "base_signal = np.random.randn(n_timepoints)\n",
    "channels = np.zeros((n_channels, n_timepoints))\n",
    "\n",
    "for i in range(n_channels):\n",
    "    # Each channel is base signal + independent noise\n",
    "    channels[i] = 0.7 * base_signal + 0.3 * np.random.randn(n_timepoints)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(channels)\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
    "print(f\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Compute covariance matrix\n",
    "covariance_matrix = np.cov(channels)\n",
    "print(f\"\\nCovariance matrix shape: {covariance_matrix.shape}\")\n",
    "\n",
    "# 4. Eigendecomposition (used in PCA, ICA, etc.)\n",
    "print(\"\\n=== Eigendecomposition ===\")\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# Sort by eigenvalue magnitude\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"Sorted eigenvalues: {eigenvalues_sorted}\")\n",
    "print(f\"Explained variance ratio: {eigenvalues_sorted / np.sum(eigenvalues_sorted)}\")\n",
    "\n",
    "# 5. Matrix norms (useful for regularization)\n",
    "print(\"\\n=== Matrix Norms ===\")\n",
    "\n",
    "sample_matrix = np.random.randn(5, 5)\n",
    "print(f\"Frobenius norm: {np.linalg.norm(sample_matrix, 'fro'):.3f}\")\n",
    "print(f\"Nuclear norm: {np.linalg.norm(sample_matrix, 'nuc'):.3f}\")\n",
    "print(f\"Spectral norm: {np.linalg.norm(sample_matrix, 2):.3f}\")\n",
    "\n",
    "# 6. Solving linear systems (used in many algorithms)\n",
    "print(\"\\n=== Solving Linear Systems ===\")\n",
    "\n",
    "# Example: least squares solution\n",
    "A = np.random.randn(100, 10)  # Design matrix\n",
    "x_true = np.random.randn(10)  # True parameters\n",
    "y = A @ x_true + 0.1 * np.random.randn(100)  # Noisy observations\n",
    "\n",
    "# Solve least squares: x = (A^T A)^(-1) A^T y\n",
    "x_estimated = np.linalg.solve(A.T @ A, A.T @ y)\n",
    "print(f\"True parameters: {x_true[:5]}\")\n",
    "print(f\"Estimated parameters: {x_estimated[:5]}\")\n",
    "print(f\"Estimation error: {np.linalg.norm(x_true - x_estimated):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧬 Real-World Example: EEG Signal Analysis\n",
    "\n",
    "Let's put everything together with a realistic neuroscience example: analyzing EEG signals to detect different brain states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simulate Realistic EEG Data ===\n",
    "print(\"=== Simulating Realistic EEG Data ===\")\n",
    "\n",
    "# Parameters\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 10  # seconds\n",
    "n_channels = 8  # Simplified EEG montage\n",
    "n_trials = 60  # 30 eyes-closed, 30 eyes-open\n",
    "\n",
    "# Channel names (simplified 10-20 system)\n",
    "channel_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4']\n",
    "\n",
    "# Time axis\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "n_timepoints = len(time)\n",
    "\n",
    "# Initialize data array\n",
    "eeg_data = np.zeros((n_channels, n_timepoints, n_trials))\n",
    "conditions = ['eyes_closed'] * 30 + ['eyes_open'] * 30\n",
    "\n",
    "# Generate realistic EEG signals\n",
    "np.random.seed(42)\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    condition = conditions[trial]\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        # Base noise\n",
    "        signal = 0.5 * np.random.randn(n_timepoints)\n",
    "        \n",
    "        # Add physiological rhythms\n",
    "        if condition == 'eyes_closed':\n",
    "            # Strong alpha rhythm (8-12 Hz) especially in posterior channels\n",
    "            if 'P' in channel_names[ch]:  # Posterior channels\n",
    "                alpha_power = 4.0\n",
    "            else:\n",
    "                alpha_power = 2.0\n",
    "            alpha_freq = 10 + np.random.normal(0, 0.5)\n",
    "            signal += alpha_power * np.sin(2 * np.pi * alpha_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Some theta (4-8 Hz)\n",
    "            theta_freq = 6 + np.random.normal(0, 0.5)\n",
    "            signal += 1.0 * np.sin(2 * np.pi * theta_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "        else:  # eyes_open\n",
    "            # Reduced alpha, more beta activity (13-30 Hz)\n",
    "            alpha_power = 1.0\n",
    "            alpha_freq = 10 + np.random.normal(0, 0.5)\n",
    "            signal += alpha_power * np.sin(2 * np.pi * alpha_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Beta activity\n",
    "            beta_freq = 20 + np.random.normal(0, 2)\n",
    "            signal += 1.5 * np.sin(2 * np.pi * beta_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Higher frequency noise (more alertness)\n",
    "            signal += 0.3 * np.random.randn(n_timepoints)\n",
    "        \n",
    "        # Add some 60Hz line noise\n",
    "        signal += 0.1 * np.sin(2 * np.pi * 60 * time)\n",
    "        \n",
    "        # Store the signal\n",
    "        eeg_data[ch, :, trial] = signal\n",
    "\n",
    "print(f\"Generated EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Conditions: {len(set(conditions))} unique conditions\")\n",
    "print(f\"Data range: {eeg_data.min():.2f} to {eeg_data.max():.2f} μV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exploratory Data Analysis ===\n",
    "print(\"=== Exploratory Data Analysis ===\")\n",
    "\n",
    "# 1. Visualize sample trials\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot sample eyes-closed trial\n",
    "axes[0, 0].plot(time[:1000], eeg_data[6, :1000, 0])  # P3 channel, first 4 seconds\n",
    "axes[0, 0].set_title('Eyes Closed - P3 Channel')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude (μV)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot sample eyes-open trial\n",
    "axes[0, 1].plot(time[:1000], eeg_data[6, :1000, 35])  # P3 channel, eyes-open trial\n",
    "axes[0, 1].set_title('Eyes Open - P3 Channel')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Amplitude (μV)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Power spectral density comparison\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Compute PSD for both conditions\n",
    "freqs, psd_closed = welch(eeg_data[6, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "freqs, psd_open = welch(eeg_data[6, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "\n",
    "axes[1, 0].semilogy(freqs, psd_closed, label='Eyes Closed', alpha=0.8)\n",
    "axes[1, 0].semilogy(freqs, psd_open, label='Eyes Open', alpha=0.8)\n",
    "axes[1, 0].set_xlim(0, 40)\n",
    "axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 0].set_ylabel('PSD (μV²/Hz)')\n",
    "axes[1, 0].set_title('Power Spectral Density - P3 Channel')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Topographic map of alpha power\n",
    "alpha_band = (8, 12)\n",
    "alpha_indices = (freqs >= alpha_band[0]) & (freqs <= alpha_band[1])\n",
    "\n",
    "alpha_power_closed = np.zeros(n_channels)\n",
    "alpha_power_open = np.zeros(n_channels)\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    # Eyes closed\n",
    "    freqs_ch, psd_ch = welch(eeg_data[ch, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "    alpha_power_closed[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n",
    "    \n",
    "    # Eyes open\n",
    "    freqs_ch, psd_ch = welch(eeg_data[ch, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "    alpha_power_open[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n",
    "\n",
    "# Bar plot of alpha power by channel\n",
    "x = np.arange(n_channels)\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, alpha_power_closed, width, label='Eyes Closed', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, alpha_power_open, width, label='Eyes Open', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Channel')\n",
    "axes[1, 1].set_ylabel('Alpha Power (μV²)')\n",
    "axes[1, 1].set_title('Alpha Power by Channel (8-12 Hz)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(channel_names, rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Statistical Analysis ===\n",
    "print(\"\\n=== Statistical Analysis ===\")\n",
    "\n",
    "# Compare alpha power between conditions\n",
    "print(\"Alpha power comparison:\")\n",
    "print(f\"Eyes Closed - Mean: {alpha_power_closed.mean():.3f}, Std: {alpha_power_closed.std():.3f}\")\n",
    "print(f\"Eyes Open - Mean: {alpha_power_open.mean():.3f}, Std: {alpha_power_open.std():.3f}\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt(((alpha_power_closed.std()**2) + (alpha_power_open.std()**2)) / 2)\n",
    "cohens_d = (alpha_power_closed.mean() - alpha_power_open.mean()) / pooled_std\n",
    "print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "# Find channels with largest differences\n",
    "alpha_diff = alpha_power_closed - alpha_power_open\n",
    "max_diff_channel = np.argmax(alpha_diff)\n",
    "print(f\"\\nLargest alpha difference in channel: {channel_names[max_diff_channel]} ({alpha_diff[max_diff_channel]:.3f} μV²)\")\n",
    "\n",
    "# Correlation between channels\n",
    "print(\"\\n=== Channel Connectivity Analysis ===\")\n",
    "\n",
    "# Compute average correlation matrices for each condition\n",
    "corr_closed = np.zeros((n_channels, n_channels))\n",
    "corr_open = np.zeros((n_channels, n_channels))\n",
    "\n",
    "for trial in range(30):\n",
    "    corr_closed += np.corrcoef(eeg_data[:, :, trial])\n",
    "    corr_open += np.corrcoef(eeg_data[:, :, trial + 30])\n",
    "\n",
    "corr_closed /= 30\n",
    "corr_open /= 30\n",
    "\n",
    "# Plot correlation matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "im1 = axes[0].imshow(corr_closed, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Eyes Closed - Correlation Matrix')\n",
    "axes[0].set_xticks(range(n_channels))\n",
    "axes[0].set_yticks(range(n_channels))\n",
    "axes[0].set_xticklabels(channel_names, rotation=45)\n",
    "axes[0].set_yticklabels(channel_names)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(corr_open, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Eyes Open - Correlation Matrix')\n",
    "axes[1].set_xticks(range(n_channels))\n",
    "axes[1].set_yticks(range(n_channels))\n",
    "axes[1].set_xticklabels(channel_names, rotation=45)\n",
    "axes[1].set_yticklabels(channel_names)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Difference matrix\n",
    "corr_diff = corr_closed - corr_open\n",
    "im3 = axes[2].imshow(corr_diff, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
    "axes[2].set_title('Correlation Difference (Closed - Open)')\n",
    "axes[2].set_xticks(range(n_channels))\n",
    "axes[2].set_yticks(range(n_channels))\n",
    "axes[2].set_xticklabels(channel_names, rotation=45)\n",
    "axes[2].set_yticklabels(channel_names)\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "avg_corr_closed = np.mean(corr_closed[np.triu_indices(n_channels, k=1)])\n",
    "avg_corr_open = np.mean(corr_open[np.triu_indices(n_channels, k=1)])\n",
    "\n",
    "print(f\"Average correlation - Eyes Closed: {avg_corr_closed:.3f}\")\n",
    "print(f\"Average correlation - Eyes Open: {avg_corr_open:.3f}\")\n",
    "print(f\"Difference: {avg_corr_closed - avg_corr_open:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏋️ Practice Exercises: Test Your Skills!\n",
    "\n",
    "Time to put your NumPy skills to the test! Try these exercises that mirror real neuroscience challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Event-Related Potential (ERP) Analysis 🎯\n",
    "\n",
    "**Task**: Analyze event-related potentials by extracting epochs around stimulus events and computing averaged responses.\n",
    "\n",
    "**Background**: In EEG experiments, we often want to see how the brain responds to specific stimuli (like seeing a face or hearing a tone). We extract small time windows around each stimulus and average across trials to reveal the typical brain response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise 1: Event-Related Potential Analysis ===\n",
    "print(\"=== Exercise 1: ERP Analysis ===\")\n",
    "\n",
    "# Simulate continuous EEG data\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 120  # seconds (2 minutes)\n",
    "n_channels = 4\n",
    "channel_names = ['Fz', 'Cz', 'Pz', 'Oz']\n",
    "\n",
    "# Generate continuous EEG\n",
    "np.random.seed(123)\n",
    "time_continuous = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "continuous_eeg = np.random.randn(n_channels, len(time_continuous)) * 2\n",
    "\n",
    "# Add some background alpha rhythm\n",
    "for ch in range(n_channels):\n",
    "    continuous_eeg[ch] += 3 * np.sin(2 * np.pi * 10 * time_continuous + ch * 0.5)\n",
    "\n",
    "# Generate stimulus events (random times)\n",
    "n_events = 50\n",
    "event_times = np.sort(np.random.uniform(5, duration-5, n_events))  # Avoid edges\n",
    "event_samples = (event_times * sampling_rate).astype(int)\n",
    "\n",
    "# Add stimulus-locked responses to the data\n",
    "for event_sample in event_samples:\n",
    "    # Create a stereotypical ERP response\n",
    "    erp_time = np.linspace(0, 1, 250)  # 1 second response\n",
    "    \n",
    "    # N100 component (negative deflection at 100ms)\n",
    "    n100 = -5 * np.exp(-((erp_time - 0.1)**2) / (2 * 0.02**2))\n",
    "    \n",
    "    # P300 component (positive deflection at 300ms)\n",
    "    p300 = 8 * np.exp(-((erp_time - 0.3)**2) / (2 * 0.05**2))\n",
    "    \n",
    "    erp_response = n100 + p300\n",
    "    \n",
    "    # Add to each channel (with some variability)\n",
    "    for ch in range(n_channels):\n",
    "        if event_sample + 250 < continuous_eeg.shape[1]:  # Make sure we don't go out of bounds\n",
    "            # Different channels have different sensitivities\n",
    "            sensitivity = [0.5, 1.0, 1.2, 0.8][ch]  # Cz and Pz are most sensitive\n",
    "            continuous_eeg[ch, event_sample:event_sample+250] += sensitivity * erp_response\n",
    "\n",
    "print(f\"Continuous EEG shape: {continuous_eeg.shape}\")\n",
    "print(f\"Number of events: {n_events}\")\n",
    "print(f\"Event times (first 10): {event_times[:10]}\")\n",
    "\n",
    "# YOUR TASK: Complete the following functions\n",
    "\n",
    "def extract_epochs(continuous_data, event_samples, pre_stim=0.2, post_stim=0.8, sampling_rate=250):\n",
    "    \"\"\"\n",
    "    Extract epochs around stimulus events.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    continuous_data : array, shape (n_channels, n_timepoints)\n",
    "        Continuous EEG data\n",
    "    event_samples : array\n",
    "        Sample indices of stimulus events\n",
    "    pre_stim : float\n",
    "        Time before stimulus (seconds)\n",
    "    post_stim : float\n",
    "        Time after stimulus (seconds)\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs : array, shape (n_channels, n_timepoints_epoch, n_events)\n",
    "        Extracted epochs\n",
    "    epoch_times : array\n",
    "        Time axis for epochs (relative to stimulus)\n",
    "    \"\"\"\n",
    "    # TODO: Implement epoch extraction\n",
    "    pre_samples = int(pre_stim * sampling_rate)\n",
    "    post_samples = int(post_stim * sampling_rate)\n",
    "    epoch_length = pre_samples + post_samples\n",
    "    \n",
    "    n_channels = continuous_data.shape[0]\n",
    "    valid_events = []\n",
    "    epochs_list = []\n",
    "    \n",
    "    for event_sample in event_samples:\n",
    "        start_sample = event_sample - pre_samples\n",
    "        end_sample = event_sample + post_samples\n",
    "        \n",
    "        # Check if epoch is within data bounds\n",
    "        if start_sample >= 0 and end_sample < continuous_data.shape[1]:\n",
    "            epoch = continuous_data[:, start_sample:end_sample]\n",
    "            epochs_list.append(epoch)\n",
    "            valid_events.append(event_sample)\n",
    "    \n",
    "    epochs = np.stack(epochs_list, axis=2)\n",
    "    epoch_times = np.linspace(-pre_stim, post_stim, epoch_length)\n",
    "    \n",
    "    return epochs, epoch_times\n",
    "\n",
    "def baseline_correct_epochs(epochs, baseline_period, epoch_times):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "    baseline_period : tuple\n",
    "        (start_time, end_time) for baseline period\n",
    "    epoch_times : array\n",
    "        Time axis for epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_corrected : array\n",
    "        Baseline-corrected epochs\n",
    "    \"\"\"\n",
    "    # TODO: Implement baseline correction\n",
    "    baseline_mask = (epoch_times >= baseline_period[0]) & (epoch_times <= baseline_period[1])\n",
    "    baseline_mean = np.mean(epochs[:, baseline_mask, :], axis=1, keepdims=True)\n",
    "    epochs_corrected = epochs - baseline_mean\n",
    "    \n",
    "    return epochs_corrected\n",
    "\n",
    "def compute_erp(epochs):\n",
    "    \"\"\"\n",
    "    Compute event-related potential by averaging across trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    erp : array, shape (n_channels, n_timepoints)\n",
    "        Averaged ERP\n",
    "    erp_std : array, shape (n_channels, n_timepoints)\n",
    "        Standard deviation across trials\n",
    "    \"\"\"\n",
    "    # TODO: Implement ERP computation\n",
    "    erp = np.mean(epochs, axis=2)\n",
    "    erp_std = np.std(epochs, axis=2)\n",
    "    \n",
    "    return erp, erp_std\n",
    "\n",
    "# Test your implementation\n",
    "epochs, epoch_times = extract_epochs(continuous_eeg, event_samples, pre_stim=0.2, post_stim=0.8)\n",
    "epochs_corrected = baseline_correct_epochs(epochs, baseline_period=(-0.2, 0), epoch_times=epoch_times)\n",
    "erp, erp_std = compute_erp(epochs_corrected)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Epochs shape: {epochs.shape}\")\n",
    "print(f\"ERP shape: {erp.shape}\")\n",
    "print(f\"Epoch time range: {epoch_times[0]:.3f} to {epoch_times[-1]:.3f} seconds\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    plt.subplot(2, 2, ch + 1)\n",
    "    plt.plot(epoch_times, erp[ch], 'b-', linewidth=2, label='ERP')\n",
    "    plt.fill_between(epoch_times, erp[ch] - erp_std[ch], erp[ch] + erp_std[ch], \n",
    "                     alpha=0.3, color='blue', label='±1 SD')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='Stimulus')\n",
    "    plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude (μV)')\n",
    "    plt.title(f'ERP - Channel {channel_names[ch]}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if ch == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis questions for you to think about:\n",
    "print(\"\\n=== Analysis Questions ===\")\n",
    "print(\"1. Which channel shows the strongest P300 response?\")\n",
    "print(\"2. At what time does the N100 component peak?\")\n",
    "print(\"3. How does the signal-to-noise ratio compare across channels?\")\n",
    "\n",
    "# Find P300 peak\n",
    "p300_window = (epoch_times >= 0.2) & (epoch_times <= 0.4)\n",
    "p300_peaks = np.argmax(erp[:, p300_window], axis=1)\n",
    "p300_peak_times = epoch_times[p300_window][p300_peaks]\n",
    "\n",
    "print(f\"\\nP300 peak times by channel:\")\n",
    "for ch in range(n_channels):\n",
    "    print(f\"  {channel_names[ch]}: {p300_peak_times[ch]:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Spectral Analysis and Connectivity 🌊\n",
    "\n",
    "**Task**: Analyze the frequency content of neural signals and compute connectivity between brain regions.\n",
    "\n",
    "**Background**: The brain operates at different frequency bands (delta, theta, alpha, beta, gamma). Understanding how these frequencies change and how they're synchronized between brain regions gives us insights into brain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise 2: Spectral Analysis and Connectivity ===\n",
    "print(\"=== Exercise 2: Spectral Analysis and Connectivity ===\")\n",
    "\n",
    "# Generate sample data with known connectivity\n",
    "np.random.seed(456)\n",
    "sampling_rate = 500  # Hz\n",
    "duration = 60  # seconds\n",
    "n_channels = 6\n",
    "channel_names = ['F3', 'F4', 'C3', 'C4', 'P3', 'P4']\n",
    "\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "n_timepoints = len(time)\n",
    "\n",
    "# Create signals with known connectivity patterns\n",
    "signals = np.zeros((n_channels, n_timepoints))\n",
    "\n",
    "# Generate base oscillations\n",
    "alpha_source = np.sin(2 * np.pi * 10 * time)  # 10 Hz alpha\n",
    "beta_source = np.sin(2 * np.pi * 20 * time)   # 20 Hz beta\n",
    "gamma_source = np.sin(2 * np.pi * 40 * time)  # 40 Hz gamma\n",
    "\n",
    "# Add noise and connectivity patterns\n",
    "for ch in range(n_channels):\n",
    "    # Base noise\n",
    "    signals[ch] = 0.5 * np.random.randn(n_timepoints)\n",
    "    \n",
    "    # Add frequency-specific connectivity\n",
    "    if 'F' in channel_names[ch]:  # Frontal channels\n",
    "        signals[ch] += 2 * beta_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    elif 'C' in channel_names[ch]:  # Central channels\n",
    "        signals[ch] += 1.5 * alpha_source + 1 * beta_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    elif 'P' in channel_names[ch]:  # Posterior channels\n",
    "        signals[ch] += 3 * alpha_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    \n",
    "    # Add some gamma coupling\n",
    "    if ch % 2 == 0:  # Left hemisphere\n",
    "        signals[ch] += 0.8 * gamma_source\n",
    "    else:  # Right hemisphere\n",
    "        # Delayed gamma (simulate inter-hemispheric delay)\n",
    "        delay_samples = int(0.01 * sampling_rate)  # 10ms delay\n",
    "        delayed_gamma = np.zeros_like(gamma_source)\n",
    "        delayed_gamma[delay_samples:] = gamma_source[:-delay_samples]\n",
    "        signals[ch] += 0.8 * delayed_gamma\n",
    "\n",
    "print(f\"Generated signals shape: {signals.shape}\")\n",
    "print(f\"Duration: {duration} seconds\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "\n",
    "# YOUR TASK: Implement the following functions\n",
    "\n",
    "def compute_power_spectrum(data, sampling_rate, nperseg=None):\n",
    "    \"\"\"\n",
    "    Compute power spectral density for multi-channel data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array, shape (n_channels, n_timepoints)\n",
    "        Multi-channel signal data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "    nperseg : int, optional\n",
    "        Length of each segment for Welch's method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    psd : array, shape (n_channels, n_frequencies)\n",
    "        Power spectral density for each channel\n",
    "    \"\"\"\n",
    "    # TODO: Implement power spectrum computation\n",
    "    if nperseg is None:\n",
    "        nperseg = sampling_rate * 4  # 4 second windows\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Compute PSD for first channel to get frequency array\n",
    "    frequencies, psd_ch = signal.welch(data[0], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    # Initialize PSD array\n",
    "    psd = np.zeros((n_channels, len(frequencies)))\n",
    "    \n",
    "    # Compute PSD for each channel\n",
    "    for ch in range(n_channels):\n",
    "        frequencies, psd[ch] = signal.welch(data[ch], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    return frequencies, psd\n",
    "\n",
    "def compute_band_power(psd, frequencies, freq_bands):\n",
    "    \"\"\"\n",
    "    Compute power in specific frequency bands.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    psd : array, shape (n_channels, n_frequencies)\n",
    "        Power spectral density\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    freq_bands : dict\n",
    "        Dictionary of frequency bands {band_name: (low_freq, high_freq)}\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    band_powers : dict\n",
    "        Dictionary of band powers {band_name: array of powers per channel}\n",
    "    \"\"\"\n",
    "    # TODO: Implement band power computation\n",
    "    band_powers = {}\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "        # Create frequency mask\n",
    "        freq_mask = (frequencies >= low_freq) & (frequencies <= high_freq)\n",
    "        \n",
    "        # Compute power in band for each channel\n",
    "        band_power = np.zeros(psd.shape[0])\n",
    "        for ch in range(psd.shape[0]):\n",
    "            band_power[ch] = np.trapz(psd[ch, freq_mask], frequencies[freq_mask])\n",
    "        \n",
    "        band_powers[band_name] = band_power\n",
    "    \n",
    "    return band_powers\n",
    "\n",
    "def compute_coherence(data, sampling_rate, nperseg=None):\n",
    "    \"\"\"\n",
    "    Compute coherence between all pairs of channels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array, shape (n_channels, n_timepoints)\n",
    "        Multi-channel signal data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "    nperseg : int, optional\n",
    "        Length of each segment\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    coherence : array, shape (n_channels, n_channels, n_frequencies)\n",
    "        Coherence matrix\n",
    "    \"\"\"\n",
    "    # TODO: Implement coherence computation\n",
    "    if nperseg is None:\n",
    "        nperseg = sampling_rate * 4\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Compute coherence for first pair to get frequency array\n",
    "    frequencies, coh_temp = signal.coherence(data[0], data[1], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    # Initialize coherence array\n",
    "    coherence = np.zeros((n_channels, n_channels, len(frequencies)))\n",
    "    \n",
    "    # Compute coherence for all pairs\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i == j:\n",
    "                coherence[i, j, :] = 1.0  # Perfect coherence with self\n",
    "            else:\n",
    "                frequencies, coherence[i, j, :] = signal.coherence(data[i], data[j], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    return frequencies, coherence\n",
    "\n",
    "# Test your implementation\n",
    "frequencies, psd = compute_power_spectrum(signals, sampling_rate)\n",
    "\n",
    "freq_bands = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 80)\n",
    "}\n",
    "\n",
    "band_powers = compute_band_power(psd, frequencies, freq_bands)\n",
    "coh_frequencies, coherence = compute_coherence(signals, sampling_rate)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"PSD shape: {psd.shape}\")\n",
    "print(f\"Frequency range: {frequencies[0]:.1f} to {frequencies[-1]:.1f} Hz\")\n",
    "print(f\"Coherence shape: {coherence.shape}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Power spectra\n",
    "for ch in range(n_channels):\n",
    "    axes[0, 0].semilogy(frequencies, psd[ch], label=channel_names[ch], alpha=0.8)\n",
    "axes[0, 0].set_xlim(0, 60)\n",
    "axes[0, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[0, 0].set_ylabel('PSD (μV²/Hz)')\n",
    "axes[0, 0].set_title('Power Spectral Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Band powers\n",
    "band_names = list(freq_bands.keys())\n",
    "x_pos = np.arange(len(band_names))\n",
    "bar_width = 0.1\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    powers = [band_powers[band][ch] for band in band_names]\n",
    "    axes[0, 1].bar(x_pos + ch * bar_width, powers, bar_width, \n",
    "                   label=channel_names[ch], alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Frequency Band')\n",
    "axes[0, 1].set_ylabel('Power (μV²)')\n",
    "axes[0, 1].set_title('Band Powers')\n",
    "axes[0, 1].set_xticks(x_pos + bar_width * 2.5)\n",
    "axes[0, 1].set_xticklabels(band_names)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Coherence heatmap (average across frequencies)\n",
    "avg_coherence = np.mean(coherence, axis=2)\n",
    "im = axes[0, 2].imshow(avg_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('Average Coherence Matrix')\n",
    "axes[0, 2].set_xticks(range(n_channels))\n",
    "axes[0, 2].set_yticks(range(n_channels))\n",
    "axes[0, 2].set_xticklabels(channel_names)\n",
    "axes[0, 2].set_yticklabels(channel_names)\n",
    "plt.colorbar(im, ax=axes[0, 2])\n",
    "\n",
    "# 4. Alpha coherence network\n",
    "alpha_idx = (coh_frequencies >= 8) & (coh_frequencies <= 13)\n",
    "alpha_coherence = np.mean(coherence[:, :, alpha_idx], axis=2)\n",
    "im2 = axes[1, 0].imshow(alpha_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('Alpha Band Coherence (8-13 Hz)')\n",
    "axes[1, 0].set_xticks(range(n_channels))\n",
    "axes[1, 0].set_yticks(range(n_channels))\n",
    "axes[1, 0].set_xticklabels(channel_names)\n",
    "axes[1, 0].set_yticklabels(channel_names)\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# 5. Beta coherence network\n",
    "beta_idx = (coh_frequencies >= 13) & (coh_frequencies <= 30)\n",
    "beta_coherence = np.mean(coherence[:, :, beta_idx], axis=2)\n",
    "im3 = axes[1, 1].imshow(beta_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Beta Band Coherence (13-30 Hz)')\n",
    "axes[1, 1].set_xticks(range(n_channels))\n",
    "axes[1, 1].set_yticks(range(n_channels))\n",
    "axes[1, 1].set_xticklabels(channel_names)\n",
    "axes[1, 1].set_yticklabels(channel_names)\n",
    "plt.colorbar(im3, ax=axes[1, 1])\n",
    "\n",
    "# 6. Coherence spectrum between F3 and F4\n",
    "f3_idx = channel_names.index('F3')\n",
    "f4_idx = channel_names.index('F4')\n",
    "axes[1, 2].plot(coh_frequencies, coherence[f3_idx, f4_idx, :], 'b-', linewidth=2)\n",
    "axes[1, 2].set_xlim(0, 60)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 2].set_ylabel('Coherence')\n",
    "axes[1, 2].set_title('F3-F4 Coherence Spectrum')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis summary\n",
    "print(\"\\n=== Analysis Summary ===\")\n",
    "print(\"Band power analysis:\")\n",
    "for band in band_names:\n",
    "    max_channel = np.argmax(band_powers[band])\n",
    "    print(f\"  {band}: strongest in {channel_names[max_channel]} ({band_powers[band][max_channel]:.3f} μV²)\")\n",
    "\n",
    "print(\"\\nCoherence analysis:\")\n",
    "# Find strongest coherence pairs (excluding self-connections)\n",
    "coherence_no_diag = avg_coherence.copy()\n",
    "np.fill_diagonal(coherence_no_diag, 0)\n",
    "max_coherence_idx = np.unravel_index(np.argmax(coherence_no_diag), coherence_no_diag.shape)\n",
    "print(f\"  Strongest coherence: {channel_names[max_coherence_idx[0]]}-{channel_names[max_coherence_idx[1]]} ({coherence_no_diag[max_coherence_idx]:.3f})\")\n",
    "\n",
    "# Interhemispheric coherence\n",
    "left_channels = [i for i, ch in enumerate(channel_names) if '3' in ch]\n",
    "right_channels = [i for i, ch in enumerate(channel_names) if '4' in ch]\n",
    "interhemispheric_coherence = []\n",
    "\n",
    "for l_ch in left_channels:\n",
    "    for r_ch in right_channels:\n",
    "        if channel_names[l_ch][0] == channel_names[r_ch][0]:  # Same region\n",
    "            coh_val = avg_coherence[l_ch, r_ch]\n",
    "            interhemispheric_coherence.append(coh_val)\n",
    "            print(f\"  {channel_names[l_ch]}-{channel_names[r_ch]} coherence: {coh_val:.3f}\")\n",
    "\n",
    "print(f\"  Average interhemispheric coherence: {np.mean(interhemispheric_coherence):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Key Takeaways: What You've Learned\n",
    "\n",
    "Congratulations! You've just completed a comprehensive review of Python and NumPy fundamentals for neuroscience. Here's what you've mastered:\n",
    "\n",
    "## 🐍 Python Skills\n",
    "- **List comprehensions** for efficient data processing\n",
    "- **Functions** for building reusable analysis tools\n",
    "- **Classes** for organizing complex analysis pipelines\n",
    "- **Best practices** for scientific computing\n",
    "\n",
    "## 🔢 NumPy Mastery\n",
    "- **Array creation and manipulation** for multi-dimensional brain data\n",
    "- **Indexing and slicing** to extract specific channels, time windows, and trials\n",
    "- **Vectorized operations** for lightning-fast computations\n",
    "- **Mathematical operations** essential for signal processing\n",
    "- **Linear algebra** foundations for neural networks\n",
    "\n",
    "## 🧠 Neuroscience Applications\n",
    "- **EEG data processing** with realistic multi-channel signals\n",
    "- **Event-related potential (ERP) analysis** for studying brain responses\n",
    "- **Spectral analysis** to understand frequency content\n",
    "- **Connectivity analysis** using coherence measures\n",
    "- **Statistical analysis** of neural data\n",
    "\n",
    "## 💡 Performance Tips You've Learned\n",
    "\n",
    "1. **Always use vectorized operations** instead of Python loops\n",
    "2. **Specify axis parameters** carefully when working with multi-dimensional data\n",
    "3. **Use keepdims=True** when you need to preserve array dimensions\n",
    "4. **Leverage NumPy's broadcasting** for efficient operations\n",
    "5. **Choose appropriate data types** to save memory\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "In the next notebook (`02_ml_foundations.ipynb`), you'll build on these fundamentals to explore:\n",
    "\n",
    "- **Machine learning concepts** relevant to neuroscience\n",
    "- **Classification and regression** for brain state detection\n",
    "- **Cross-validation** and model evaluation\n",
    "- **Feature engineering** for neural signals\n",
    "- **Preparing for deep learning** with PyTorch\n",
    "\n",
    "You're well-equipped for this journey! The NumPy skills you've practiced here will be your foundation for everything that comes next.\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue your deep learning journey? See you in the next notebook! 🧠⚡**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
