{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Python & NumPy Fundamentals for Neuroscience\n",
    "\n",
    "Welcome back, neural network explorer! ðŸ§ âš¡\n",
    "\n",
    "Before we dive into the exciting world of brain signal analysis, let's make sure you're comfortable with the core tools you'll be using every day. This notebook is designed as a **refresher** - not a complete introduction to programming.\n",
    "\n",
    "## What We'll Cover Today ðŸŽ¯\n",
    "\n",
    "1. **Python Essentials**: The language constructs you'll use constantly\n",
    "2. **NumPy Fundamentals**: The backbone of scientific computing\n",
    "3. **Data Structures**: Arrays, lists, and when to use each\n",
    "4. **Mathematical Operations**: Linear algebra for neural networks\n",
    "5. **Brain Signal Examples**: Real-world applications of what you're learning\n",
    "\n",
    "## ðŸ’¡ Why This Matters\n",
    "\n",
    "In neuroscience, you'll constantly work with:\n",
    "- **Multi-dimensional arrays** (time Ã— channels Ã— trials)\n",
    "- **Mathematical operations** (filtering, Fourier transforms, matrix multiplication)\n",
    "- **Data manipulation** (selecting time windows, averaging across trials)\n",
    "- **Efficient computation** (vectorized operations for speed)\n",
    "\n",
    "Master these fundamentals, and you'll breeze through the advanced topics!\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to refresh your Python skills? Let's go! ðŸš€*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Setting Up Our Environment\n",
    "\n",
    "First, let's import the libraries we'll be using and set up some nice defaults for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for today's session\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib for nice plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# NumPy settings for cleaner output\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"âœ… Environment ready!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Python Essentials: Quick Review\n",
    "\n",
    "Let's quickly review the Python constructs you'll use most often in neuroscience computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions: Your Secret Weapon ðŸ”§\n",
    "\n",
    "List comprehensions are incredibly useful for data processing. In neuroscience, you'll often need to apply operations across multiple files, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Processing multiple EEG channels\n",
    "channel_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']\n",
    "\n",
    "# Traditional approach\n",
    "frontal_channels = []\n",
    "for channel in channel_names:\n",
    "    if channel.startswith('F'):\n",
    "        frontal_channels.append(channel)\n",
    "        \n",
    "print(\"Traditional approach:\", frontal_channels)\n",
    "\n",
    "# List comprehension (much cleaner!)\n",
    "frontal_channels_lc = [ch for ch in channel_names if ch.startswith('F')]\n",
    "print(\"List comprehension:\", frontal_channels_lc)\n",
    "\n",
    "# More complex example: Extract channel indices\n",
    "frontal_indices = [i for i, ch in enumerate(channel_names) if ch.startswith('F')]\n",
    "print(\"Frontal channel indices:\", frontal_indices)\n",
    "\n",
    "# With transformation\n",
    "channel_pairs = [(ch, f\"{ch}_processed\") for ch in channel_names[:3]]\n",
    "print(\"Channel pairs:\", channel_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Building Reusable Analysis Tools ðŸ”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_spectrum(signal_data, sampling_rate=250, freq_bands=None):\n",
    "    \"\"\"\n",
    "    Calculate power spectrum for EEG-like data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal_data : array-like\n",
    "        Time series data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency in Hz\n",
    "    freq_bands : dict, optional\n",
    "        Dictionary of frequency bands to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    power : array\n",
    "        Power spectral density\n",
    "    \"\"\"\n",
    "    if freq_bands is None:\n",
    "        freq_bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30)\n",
    "        }\n",
    "    \n",
    "    # Calculate power spectrum using Welch's method\n",
    "    frequencies, power = signal.welch(signal_data, sampling_rate, nperseg=sampling_rate*2)\n",
    "    \n",
    "    # Calculate band powers\n",
    "    band_powers = {}\n",
    "    for band, (low, high) in freq_bands.items():\n",
    "        band_mask = (frequencies >= low) & (frequencies <= high)\n",
    "        band_powers[band] = np.trapz(power[band_mask], frequencies[band_mask])\n",
    "    \n",
    "    return frequencies, power, band_powers\n",
    "\n",
    "# Test our function\n",
    "test_signal = np.random.randn(1000) + 2*np.sin(2*np.pi*10*np.linspace(0, 4, 1000))\n",
    "freqs, psd, bands = calculate_power_spectrum(test_signal)\n",
    "\n",
    "print(\"Band powers:\")\n",
    "for band, power in bands.items():\n",
    "    print(f\"  {band}: {power:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes: Organizing Complex Analysis Pipelines ðŸ—ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGProcessor:\n",
    "    \"\"\"A simple EEG processing class to demonstrate object-oriented concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_rate=250, channels=None):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.channels = channels or ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4']\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        \"\"\"Load EEG data (channels Ã— time_points).\"\"\"\n",
    "        self.raw_data = np.array(data)\n",
    "        print(f\"Data loaded: {self.raw_data.shape} (channels Ã— time_points)\")\n",
    "        \n",
    "    def apply_bandpass_filter(self, low_freq=1, high_freq=40):\n",
    "        \"\"\"Apply a bandpass filter to remove noise.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded! Use load_data() first.\")\n",
    "            \n",
    "        # Design butterworth filter\n",
    "        sos = signal.butter(4, [low_freq, high_freq], \n",
    "                           btype='bandpass', fs=self.sampling_rate, output='sos')\n",
    "        \n",
    "        # Apply filter to each channel\n",
    "        self.processed_data = np.zeros_like(self.raw_data)\n",
    "        for i, channel_data in enumerate(self.raw_data):\n",
    "            self.processed_data[i] = signal.sosfilt(sos, channel_data)\n",
    "            \n",
    "        print(f\"âœ… Bandpass filter applied ({low_freq}-{high_freq} Hz)\")\n",
    "        \n",
    "    def plot_channel(self, channel_idx=0, duration=2.0):\n",
    "        \"\"\"Plot raw vs processed data for a specific channel.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded!\")\n",
    "            \n",
    "        # Time axis\n",
    "        n_samples = int(duration * self.sampling_rate)\n",
    "        time_axis = np.linspace(0, duration, n_samples)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot raw data\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(time_axis, self.raw_data[channel_idx, :n_samples], 'b-', alpha=0.7)\n",
    "        plt.title(f'Raw EEG - Channel {self.channels[channel_idx]}')\n",
    "        plt.ylabel('Amplitude (Î¼V)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot processed data (if available)\n",
    "        if self.processed_data is not None:\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(time_axis, self.processed_data[channel_idx, :n_samples], 'r-', alpha=0.7)\n",
    "            plt.title(f'Filtered EEG - Channel {self.channels[channel_idx]}')\n",
    "            plt.ylabel('Amplitude (Î¼V)')\n",
    "            plt.xlabel('Time (seconds)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demo the class\n",
    "processor = EEGProcessor(sampling_rate=250)\n",
    "\n",
    "# Simulate some EEG data with noise\n",
    "time_points = np.linspace(0, 10, 2500)  # 10 seconds at 250 Hz\n",
    "n_channels = len(processor.channels)\n",
    "\n",
    "# Create realistic EEG-like signals\n",
    "fake_eeg = np.zeros((n_channels, len(time_points)))\n",
    "for i in range(n_channels):\n",
    "    # Alpha waves (10 Hz) + noise + some artifacts\n",
    "    alpha_wave = 2 * np.sin(2 * np.pi * 10 * time_points + i * 0.5)\n",
    "    theta_wave = 1 * np.sin(2 * np.pi * 6 * time_points + i * 0.3)\n",
    "    noise = 0.5 * np.random.randn(len(time_points))\n",
    "    artifacts = 10 * np.sin(2 * np.pi * 60 * time_points)  # 60 Hz line noise\n",
    "    \n",
    "    fake_eeg[i] = alpha_wave + theta_wave + noise + artifacts\n",
    "\n",
    "# Process the data\n",
    "processor.load_data(fake_eeg)\n",
    "processor.apply_bandpass_filter(low_freq=1, high_freq=40)\n",
    "processor.plot_channel(channel_idx=0, duration=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¢ NumPy Fundamentals: The Heart of Scientific Computing\n",
    "\n",
    "NumPy is the foundation of scientific computing in Python. In neuroscience, you'll use it for everything from basic array operations to complex signal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Creation and Basic Operations ðŸŽ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to create arrays (common in neuroscience)\n",
    "\n",
    "# 1. Time axis for signals\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 2.0  # seconds\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "print(f\"Time axis: {time[:5]}...{time[-5:]}\")\n",
    "print(f\"Shape: {time.shape}\")\n",
    "\n",
    "# 2. Initialize arrays for data storage\n",
    "n_channels = 64\n",
    "n_timepoints = 1000\n",
    "n_trials = 100\n",
    "\n",
    "# Empty array for EEG data (channels Ã— time Ã— trials)\n",
    "eeg_data = np.zeros((n_channels, n_timepoints, n_trials))\n",
    "print(f\"\\nEEG data array shape: {eeg_data.shape}\")\n",
    "\n",
    "# Random data (useful for testing)\n",
    "random_signals = np.random.randn(n_channels, n_timepoints)\n",
    "print(f\"Random signals shape: {random_signals.shape}\")\n",
    "\n",
    "# 3. Structured arrays for experiments\n",
    "trial_conditions = np.array(['rest', 'task', 'rest', 'task'] * 25)\n",
    "print(f\"\\nTrial conditions (first 10): {trial_conditions[:10]}\")\n",
    "\n",
    "# 4. Frequency arrays for spectral analysis\n",
    "frequencies = np.fft.fftfreq(n_timepoints, 1/sampling_rate)\n",
    "positive_freqs = frequencies[:n_timepoints//2]\n",
    "print(f\"\\nFrequency range: {positive_freqs[0]:.1f} to {positive_freqs[-1]:.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Indexing and Slicing: Extracting What You Need ðŸŽ¯\n",
    "\n",
    "In neuroscience, you'll constantly need to extract specific time windows, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample EEG data (channels Ã— time Ã— trials)\n",
    "np.random.seed(42)  # For reproducible results\n",
    "n_channels, n_timepoints, n_trials = 10, 1000, 50\n",
    "eeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n",
    "\n",
    "# Add some structure to make it more realistic\n",
    "time_axis = np.linspace(0, 4, n_timepoints)  # 4 seconds\n",
    "for ch in range(n_channels):\n",
    "    for trial in range(n_trials):\n",
    "        # Add alpha waves (10 Hz) with some variability\n",
    "        alpha_freq = 10 + np.random.normal(0, 1)\n",
    "        eeg_data[ch, :, trial] += 2 * np.sin(2 * np.pi * alpha_freq * time_axis)\n",
    "\n",
    "print(f\"EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Data type: {eeg_data.dtype}\")\n",
    "print(f\"Memory usage: {eeg_data.nbytes / 1024:.1f} KB\")\n",
    "\n",
    "# Common indexing operations\n",
    "print(\"\\n=== Common Indexing Operations ===\")\n",
    "\n",
    "# 1. Select specific channels\n",
    "frontal_channels = [0, 1, 2]  # Assuming these are frontal\n",
    "frontal_data = eeg_data[frontal_channels, :, :]\n",
    "print(f\"1. Frontal channels data shape: {frontal_data.shape}\")\n",
    "\n",
    "# 2. Select time window (e.g., 1-3 seconds)\n",
    "start_time, end_time = 1.0, 3.0\n",
    "start_idx = int(start_time * (n_timepoints / 4))  # 4 seconds total\n",
    "end_idx = int(end_time * (n_timepoints / 4))\n",
    "time_window = eeg_data[:, start_idx:end_idx, :]\n",
    "print(f\"2. Time window (1-3s) shape: {time_window.shape}\")\n",
    "\n",
    "# 3. Select specific trials\n",
    "task_trials = [1, 3, 5, 7, 9]  # Assuming these are task trials\n",
    "task_data = eeg_data[:, :, task_trials]\n",
    "print(f\"3. Task trials data shape: {task_data.shape}\")\n",
    "\n",
    "# 4. Complex indexing: frontal channels, specific time window, task trials\n",
    "subset = eeg_data[frontal_channels, start_idx:end_idx, task_trials]\n",
    "print(f\"4. Complex subset shape: {subset.shape}\")\n",
    "\n",
    "# 5. Boolean indexing (very powerful!)\n",
    "# Find time points where signal is above threshold\n",
    "channel_0_trial_0 = eeg_data[0, :, 0]\n",
    "high_amplitude_mask = np.abs(channel_0_trial_0) > 2\n",
    "high_amplitude_points = channel_0_trial_0[high_amplitude_mask]\n",
    "print(f\"5. High amplitude points: {len(high_amplitude_points)} out of {n_timepoints}\")\n",
    "\n",
    "# 6. Advanced: condition-based selection\n",
    "# Get trials where the mean amplitude is above median\n",
    "trial_means = np.mean(eeg_data, axis=(0, 1))  # Mean across channels and time\n",
    "high_activity_trials = trial_means > np.median(trial_means)\n",
    "high_activity_data = eeg_data[:, :, high_activity_trials]\n",
    "print(f\"6. High activity trials: {high_activity_data.shape[2]} out of {n_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Operations: The Power of Vectorization âš¡\n",
    "\n",
    "Vectorized operations are much faster than Python loops. This is crucial when processing large neuroscience datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare vectorized vs loop operations\n",
    "n_samples = 100000\n",
    "data = np.random.randn(n_samples)\n",
    "\n",
    "# Method 1: Python loop (slow)\n",
    "start_time = time.time()\n",
    "result_loop = []\n",
    "for value in data:\n",
    "    result_loop.append(value ** 2 + 2 * value + 1)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Vectorized operations (fast)\n",
    "start_time = time.time()\n",
    "result_vectorized = data ** 2 + 2 * data + 1\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop method: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized method: {vectorized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {loop_time/vectorized_time:.1f}x faster!\")\n",
    "\n",
    "# Verify they give the same result\n",
    "print(f\"Results match: {np.allclose(result_loop, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Operations for Neuroscience ðŸ§®\n",
    "\n",
    "Let's explore the mathematical operations you'll use most frequently in brain signal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample multi-channel EEG data\n",
    "np.random.seed(123)\n",
    "n_channels, n_timepoints, n_trials = 8, 500, 20\n",
    "eeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n",
    "\n",
    "# Add realistic signal structure\n",
    "time = np.linspace(0, 2, n_timepoints)  # 2 seconds\n",
    "for ch in range(n_channels):\n",
    "    for trial in range(n_trials):\n",
    "        # Add alpha rhythm (10 Hz) and some theta (6 Hz)\n",
    "        alpha = 3 * np.sin(2 * np.pi * 10 * time + ch * 0.2)\n",
    "        theta = 1.5 * np.sin(2 * np.pi * 6 * time + ch * 0.1)\n",
    "        eeg_data[ch, :, trial] += alpha + theta\n",
    "\n",
    "print(f\"Sample EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Data range: {eeg_data.min():.2f} to {eeg_data.max():.2f}\")\n",
    "\n",
    "# === Statistical Operations ===\n",
    "print(\"\\n=== Statistical Operations ===\")\n",
    "\n",
    "# 1. Basic statistics across different axes\n",
    "trial_averages = np.mean(eeg_data, axis=2)  # Average across trials\n",
    "channel_averages = np.mean(eeg_data, axis=0)  # Average across channels\n",
    "time_averages = np.mean(eeg_data, axis=1)  # Average across time\n",
    "\n",
    "print(f\"Trial averages shape: {trial_averages.shape}\")\n",
    "print(f\"Channel averages shape: {channel_averages.shape}\")\n",
    "print(f\"Time averages shape: {time_averages.shape}\")\n",
    "\n",
    "# 2. Standard deviation and variance\n",
    "trial_std = np.std(eeg_data, axis=2)\n",
    "print(f\"\\nStandard deviation across trials: {trial_std.mean():.3f}\")\n",
    "\n",
    "# 3. Percentiles (useful for outlier detection)\n",
    "percentiles = np.percentile(eeg_data, [5, 25, 50, 75, 95])\n",
    "print(f\"Data percentiles: {percentiles}\")\n",
    "\n",
    "# === Signal Processing Operations ===\n",
    "print(\"\\n=== Signal Processing Operations ===\")\n",
    "\n",
    "# 1. Z-score normalization (common preprocessing step)\n",
    "eeg_normalized = (eeg_data - np.mean(eeg_data, axis=1, keepdims=True)) / np.std(eeg_data, axis=1, keepdims=True)\n",
    "print(f\"Normalized data mean: {np.mean(eeg_normalized):.6f}\")\n",
    "print(f\"Normalized data std: {np.std(eeg_normalized):.6f}\")\n",
    "\n",
    "# 2. Baseline correction (subtract pre-stimulus period)\n",
    "baseline_period = slice(0, 50)  # First 50 time points\n",
    "baseline_mean = np.mean(eeg_data[:, baseline_period, :], axis=1, keepdims=True)\n",
    "eeg_baseline_corrected = eeg_data - baseline_mean\n",
    "print(f\"Baseline corrected data range: {eeg_baseline_corrected.min():.2f} to {eeg_baseline_corrected.max():.2f}\")\n",
    "\n",
    "# 3. Root Mean Square (RMS) - measure of signal power\n",
    "rms_values = np.sqrt(np.mean(eeg_data**2, axis=1))\n",
    "print(f\"RMS values shape: {rms_values.shape}\")\n",
    "print(f\"Average RMS across channels: {np.mean(rms_values):.3f}\")\n",
    "\n",
    "# === Advanced Operations ===\n",
    "print(\"\\n=== Advanced Operations ===\")\n",
    "\n",
    "# 1. Correlation between channels\n",
    "# Take first trial, correlate channels across time\n",
    "channel_correlations = np.corrcoef(eeg_data[:, :, 0])\n",
    "print(f\"Channel correlation matrix shape: {channel_correlations.shape}\")\n",
    "print(f\"Average correlation: {np.mean(channel_correlations[np.triu_indices(n_channels, k=1)]):.3f}\")\n",
    "\n",
    "# 2. Covariance matrix\n",
    "cov_matrix = np.cov(eeg_data[:, :, 0])\n",
    "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
    "\n",
    "# 3. Principal Component Analysis (PCA) preparation\n",
    "# Center the data\n",
    "data_centered = eeg_data - np.mean(eeg_data, axis=1, keepdims=True)\n",
    "# Compute covariance matrix\n",
    "cov_temporal = np.cov(data_centered[:, :, 0].T)  # Time x Time covariance\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_temporal)\n",
    "print(f\"Top 5 eigenvalues: {np.sort(eigenvalues)[-5:][::-1]}\")\n",
    "\n",
    "# 4. Sliding window analysis\n",
    "window_size = 50\n",
    "step_size = 10\n",
    "windows = []\n",
    "for start in range(0, n_timepoints - window_size, step_size):\n",
    "    window_data = eeg_data[:, start:start+window_size, :]\n",
    "    window_mean = np.mean(window_data, axis=1)  # Average across time in window\n",
    "    windows.append(window_mean)\n",
    "\n",
    "windowed_analysis = np.array(windows)\n",
    "print(f\"Windowed analysis shape: {windowed_analysis.shape}\")\n",
    "print(f\"Number of windows: {len(windows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra for Neural Networks ðŸ§ \n",
    "\n",
    "Understanding matrix operations is crucial for neural networks and many signal processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Matrix Operations Fundamental to Neural Networks ===\n",
    "print(\"=== Matrix Operations for Neural Networks ===\")\n",
    "\n",
    "# 1. Basic matrix multiplication (the heart of neural networks)\n",
    "# Simulate a simple neural network layer\n",
    "input_features = 64  # e.g., 64 EEG channels\n",
    "hidden_units = 32\n",
    "batch_size = 10\n",
    "\n",
    "# Input data (batch_size Ã— input_features)\n",
    "X = np.random.randn(batch_size, input_features)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "\n",
    "# Weight matrix (input_features Ã— hidden_units)\n",
    "W = np.random.randn(input_features, hidden_units) * 0.1  # Small random weights\n",
    "print(f\"Weight W shape: {W.shape}\")\n",
    "\n",
    "# Bias vector (hidden_units,)\n",
    "b = np.zeros(hidden_units)\n",
    "print(f\"Bias b shape: {b.shape}\")\n",
    "\n",
    "# Forward pass: linear transformation\n",
    "z = np.dot(X, W) + b  # or X @ W + b\n",
    "print(f\"Linear output z shape: {z.shape}\")\n",
    "\n",
    "# Activation function (ReLU)\n",
    "a = np.maximum(0, z)\n",
    "print(f\"Activated output a shape: {a.shape}\")\n",
    "print(f\"Activated units: {np.sum(a > 0)} out of {a.size}\")\n",
    "\n",
    "# 2. Batch operations (processing multiple samples simultaneously)\n",
    "print(\"\\n=== Batch Operations ===\")\n",
    "\n",
    "# Simulate processing multiple EEG epochs\n",
    "n_epochs = 100\n",
    "n_channels = 10\n",
    "n_timepoints = 250\n",
    "\n",
    "# Create batch of EEG data\n",
    "eeg_batch = np.random.randn(n_epochs, n_channels, n_timepoints)\n",
    "print(f\"EEG batch shape: {eeg_batch.shape}\")\n",
    "\n",
    "# Flatten each epoch for neural network input\n",
    "eeg_flattened = eeg_batch.reshape(n_epochs, -1)\n",
    "print(f\"Flattened EEG shape: {eeg_flattened.shape}\")\n",
    "\n",
    "# Apply transformation to entire batch\n",
    "feature_weights = np.random.randn(n_channels * n_timepoints, 50)\n",
    "features = np.dot(eeg_flattened, feature_weights)\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "# 3. Covariance and correlation matrices (important for connectivity analysis)\n",
    "print(\"\\n=== Covariance and Correlation ===\")\n",
    "\n",
    "# Simulate multi-channel EEG data\n",
    "n_channels = 5\n",
    "n_timepoints = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create correlated channels (simulate brain connectivity)\n",
    "base_signal = np.random.randn(n_timepoints)\n",
    "channels = np.zeros((n_channels, n_timepoints))\n",
    "\n",
    "for i in range(n_channels):\n",
    "    # Each channel is base signal + independent noise\n",
    "    channels[i] = 0.7 * base_signal + 0.3 * np.random.randn(n_timepoints)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(channels)\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
    "print(f\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Compute covariance matrix\n",
    "covariance_matrix = np.cov(channels)\n",
    "print(f\"\\nCovariance matrix shape: {covariance_matrix.shape}\")\n",
    "\n",
    "# 4. Eigendecomposition (used in PCA, ICA, etc.)\n",
    "print(\"\\n=== Eigendecomposition ===\")\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# Sort by eigenvalue magnitude\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"Sorted eigenvalues: {eigenvalues_sorted}\")\n",
    "print(f\"Explained variance ratio: {eigenvalues_sorted / np.sum(eigenvalues_sorted)}\")\n",
    "\n",
    "# 5. Matrix norms (useful for regularization)\n",
    "print(\"\\n=== Matrix Norms ===\")\n",
    "\n",
    "sample_matrix = np.random.randn(5, 5)\n",
    "print(f\"Frobenius norm: {np.linalg.norm(sample_matrix, 'fro'):.3f}\")\n",
    "print(f\"Nuclear norm: {np.linalg.norm(sample_matrix, 'nuc'):.3f}\")\n",
    "print(f\"Spectral norm: {np.linalg.norm(sample_matrix, 2):.3f}\")\n",
    "\n",
    "# 6. Solving linear systems (used in many algorithms)\n",
    "print(\"\\n=== Solving Linear Systems ===\")\n",
    "\n",
    "# Example: least squares solution\n",
    "A = np.random.randn(100, 10)  # Design matrix\n",
    "x_true = np.random.randn(10)  # True parameters\n",
    "y = A @ x_true + 0.1 * np.random.randn(100)  # Noisy observations\n",
    "\n",
    "# Solve least squares: x = (A^T A)^(-1) A^T y\n",
    "x_estimated = np.linalg.solve(A.T @ A, A.T @ y)\n",
    "print(f\"True parameters: {x_true[:5]}\")\n",
    "print(f\"Estimated parameters: {x_estimated[:5]}\")\n",
    "print(f\"Estimation error: {np.linalg.norm(x_true - x_estimated):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¬ Real-World Example: EEG Signal Analysis\n",
    "\n",
    "Let's put everything together with a realistic neuroscience example: analyzing EEG signals to detect different brain states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simulate Realistic EEG Data ===\n",
    "print(\"=== Simulating Realistic EEG Data ===\")\n",
    "\n",
    "# Parameters\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 10  # seconds\n",
    "n_channels = 8  # Simplified EEG montage\n",
    "n_trials = 60  # 30 eyes-closed, 30 eyes-open\n",
    "\n",
    "# Channel names (simplified 10-20 system)\n",
    "channel_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4']\n",
    "\n",
    "# Time axis\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "n_timepoints = len(time)\n",
    "\n",
    "# Initialize data array\n",
    "eeg_data = np.zeros((n_channels, n_timepoints, n_trials))\n",
    "conditions = ['eyes_closed'] * 30 + ['eyes_open'] * 30\n",
    "\n",
    "# Generate realistic EEG signals\n",
    "np.random.seed(42)\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    condition = conditions[trial]\n",
    "    \n",
    "    for ch in range(n_channels):\n",
    "        # Base noise\n",
    "        signal = 0.5 * np.random.randn(n_timepoints)\n",
    "        \n",
    "        # Add physiological rhythms\n",
    "        if condition == 'eyes_closed':\n",
    "            # Strong alpha rhythm (8-12 Hz) especially in posterior channels\n",
    "            if 'P' in channel_names[ch]:  # Posterior channels\n",
    "                alpha_power = 4.0\n",
    "            else:\n",
    "                alpha_power = 2.0\n",
    "            alpha_freq = 10 + np.random.normal(0, 0.5)\n",
    "            signal += alpha_power * np.sin(2 * np.pi * alpha_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Some theta (4-8 Hz)\n",
    "            theta_freq = 6 + np.random.normal(0, 0.5)\n",
    "            signal += 1.0 * np.sin(2 * np.pi * theta_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "        else:  # eyes_open\n",
    "            # Reduced alpha, more beta activity (13-30 Hz)\n",
    "            alpha_power = 1.0\n",
    "            alpha_freq = 10 + np.random.normal(0, 0.5)\n",
    "            signal += alpha_power * np.sin(2 * np.pi * alpha_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Beta activity\n",
    "            beta_freq = 20 + np.random.normal(0, 2)\n",
    "            signal += 1.5 * np.sin(2 * np.pi * beta_freq * time + np.random.uniform(0, 2*np.pi))\n",
    "            \n",
    "            # Higher frequency noise (more alertness)\n",
    "            signal += 0.3 * np.random.randn(n_timepoints)\n",
    "        \n",
    "        # Add some 60Hz line noise\n",
    "        signal += 0.1 * np.sin(2 * np.pi * 60 * time)\n",
    "        \n",
    "        # Store the signal\n",
    "        eeg_data[ch, :, trial] = signal\n",
    "\n",
    "print(f\"Generated EEG data shape: {eeg_data.shape}\")\n",
    "print(f\"Conditions: {len(set(conditions))} unique conditions\")\n",
    "print(f\"Data range: {eeg_data.min():.2f} to {eeg_data.max():.2f} Î¼V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exploratory Data Analysis ===\n",
    "print(\"=== Exploratory Data Analysis ===\")\n",
    "\n",
    "# 1. Visualize sample trials\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot sample eyes-closed trial\n",
    "axes[0, 0].plot(time[:1000], eeg_data[6, :1000, 0])  # P3 channel, first 4 seconds\n",
    "axes[0, 0].set_title('Eyes Closed - P3 Channel')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude (Î¼V)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot sample eyes-open trial\n",
    "axes[0, 1].plot(time[:1000], eeg_data[6, :1000, 35])  # P3 channel, eyes-open trial\n",
    "axes[0, 1].set_title('Eyes Open - P3 Channel')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Amplitude (Î¼V)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Power spectral density comparison\n",
    "from scipy.signal import welch\n",
    "\n",
    "# Compute PSD for both conditions\n",
    "freqs, psd_closed = welch(eeg_data[6, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "freqs, psd_open = welch(eeg_data[6, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "\n",
    "axes[1, 0].semilogy(freqs, psd_closed, label='Eyes Closed', alpha=0.8)\n",
    "axes[1, 0].semilogy(freqs, psd_open, label='Eyes Open', alpha=0.8)\n",
    "axes[1, 0].set_xlim(0, 40)\n",
    "axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 0].set_ylabel('PSD (Î¼VÂ²/Hz)')\n",
    "axes[1, 0].set_title('Power Spectral Density - P3 Channel')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Topographic map of alpha power\n",
    "alpha_band = (8, 12)\n",
    "alpha_indices = (freqs >= alpha_band[0]) & (freqs <= alpha_band[1])\n",
    "\n",
    "alpha_power_closed = np.zeros(n_channels)\n",
    "alpha_power_open = np.zeros(n_channels)\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    # Eyes closed\n",
    "    freqs_ch, psd_ch = welch(eeg_data[ch, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "    alpha_power_closed[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n",
    "    \n",
    "    # Eyes open\n",
    "    freqs_ch, psd_ch = welch(eeg_data[ch, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n",
    "    alpha_power_open[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n",
    "\n",
    "# Bar plot of alpha power by channel\n",
    "x = np.arange(n_channels)\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, alpha_power_closed, width, label='Eyes Closed', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, alpha_power_open, width, label='Eyes Open', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Channel')\n",
    "axes[1, 1].set_ylabel('Alpha Power (Î¼VÂ²)')\n",
    "axes[1, 1].set_title('Alpha Power by Channel (8-12 Hz)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(channel_names, rotation=45)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Statistical Analysis ===\n",
    "print(\"\\n=== Statistical Analysis ===\")\n",
    "\n",
    "# Compare alpha power between conditions\n",
    "print(\"Alpha power comparison:\")\n",
    "print(f\"Eyes Closed - Mean: {alpha_power_closed.mean():.3f}, Std: {alpha_power_closed.std():.3f}\")\n",
    "print(f\"Eyes Open - Mean: {alpha_power_open.mean():.3f}, Std: {alpha_power_open.std():.3f}\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "pooled_std = np.sqrt(((alpha_power_closed.std()**2) + (alpha_power_open.std()**2)) / 2)\n",
    "cohens_d = (alpha_power_closed.mean() - alpha_power_open.mean()) / pooled_std\n",
    "print(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n",
    "\n",
    "# Find channels with largest differences\n",
    "alpha_diff = alpha_power_closed - alpha_power_open\n",
    "max_diff_channel = np.argmax(alpha_diff)\n",
    "print(f\"\\nLargest alpha difference in channel: {channel_names[max_diff_channel]} ({alpha_diff[max_diff_channel]:.3f} Î¼VÂ²)\")\n",
    "\n",
    "# Correlation between channels\n",
    "print(\"\\n=== Channel Connectivity Analysis ===\")\n",
    "\n",
    "# Compute average correlation matrices for each condition\n",
    "corr_closed = np.zeros((n_channels, n_channels))\n",
    "corr_open = np.zeros((n_channels, n_channels))\n",
    "\n",
    "for trial in range(30):\n",
    "    corr_closed += np.corrcoef(eeg_data[:, :, trial])\n",
    "    corr_open += np.corrcoef(eeg_data[:, :, trial + 30])\n",
    "\n",
    "corr_closed /= 30\n",
    "corr_open /= 30\n",
    "\n",
    "# Plot correlation matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "im1 = axes[0].imshow(corr_closed, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0].set_title('Eyes Closed - Correlation Matrix')\n",
    "axes[0].set_xticks(range(n_channels))\n",
    "axes[0].set_yticks(range(n_channels))\n",
    "axes[0].set_xticklabels(channel_names, rotation=45)\n",
    "axes[0].set_yticklabels(channel_names)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(corr_open, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1].set_title('Eyes Open - Correlation Matrix')\n",
    "axes[1].set_xticks(range(n_channels))\n",
    "axes[1].set_yticks(range(n_channels))\n",
    "axes[1].set_xticklabels(channel_names, rotation=45)\n",
    "axes[1].set_yticklabels(channel_names)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Difference matrix\n",
    "corr_diff = corr_closed - corr_open\n",
    "im3 = axes[2].imshow(corr_diff, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
    "axes[2].set_title('Correlation Difference (Closed - Open)')\n",
    "axes[2].set_xticks(range(n_channels))\n",
    "axes[2].set_yticks(range(n_channels))\n",
    "axes[2].set_xticklabels(channel_names, rotation=45)\n",
    "axes[2].set_yticklabels(channel_names)\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "avg_corr_closed = np.mean(corr_closed[np.triu_indices(n_channels, k=1)])\n",
    "avg_corr_open = np.mean(corr_open[np.triu_indices(n_channels, k=1)])\n",
    "\n",
    "print(f\"Average correlation - Eyes Closed: {avg_corr_closed:.3f}\")\n",
    "print(f\"Average correlation - Eyes Open: {avg_corr_open:.3f}\")\n",
    "print(f\"Difference: {avg_corr_closed - avg_corr_open:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‹ï¸ Practice Exercises: Test Your Skills!\n",
    "\n",
    "Time to put your NumPy skills to the test! Try these exercises that mirror real neuroscience challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Event-Related Potential (ERP) Analysis ðŸŽ¯\n",
    "\n",
    "**Task**: Analyze event-related potentials by extracting epochs around stimulus events and computing averaged responses.\n",
    "\n",
    "**Background**: In EEG experiments, we often want to see how the brain responds to specific stimuli (like seeing a face or hearing a tone). We extract small time windows around each stimulus and average across trials to reveal the typical brain response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise 1: Event-Related Potential Analysis ===\n",
    "print(\"=== Exercise 1: ERP Analysis ===\")\n",
    "\n",
    "# Simulate continuous EEG data\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 120  # seconds (2 minutes)\n",
    "n_channels = 4\n",
    "channel_names = ['Fz', 'Cz', 'Pz', 'Oz']\n",
    "\n",
    "# Generate continuous EEG\n",
    "np.random.seed(123)\n",
    "time_continuous = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "continuous_eeg = np.random.randn(n_channels, len(time_continuous)) * 2\n",
    "\n",
    "# Add some background alpha rhythm\n",
    "for ch in range(n_channels):\n",
    "    continuous_eeg[ch] += 3 * np.sin(2 * np.pi * 10 * time_continuous + ch * 0.5)\n",
    "\n",
    "# Generate stimulus events (random times)\n",
    "n_events = 50\n",
    "event_times = np.sort(np.random.uniform(5, duration-5, n_events))  # Avoid edges\n",
    "event_samples = (event_times * sampling_rate).astype(int)\n",
    "\n",
    "# Add stimulus-locked responses to the data\n",
    "for event_sample in event_samples:\n",
    "    # Create a stereotypical ERP response\n",
    "    erp_time = np.linspace(0, 1, 250)  # 1 second response\n",
    "    \n",
    "    # N100 component (negative deflection at 100ms)\n",
    "    n100 = -5 * np.exp(-((erp_time - 0.1)**2) / (2 * 0.02**2))\n",
    "    \n",
    "    # P300 component (positive deflection at 300ms)\n",
    "    p300 = 8 * np.exp(-((erp_time - 0.3)**2) / (2 * 0.05**2))\n",
    "    \n",
    "    erp_response = n100 + p300\n",
    "    \n",
    "    # Add to each channel (with some variability)\n",
    "    for ch in range(n_channels):\n",
    "        if event_sample + 250 < continuous_eeg.shape[1]:  # Make sure we don't go out of bounds\n",
    "            # Different channels have different sensitivities\n",
    "            sensitivity = [0.5, 1.0, 1.2, 0.8][ch]  # Cz and Pz are most sensitive\n",
    "            continuous_eeg[ch, event_sample:event_sample+250] += sensitivity * erp_response\n",
    "\n",
    "print(f\"Continuous EEG shape: {continuous_eeg.shape}\")\n",
    "print(f\"Number of events: {n_events}\")\n",
    "print(f\"Event times (first 10): {event_times[:10]}\")\n",
    "\n",
    "# YOUR TASK: Complete the following functions\n",
    "\n",
    "def extract_epochs(continuous_data, event_samples, pre_stim=0.2, post_stim=0.8, sampling_rate=250):\n",
    "    \"\"\"\n",
    "    Extract epochs around stimulus events.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    continuous_data : array, shape (n_channels, n_timepoints)\n",
    "        Continuous EEG data\n",
    "    event_samples : array\n",
    "        Sample indices of stimulus events\n",
    "    pre_stim : float\n",
    "        Time before stimulus (seconds)\n",
    "    post_stim : float\n",
    "        Time after stimulus (seconds)\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs : array, shape (n_channels, n_timepoints_epoch, n_events)\n",
    "        Extracted epochs\n",
    "    epoch_times : array\n",
    "        Time axis for epochs (relative to stimulus)\n",
    "    \"\"\"\n",
    "    # TODO: Implement epoch extraction\n",
    "    pre_samples = int(pre_stim * sampling_rate)\n",
    "    post_samples = int(post_stim * sampling_rate)\n",
    "    epoch_length = pre_samples + post_samples\n",
    "    \n",
    "    n_channels = continuous_data.shape[0]\n",
    "    valid_events = []\n",
    "    epochs_list = []\n",
    "    \n",
    "    for event_sample in event_samples:\n",
    "        start_sample = event_sample - pre_samples\n",
    "        end_sample = event_sample + post_samples\n",
    "        \n",
    "        # Check if epoch is within data bounds\n",
    "        if start_sample >= 0 and end_sample < continuous_data.shape[1]:\n",
    "            epoch = continuous_data[:, start_sample:end_sample]\n",
    "            epochs_list.append(epoch)\n",
    "            valid_events.append(event_sample)\n",
    "    \n",
    "    epochs = np.stack(epochs_list, axis=2)\n",
    "    epoch_times = np.linspace(-pre_stim, post_stim, epoch_length)\n",
    "    \n",
    "    return epochs, epoch_times\n",
    "\n",
    "def baseline_correct_epochs(epochs, baseline_period, epoch_times):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "    baseline_period : tuple\n",
    "        (start_time, end_time) for baseline period\n",
    "    epoch_times : array\n",
    "        Time axis for epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_corrected : array\n",
    "        Baseline-corrected epochs\n",
    "    \"\"\"\n",
    "    # TODO: Implement baseline correction\n",
    "    baseline_mask = (epoch_times >= baseline_period[0]) & (epoch_times <= baseline_period[1])\n",
    "    baseline_mean = np.mean(epochs[:, baseline_mask, :], axis=1, keepdims=True)\n",
    "    epochs_corrected = epochs - baseline_mean\n",
    "    \n",
    "    return epochs_corrected\n",
    "\n",
    "def compute_erp(epochs):\n",
    "    \"\"\"\n",
    "    Compute event-related potential by averaging across trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    erp : array, shape (n_channels, n_timepoints)\n",
    "        Averaged ERP\n",
    "    erp_std : array, shape (n_channels, n_timepoints)\n",
    "        Standard deviation across trials\n",
    "    \"\"\"\n",
    "    # TODO: Implement ERP computation\n",
    "    erp = np.mean(epochs, axis=2)\n",
    "    erp_std = np.std(epochs, axis=2)\n",
    "    \n",
    "    return erp, erp_std\n",
    "\n",
    "# Test your implementation\n",
    "epochs, epoch_times = extract_epochs(continuous_eeg, event_samples, pre_stim=0.2, post_stim=0.8)\n",
    "epochs_corrected = baseline_correct_epochs(epochs, baseline_period=(-0.2, 0), epoch_times=epoch_times)\n",
    "erp, erp_std = compute_erp(epochs_corrected)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Epochs shape: {epochs.shape}\")\n",
    "print(f\"ERP shape: {erp.shape}\")\n",
    "print(f\"Epoch time range: {epoch_times[0]:.3f} to {epoch_times[-1]:.3f} seconds\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    plt.subplot(2, 2, ch + 1)\n",
    "    plt.plot(epoch_times, erp[ch], 'b-', linewidth=2, label='ERP')\n",
    "    plt.fill_between(epoch_times, erp[ch] - erp_std[ch], erp[ch] + erp_std[ch], \n",
    "                     alpha=0.3, color='blue', label='Â±1 SD')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='Stimulus')\n",
    "    plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude (Î¼V)')\n",
    "    plt.title(f'ERP - Channel {channel_names[ch]}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if ch == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis questions for you to think about:\n",
    "print(\"\\n=== Analysis Questions ===\")\n",
    "print(\"1. Which channel shows the strongest P300 response?\")\n",
    "print(\"2. At what time does the N100 component peak?\")\n",
    "print(\"3. How does the signal-to-noise ratio compare across channels?\")\n",
    "\n",
    "# Find P300 peak\n",
    "p300_window = (epoch_times >= 0.2) & (epoch_times <= 0.4)\n",
    "p300_peaks = np.argmax(erp[:, p300_window], axis=1)\n",
    "p300_peak_times = epoch_times[p300_window][p300_peaks]\n",
    "\n",
    "print(f\"\\nP300 peak times by channel:\")\n",
    "for ch in range(n_channels):\n",
    "    print(f\"  {channel_names[ch]}: {p300_peak_times[ch]:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Spectral Analysis and Connectivity ðŸŒŠ\n",
    "\n",
    "**Task**: Analyze the frequency content of neural signals and compute connectivity between brain regions.\n",
    "\n",
    "**Background**: The brain operates at different frequency bands (delta, theta, alpha, beta, gamma). Understanding how these frequencies change and how they're synchronized between brain regions gives us insights into brain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise 2: Spectral Analysis and Connectivity ===\n",
    "print(\"=== Exercise 2: Spectral Analysis and Connectivity ===\")\n",
    "\n",
    "# Generate sample data with known connectivity\n",
    "np.random.seed(456)\n",
    "sampling_rate = 500  # Hz\n",
    "duration = 60  # seconds\n",
    "n_channels = 6\n",
    "channel_names = ['F3', 'F4', 'C3', 'C4', 'P3', 'P4']\n",
    "\n",
    "time = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "n_timepoints = len(time)\n",
    "\n",
    "# Create signals with known connectivity patterns\n",
    "signals = np.zeros((n_channels, n_timepoints))\n",
    "\n",
    "# Generate base oscillations\n",
    "alpha_source = np.sin(2 * np.pi * 10 * time)  # 10 Hz alpha\n",
    "beta_source = np.sin(2 * np.pi * 20 * time)   # 20 Hz beta\n",
    "gamma_source = np.sin(2 * np.pi * 40 * time)  # 40 Hz gamma\n",
    "\n",
    "# Add noise and connectivity patterns\n",
    "for ch in range(n_channels):\n",
    "    # Base noise\n",
    "    signals[ch] = 0.5 * np.random.randn(n_timepoints)\n",
    "    \n",
    "    # Add frequency-specific connectivity\n",
    "    if 'F' in channel_names[ch]:  # Frontal channels\n",
    "        signals[ch] += 2 * beta_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    elif 'C' in channel_names[ch]:  # Central channels\n",
    "        signals[ch] += 1.5 * alpha_source + 1 * beta_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    elif 'P' in channel_names[ch]:  # Posterior channels\n",
    "        signals[ch] += 3 * alpha_source + 0.5 * np.random.randn(n_timepoints)\n",
    "    \n",
    "    # Add some gamma coupling\n",
    "    if ch % 2 == 0:  # Left hemisphere\n",
    "        signals[ch] += 0.8 * gamma_source\n",
    "    else:  # Right hemisphere\n",
    "        # Delayed gamma (simulate inter-hemispheric delay)\n",
    "        delay_samples = int(0.01 * sampling_rate)  # 10ms delay\n",
    "        delayed_gamma = np.zeros_like(gamma_source)\n",
    "        delayed_gamma[delay_samples:] = gamma_source[:-delay_samples]\n",
    "        signals[ch] += 0.8 * delayed_gamma\n",
    "\n",
    "print(f\"Generated signals shape: {signals.shape}\")\n",
    "print(f\"Duration: {duration} seconds\")\n",
    "print(f\"Sampling rate: {sampling_rate} Hz\")\n",
    "\n",
    "# YOUR TASK: Implement the following functions\n",
    "\n",
    "def compute_power_spectrum(data, sampling_rate, nperseg=None):\n",
    "    \"\"\"\n",
    "    Compute power spectral density for multi-channel data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array, shape (n_channels, n_timepoints)\n",
    "        Multi-channel signal data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "    nperseg : int, optional\n",
    "        Length of each segment for Welch's method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    psd : array, shape (n_channels, n_frequencies)\n",
    "        Power spectral density for each channel\n",
    "    \"\"\"\n",
    "    # TODO: Implement power spectrum computation\n",
    "    if nperseg is None:\n",
    "        nperseg = sampling_rate * 4  # 4 second windows\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Compute PSD for first channel to get frequency array\n",
    "    frequencies, psd_ch = signal.welch(data[0], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    # Initialize PSD array\n",
    "    psd = np.zeros((n_channels, len(frequencies)))\n",
    "    \n",
    "    # Compute PSD for each channel\n",
    "    for ch in range(n_channels):\n",
    "        frequencies, psd[ch] = signal.welch(data[ch], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    return frequencies, psd\n",
    "\n",
    "def compute_band_power(psd, frequencies, freq_bands):\n",
    "    \"\"\"\n",
    "    Compute power in specific frequency bands.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    psd : array, shape (n_channels, n_frequencies)\n",
    "        Power spectral density\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    freq_bands : dict\n",
    "        Dictionary of frequency bands {band_name: (low_freq, high_freq)}\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    band_powers : dict\n",
    "        Dictionary of band powers {band_name: array of powers per channel}\n",
    "    \"\"\"\n",
    "    # TODO: Implement band power computation\n",
    "    band_powers = {}\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in freq_bands.items():\n",
    "        # Create frequency mask\n",
    "        freq_mask = (frequencies >= low_freq) & (frequencies <= high_freq)\n",
    "        \n",
    "        # Compute power in band for each channel\n",
    "        band_power = np.zeros(psd.shape[0])\n",
    "        for ch in range(psd.shape[0]):\n",
    "            band_power[ch] = np.trapz(psd[ch, freq_mask], frequencies[freq_mask])\n",
    "        \n",
    "        band_powers[band_name] = band_power\n",
    "    \n",
    "    return band_powers\n",
    "\n",
    "def compute_coherence(data, sampling_rate, nperseg=None):\n",
    "    \"\"\"\n",
    "    Compute coherence between all pairs of channels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array, shape (n_channels, n_timepoints)\n",
    "        Multi-channel signal data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "    nperseg : int, optional\n",
    "        Length of each segment\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    coherence : array, shape (n_channels, n_channels, n_frequencies)\n",
    "        Coherence matrix\n",
    "    \"\"\"\n",
    "    # TODO: Implement coherence computation\n",
    "    if nperseg is None:\n",
    "        nperseg = sampling_rate * 4\n",
    "    \n",
    "    n_channels = data.shape[0]\n",
    "    \n",
    "    # Compute coherence for first pair to get frequency array\n",
    "    frequencies, coh_temp = signal.coherence(data[0], data[1], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    # Initialize coherence array\n",
    "    coherence = np.zeros((n_channels, n_channels, len(frequencies)))\n",
    "    \n",
    "    # Compute coherence for all pairs\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i == j:\n",
    "                coherence[i, j, :] = 1.0  # Perfect coherence with self\n",
    "            else:\n",
    "                frequencies, coherence[i, j, :] = signal.coherence(data[i], data[j], sampling_rate, nperseg=nperseg)\n",
    "    \n",
    "    return frequencies, coherence\n",
    "\n",
    "# Test your implementation\n",
    "frequencies, psd = compute_power_spectrum(signals, sampling_rate)\n",
    "\n",
    "freq_bands = {\n",
    "    'delta': (1, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30),\n",
    "    'gamma': (30, 80)\n",
    "}\n",
    "\n",
    "band_powers = compute_band_power(psd, frequencies, freq_bands)\n",
    "coh_frequencies, coherence = compute_coherence(signals, sampling_rate)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"PSD shape: {psd.shape}\")\n",
    "print(f\"Frequency range: {frequencies[0]:.1f} to {frequencies[-1]:.1f} Hz\")\n",
    "print(f\"Coherence shape: {coherence.shape}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Power spectra\n",
    "for ch in range(n_channels):\n",
    "    axes[0, 0].semilogy(frequencies, psd[ch], label=channel_names[ch], alpha=0.8)\n",
    "axes[0, 0].set_xlim(0, 60)\n",
    "axes[0, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[0, 0].set_ylabel('PSD (Î¼VÂ²/Hz)')\n",
    "axes[0, 0].set_title('Power Spectral Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Band powers\n",
    "band_names = list(freq_bands.keys())\n",
    "x_pos = np.arange(len(band_names))\n",
    "bar_width = 0.1\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    powers = [band_powers[band][ch] for band in band_names]\n",
    "    axes[0, 1].bar(x_pos + ch * bar_width, powers, bar_width, \n",
    "                   label=channel_names[ch], alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Frequency Band')\n",
    "axes[0, 1].set_ylabel('Power (Î¼VÂ²)')\n",
    "axes[0, 1].set_title('Band Powers')\n",
    "axes[0, 1].set_xticks(x_pos + bar_width * 2.5)\n",
    "axes[0, 1].set_xticklabels(band_names)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Coherence heatmap (average across frequencies)\n",
    "avg_coherence = np.mean(coherence, axis=2)\n",
    "im = axes[0, 2].imshow(avg_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0, 2].set_title('Average Coherence Matrix')\n",
    "axes[0, 2].set_xticks(range(n_channels))\n",
    "axes[0, 2].set_yticks(range(n_channels))\n",
    "axes[0, 2].set_xticklabels(channel_names)\n",
    "axes[0, 2].set_yticklabels(channel_names)\n",
    "plt.colorbar(im, ax=axes[0, 2])\n",
    "\n",
    "# 4. Alpha coherence network\n",
    "alpha_idx = (coh_frequencies >= 8) & (coh_frequencies <= 13)\n",
    "alpha_coherence = np.mean(coherence[:, :, alpha_idx], axis=2)\n",
    "im2 = axes[1, 0].imshow(alpha_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('Alpha Band Coherence (8-13 Hz)')\n",
    "axes[1, 0].set_xticks(range(n_channels))\n",
    "axes[1, 0].set_yticks(range(n_channels))\n",
    "axes[1, 0].set_xticklabels(channel_names)\n",
    "axes[1, 0].set_yticklabels(channel_names)\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# 5. Beta coherence network\n",
    "beta_idx = (coh_frequencies >= 13) & (coh_frequencies <= 30)\n",
    "beta_coherence = np.mean(coherence[:, :, beta_idx], axis=2)\n",
    "im3 = axes[1, 1].imshow(beta_coherence, cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Beta Band Coherence (13-30 Hz)')\n",
    "axes[1, 1].set_xticks(range(n_channels))\n",
    "axes[1, 1].set_yticks(range(n_channels))\n",
    "axes[1, 1].set_xticklabels(channel_names)\n",
    "axes[1, 1].set_yticklabels(channel_names)\n",
    "plt.colorbar(im3, ax=axes[1, 1])\n",
    "\n",
    "# 6. Coherence spectrum between F3 and F4\n",
    "f3_idx = channel_names.index('F3')\n",
    "f4_idx = channel_names.index('F4')\n",
    "axes[1, 2].plot(coh_frequencies, coherence[f3_idx, f4_idx, :], 'b-', linewidth=2)\n",
    "axes[1, 2].set_xlim(0, 60)\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 2].set_ylabel('Coherence')\n",
    "axes[1, 2].set_title('F3-F4 Coherence Spectrum')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis summary\n",
    "print(\"\\n=== Analysis Summary ===\")\n",
    "print(\"Band power analysis:\")\n",
    "for band in band_names:\n",
    "    max_channel = np.argmax(band_powers[band])\n",
    "    print(f\"  {band}: strongest in {channel_names[max_channel]} ({band_powers[band][max_channel]:.3f} Î¼VÂ²)\")\n",
    "\n",
    "print(\"\\nCoherence analysis:\")\n",
    "# Find strongest coherence pairs (excluding self-connections)\n",
    "coherence_no_diag = avg_coherence.copy()\n",
    "np.fill_diagonal(coherence_no_diag, 0)\n",
    "max_coherence_idx = np.unravel_index(np.argmax(coherence_no_diag), coherence_no_diag.shape)\n",
    "print(f\"  Strongest coherence: {channel_names[max_coherence_idx[0]]}-{channel_names[max_coherence_idx[1]]} ({coherence_no_diag[max_coherence_idx]:.3f})\")\n",
    "\n",
    "# Interhemispheric coherence\n",
    "left_channels = [i for i, ch in enumerate(channel_names) if '3' in ch]\n",
    "right_channels = [i for i, ch in enumerate(channel_names) if '4' in ch]\n",
    "interhemispheric_coherence = []\n",
    "\n",
    "for l_ch in left_channels:\n",
    "    for r_ch in right_channels:\n",
    "        if channel_names[l_ch][0] == channel_names[r_ch][0]:  # Same region\n",
    "            coh_val = avg_coherence[l_ch, r_ch]\n",
    "            interhemispheric_coherence.append(coh_val)\n",
    "            print(f\"  {channel_names[l_ch]}-{channel_names[r_ch]} coherence: {coh_val:.3f}\")\n",
    "\n",
    "print(f\"  Average interhemispheric coherence: {np.mean(interhemispheric_coherence):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Key Takeaways: What You've Learned\n",
    "\n",
    "Congratulations! You've just completed a comprehensive review of Python and NumPy fundamentals for neuroscience. Here's what you've mastered:\n",
    "\n",
    "## ðŸ Python Skills\n",
    "- **List comprehensions** for efficient data processing\n",
    "- **Functions** for building reusable analysis tools\n",
    "- **Classes** for organizing complex analysis pipelines\n",
    "- **Best practices** for scientific computing\n",
    "\n",
    "## ðŸ”¢ NumPy Mastery\n",
    "- **Array creation and manipulation** for multi-dimensional brain data\n",
    "- **Indexing and slicing** to extract specific channels, time windows, and trials\n",
    "- **Vectorized operations** for lightning-fast computations\n",
    "- **Mathematical operations** essential for signal processing\n",
    "- **Linear algebra** foundations for neural networks\n",
    "\n",
    "## ðŸ§  Neuroscience Applications\n",
    "- **EEG data processing** with realistic multi-channel signals\n",
    "- **Event-related potential (ERP) analysis** for studying brain responses\n",
    "- **Spectral analysis** to understand frequency content\n",
    "- **Connectivity analysis** using coherence measures\n",
    "- **Statistical analysis** of neural data\n",
    "\n",
    "## ðŸ’¡ Performance Tips You've Learned\n",
    "\n",
    "1. **Always use vectorized operations** instead of Python loops\n",
    "2. **Specify axis parameters** carefully when working with multi-dimensional data\n",
    "3. **Use keepdims=True** when you need to preserve array dimensions\n",
    "4. **Leverage NumPy's broadcasting** for efficient operations\n",
    "5. **Choose appropriate data types** to save memory\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ What's Next?\n",
    "\n",
    "In the next notebook (`02_ml_foundations.ipynb`), you'll build on these fundamentals to explore:\n",
    "\n",
    "- **Machine learning concepts** relevant to neuroscience\n",
    "- **Classification and regression** for brain state detection\n",
    "- **Cross-validation** and model evaluation\n",
    "- **Feature engineering** for neural signals\n",
    "- **Preparing for deep learning** with PyTorch\n",
    "\n",
    "You're well-equipped for this journey! The NumPy skills you've practiced here will be your foundation for everything that comes next.\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue your deep learning journey? See you in the next notebook! ðŸ§ âš¡**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
