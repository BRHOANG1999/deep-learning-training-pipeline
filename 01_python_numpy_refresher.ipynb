{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐍 Python & NumPy Fundamentals for Neuroscience\n",
    "\n",
    "Welcome back, neural network explorer! 🧠⚡\n",
    "\n",
    "Before we dive into the exciting world of brain signal analysis, let's make sure you're comfortable with the core tools you'll be using every day. This notebook is designed as a **refresher** - not a complete introduction to programming.\n",
    "\n",
    "## What We'll Cover Today 🎯\n",
    "\n",
    "1. **Python Essentials**: The language constructs you'll use constantly\n",
    "2. **NumPy Fundamentals**: The backbone of scientific computing\n",
    "3. **Data Structures**: Arrays, lists, and when to use each\n",
    "4. **Mathematical Operations**: Linear algebra for neural networks\n",
    "5. **Brain Signal Examples**: Real-world applications of what you're learning\n",
    "\n",
    "## 💡 Why This Matters\n",
    "\n",
    "In neuroscience, you'll constantly work with:\n",
    "- **Multi-dimensional arrays** (time × channels × trials)\n",
    "- **Mathematical operations** (filtering, Fourier transforms, matrix multiplication)\n",
    "- **Data manipulation** (selecting time windows, averaging across trials)\n",
    "- **Efficient computation** (vectorized operations for speed)\n",
    "\n",
    "Master these fundamentals, and you'll breeze through the advanced topics!\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to refresh your Python skills? Let's go! 🚀*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ Setting Up Our Environment\n",
    "\n",
    "First, let's import the libraries we'll be using and set up some nice defaults for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Essential imports for today's session\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import signal\nimport time\nimport warnings\nimport sys\nimport psutil\nimport platform\nfrom IPython.display import display, HTML, clear_output\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Enhanced matplotlib setup\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\n\n# NumPy settings for cleaner output\nnp.set_printoptions(precision=3, suppress=True)\n\n# Check system capabilities\ndef check_system_capabilities():\n    \"\"\"Check system resources and provide recommendations\"\"\"\n    \n    print(\"🔍 System Capabilities Check\")\n    print(\"=\" * 40)\n    \n    # Memory check\n    try:\n        memory_info = psutil.virtual_memory()\n        total_memory_gb = memory_info.total / (1024**3)\n        available_memory_gb = memory_info.available / (1024**3)\n        \n        print(f\"💾 Total RAM: {total_memory_gb:.1f} GB\")\n        print(f\"💾 Available RAM: {available_memory_gb:.1f} GB\")\n        \n        if total_memory_gb < 4:\n            print(\"⚠️ Low RAM detected. Consider using smaller datasets.\")\n        elif total_memory_gb >= 8:\n            print(\"✅ Good RAM capacity for large neuroscience datasets.\")\n        \n    except Exception as e:\n        print(f\"ℹ️ Memory info unavailable: {e}\")\n    \n    # CPU check\n    try:\n        cpu_count = psutil.cpu_count()\n        print(f\"🔥 CPU Cores: {cpu_count}\")\n        \n        if cpu_count >= 4:\n            print(\"✅ Multi-core processing available for parallel operations.\")\n        else:\n            print(\"ℹ️ Single/dual-core system. Some operations may be slower.\")\n            \n    except Exception as e:\n        print(f\"ℹ️ CPU info unavailable: {e}\")\n    \n    # Platform-specific recommendations\n    system = platform.system()\n    print(f\"💻 Platform: {system}\")\n    \n    if system == \"Windows\":\n        print(\"💡 Windows tip: Use WSL2 for better Linux compatibility\")\n    elif system == \"Darwin\":\n        print(\"💡 macOS tip: Consider using conda for package management\")\n    elif system == \"Linux\":\n        print(\"✅ Linux detected - optimal for scientific computing\")\n    \n    print(\"=\" * 40)\n    return True\n\n# Error handling decorator\ndef handle_errors(func):\n    \"\"\"Decorator to handle common errors gracefully\"\"\"\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except ImportError as e:\n            print(f\"❌ Import Error: {e}\")\n            print(\"💡 Solution: Install missing packages or restart kernel\")\n            return None\n        except MemoryError:\n            print(\"❌ Memory Error: Dataset too large for available RAM\")\n            print(\"💡 Solution: Use smaller datasets or increase system memory\")\n            return None\n        except Exception as e:\n            print(f\"❌ Unexpected Error: {e}\")\n            print(\"💡 Solution: Check input parameters and data types\")\n            return None\n    return wrapper\n\n# Enhanced imports with error handling\n@handle_errors\ndef import_interactive_widgets():\n    \"\"\"Import interactive widgets with graceful fallback\"\"\"\n    try:\n        import ipywidgets as widgets\n        from ipywidgets import interact, interactive, fixed\n        print(\"✅ Interactive widgets available\")\n        return widgets, interact, interactive, fixed\n    except ImportError:\n        print(\"ℹ️ Interactive widgets not available (install ipywidgets)\")\n        return None, None, None, None\n\n# Run system check\ncheck_system_capabilities()\n\n# Try to import interactive widgets\nwidgets, interact, interactive, fixed = import_interactive_widgets()\n\nprint(\"\\n✅ Environment ready!\")\nprint(f\"📊 NumPy version: {np.__version__}\")\nprint(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\nprint(f\"🔬 SciPy version: {scipy.__version__}\")\nprint(f\"🐍 Python version: {sys.version.split()[0]}\")\n\n# Create a simple neuroscience glossary\nneuroscience_glossary = {\n    'EEG': 'Electroencephalography - recording electrical activity of the brain',\n    'Alpha waves': 'Brain waves (8-13 Hz) associated with relaxed awareness',\n    'Beta waves': 'Brain waves (13-30 Hz) associated with active concentration',\n    'Theta waves': 'Brain waves (4-8 Hz) associated with creativity and deep meditation',\n    'Delta waves': 'Brain waves (0.5-4 Hz) associated with deep sleep',\n    'Gamma waves': 'Brain waves (30-100 Hz) associated with high-level cognitive processing',\n    'Artifact': 'Unwanted signal contamination in neural recordings',\n    'Epoch': 'A segment of continuous EEG data, typically around a stimulus event',\n    'Montage': 'The arrangement of electrodes on the scalp',\n    'Coherence': 'A measure of synchronization between two brain signals'\n}\n\nprint(\"\\n📚 Neuroscience Glossary loaded (access via 'neuroscience_glossary')\")\nprint(\"💡 Example: print(neuroscience_glossary['EEG'])\")\n\n# Performance benchmark function\ndef benchmark_numpy_performance():\n    \"\"\"Benchmark NumPy performance on this system\"\"\"\n    \n    print(\"\\n🏃 NumPy Performance Benchmark\")\n    print(\"=\" * 35)\n    \n    # Test array creation\n    start_time = time.time()\n    large_array = np.random.randn(1000, 1000)\n    creation_time = time.time() - start_time\n    print(f\"Array creation (1M elements): {creation_time:.4f} seconds\")\n    \n    # Test matrix multiplication\n    start_time = time.time()\n    result = np.dot(large_array, large_array.T)\n    multiplication_time = time.time() - start_time\n    print(f\"Matrix multiplication: {multiplication_time:.4f} seconds\")\n    \n    # Test FFT (important for signal processing)\n    signal_1d = np.random.randn(10000)\n    start_time = time.time()\n    fft_result = np.fft.fft(signal_1d)\n    fft_time = time.time() - start_time\n    print(f\"FFT (10K samples): {fft_time:.4f} seconds\")\n    \n    # Performance assessment\n    if multiplication_time < 0.1:\n        print(\"✅ Excellent NumPy performance!\")\n    elif multiplication_time < 0.5:\n        print(\"✅ Good NumPy performance\")\n    else:\n        print(\"⚠️ Slower NumPy performance - consider updating NumPy/BLAS\")\n    \n    print(\"=\" * 35)\n    return creation_time, multiplication_time, fft_time\n\n# Run performance benchmark\nbenchmark_results = benchmark_numpy_performance()\n\nprint(\"\\n🎯 Ready to explore Python & NumPy for Neuroscience!\")\nprint(\"💡 Tip: Use 'help(function_name)' to get detailed documentation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐍 Python Essentials: Quick Review\n",
    "\n",
    "Let's quickly review the Python constructs you'll use most often in neuroscience computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Comprehensions: Your Secret Weapon 🔧\n",
    "\n",
    "List comprehensions are incredibly useful for data processing. In neuroscience, you'll often need to apply operations across multiple files, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🔧 Common Errors & Debugging Like a Neuroscientist\n\nprint(\"🚨 Common Python/NumPy Errors in Neuroscience & How to Fix Them\")\nprint(\"=\" * 60)\n\n# Create examples of common errors with solutions\ndef demonstrate_common_errors():\n    \"\"\"Show common errors and their solutions with explanations\"\"\"\n    \n    print(\"\\n1. 📊 INDEXING ERRORS\")\n    print(\"-\" * 20)\n    \n    # Example data (channels × time × trials)\n    eeg_data = np.random.randn(64, 1000, 100)\n    channels = [0, 1, 2]\n    trials = [10, 20, 30, 40, 50]\n    \n    print(\"❌ WRONG - Broadcasting Error:\")\n    print(\"   eeg_data[channels, :, trials]  # This will fail!\")\n    \n    try:\n        wrong_result = eeg_data[channels, :, trials]\n    except IndexError as e:\n        print(f\"   Error: {e}\")\n        print(\"\\n💡 SOLUTION:\")\n        print(\"   eeg_data[np.ix_(channels, range(eeg_data.shape[1]), trials)]\")\n        \n        # Correct approach\n        correct_result = eeg_data[np.ix_(channels, range(eeg_data.shape[1]), trials)]\n        print(f\"   ✅ Result shape: {correct_result.shape}\")\n    \n    print(\"\\n2. 🧮 MATRIX DIMENSION ERRORS\")\n    print(\"-\" * 30)\n    \n    print(\"❌ WRONG - Shape Mismatch:\")\n    print(\"   np.dot(matrix_64x1000, matrix_100x64)  # Incompatible shapes!\")\n    \n    matrix_a = np.random.randn(64, 1000)\n    matrix_b = np.random.randn(100, 64)\n    \n    try:\n        wrong_multiplication = np.dot(matrix_a, matrix_b)\n    except ValueError as e:\n        print(f\"   Error: {e}\")\n        print(\"\\n💡 SOLUTION:\")\n        print(\"   Check dimensions: (64,1000) @ (1000,X) ✅\")\n        print(\"   Use matrix_a @ matrix_b.T or matrix_a.T @ matrix_b\")\n        \n        # Correct approaches\n        correct_mult1 = matrix_a @ matrix_b.T  # (64,1000) @ (1000,100) = (64,100)\n        correct_mult2 = matrix_a.T @ matrix_b.T  # (1000,64) @ (64,100) = (1000,100)\n        print(f\"   ✅ Result shapes: {correct_mult1.shape}, {correct_mult2.shape}\")\n    \n    print(\"\\n3. 🔢 DATA TYPE ERRORS\")\n    print(\"-\" * 20)\n    \n    print(\"❌ WRONG - Integer Division (Python 2 style):\")\n    sampling_rate = 250\n    duration = 3\n    \n    # This can cause issues with indexing\n    wrong_samples = duration * sampling_rate / 2  # Float result\n    print(f\"   wrong_samples = {wrong_samples} (type: {type(wrong_samples)})\")\n    \n    print(\"\\n💡 SOLUTION:\")\n    correct_samples = int(duration * sampling_rate // 2)  # Integer result\n    print(f\"   correct_samples = {correct_samples} (type: {type(correct_samples)})\")\n    \n    print(\"\\n4. 📏 AXIS CONFUSION\")\n    print(\"-\" * 15)\n    \n    # Example: EEG data (channels × time × trials)\n    eeg_example = np.random.randn(8, 1000, 50)\n    \n    print(\"❌ WRONG - Unclear axis operations:\")\n    wrong_mean = np.mean(eeg_example, axis=1)  # What does this average?\n    print(f\"   np.mean(eeg_data, axis=1) → shape: {wrong_mean.shape}\")\n    print(\"   ❓ Is this averaging across time, channels, or trials?\")\n    \n    print(\"\\n💡 SOLUTION - Be explicit:\")\n    trial_average = np.mean(eeg_example, axis=2)  # Average across trials\n    time_average = np.mean(eeg_example, axis=1)   # Average across time  \n    channel_average = np.mean(eeg_example, axis=0) # Average across channels\n    \n    print(f\"   Trial average: {trial_average.shape} (channels × time)\")\n    print(f\"   Time average: {time_average.shape} (channels × trials)\")  \n    print(f\"   Channel average: {channel_average.shape} (time × trials)\")\n\n# Demonstrate common errors\ndemonstrate_common_errors()\n\nprint(\"\\n\\n🛠️ DEBUGGING TOOLKIT\")\nprint(\"=\" * 25)\n\ndef debug_array_info(array, name=\"array\"):\n    \"\"\"Comprehensive array debugging information\"\"\"\n    print(f\"\\n🔍 Debug info for '{name}':\")\n    print(f\"   Shape: {array.shape}\")\n    print(f\"   Data type: {array.dtype}\")\n    print(f\"   Memory usage: {array.nbytes / 1024:.1f} KB\")\n    print(f\"   Min/Max: {array.min():.3f} / {array.max():.3f}\")\n    print(f\"   Mean/Std: {array.mean():.3f} / {array.std():.3f}\")\n    print(f\"   Contains NaN: {np.isnan(array).any()}\")\n    print(f\"   Contains Inf: {np.isinf(array).any()}\")\n    \n    # Check for common issues\n    if np.isnan(array).any():\n        nan_count = np.isnan(array).sum()\n        print(f\"   ⚠️ Warning: {nan_count} NaN values detected!\")\n    \n    if np.isinf(array).any():\n        inf_count = np.isinf(array).sum()\n        print(f\"   ⚠️ Warning: {inf_count} infinite values detected!\")\n    \n    if array.std() == 0:\n        print(f\"   ⚠️ Warning: Zero variance - all values are identical!\")\n\n# Example usage\nsample_eeg = np.random.randn(32, 500, 20)\ndebug_array_info(sample_eeg, \"sample_EEG_data\")\n\nprint(\"\\n\\n🎯 NEUROSCIENCE-SPECIFIC DEBUGGING\")\nprint(\"=\" * 40)\n\ndef validate_eeg_data(eeg_data, sampling_rate=250, expected_duration=None):\n    \"\"\"Validate EEG data for common neuroscience issues\"\"\"\n    \n    print(\"🧠 EEG Data Validation Report\")\n    print(\"-\" * 30)\n    \n    # Basic checks\n    if eeg_data.ndim != 3:\n        print(\"❌ Expected 3D array (channels × time × trials)\")\n        return False\n    \n    n_channels, n_timepoints, n_trials = eeg_data.shape\n    duration = n_timepoints / sampling_rate\n    \n    print(f\"✅ Data shape: {n_channels} channels × {n_timepoints} timepoints × {n_trials} trials\")\n    print(f\"✅ Duration: {duration:.2f} seconds at {sampling_rate} Hz\")\n    \n    # Check for realistic values\n    amplitude_range = eeg_data.max() - eeg_data.min()\n    if amplitude_range > 1000:\n        print(f\"⚠️ Large amplitude range: {amplitude_range:.1f} μV (check for artifacts)\")\n    elif amplitude_range < 1:\n        print(f\"⚠️ Small amplitude range: {amplitude_range:.3f} μV (check scaling)\")\n    else:\n        print(f\"✅ Realistic amplitude range: {amplitude_range:.1f} μV\")\n    \n    # Check for flat channels\n    channel_stds = np.std(eeg_data, axis=(1, 2))\n    flat_channels = np.where(channel_stds < 0.01)[0]\n    if len(flat_channels) > 0:\n        print(f\"⚠️ Potentially flat channels: {flat_channels}\")\n    else:\n        print(\"✅ All channels show activity\")\n    \n    # Check for extreme outliers\n    z_scores = np.abs((eeg_data - np.mean(eeg_data)) / np.std(eeg_data))\n    extreme_outliers = np.sum(z_scores > 5)\n    if extreme_outliers > n_timepoints * n_channels * n_trials * 0.001:  # More than 0.1%\n        print(f\"⚠️ Many extreme outliers detected: {extreme_outliers}\")\n    else:\n        print(\"✅ Outlier levels normal\")\n    \n    # Expected duration check\n    if expected_duration and abs(duration - expected_duration) > 0.1:\n        print(f\"⚠️ Duration mismatch: expected {expected_duration}s, got {duration:.2f}s\")\n    \n    return True\n\n# Test the validator\nsample_eeg_data = np.random.randn(64, 1250, 30) * 50  # 64 channels, 5 seconds at 250 Hz, 30 trials\nvalidate_eeg_data(sample_eeg_data, sampling_rate=250, expected_duration=5.0)\n\nprint(\"\\n\\n💡 PREVENTION TIPS\")\nprint(\"=\" * 20)\n\nprevention_tips = [\n    \"Always check array shapes before operations: print(array.shape)\",\n    \"Use descriptive variable names: 'eeg_channels_time_trials' not 'data'\",\n    \"Add assertions for critical assumptions: assert data.ndim == 3\",\n    \"Use explicit axis parameters: np.mean(data, axis=2) not np.mean(data, 2)\",\n    \"Validate data ranges: check for realistic μV values in EEG\",\n    \"Test with small datasets first before scaling up\",\n    \"Use debug_array_info() function when things go wrong\",\n    \"Keep a backup of original data before preprocessing\"\n]\n\nfor i, tip in enumerate(prevention_tips, 1):\n    print(f\"{i:2d}. {tip}\")\n\nprint(\"\\n\\n📞 WHEN TO ASK FOR HELP\")\nprint(\"=\" * 30)\n\nhelp_scenarios = [\n    \"🔴 Memory errors with large datasets → Consider data chunking\",\n    \"🔴 Unexpected NaN/Inf values → Check data preprocessing pipeline\", \n    \"🔴 Shape mismatches in matrix operations → Review linear algebra\",\n    \"🔴 Performance issues → Profile code and optimize bottlenecks\",\n    \"🔴 Results don't match literature → Validate methodology and units\"\n]\n\nfor scenario in help_scenarios:\n    print(f\"   {scenario}\")\n\nprint(\"\\n💭 Remember: Every expert was once a beginner who never gave up debugging! 🚀\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions: Building Reusable Analysis Tools 🔨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_spectrum(signal_data, sampling_rate=250, freq_bands=None):\n",
    "    \"\"\"\n",
    "    Calculate power spectrum for EEG-like data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal_data : array-like\n",
    "        Time series data\n",
    "    sampling_rate : int\n",
    "        Sampling frequency in Hz\n",
    "    freq_bands : dict, optional\n",
    "        Dictionary of frequency bands to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    frequencies : array\n",
    "        Frequency values\n",
    "    power : array\n",
    "        Power spectral density\n",
    "    \"\"\"\n",
    "    if freq_bands is None:\n",
    "        freq_bands = {\n",
    "            'delta': (0.5, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30)\n",
    "        }\n",
    "    \n",
    "    # Calculate power spectrum using Welch's method\n",
    "    frequencies, power = signal.welch(signal_data, sampling_rate, nperseg=sampling_rate*2)\n",
    "    \n",
    "    # Calculate band powers\n",
    "    band_powers = {}\n",
    "    for band, (low, high) in freq_bands.items():\n",
    "        band_mask = (frequencies >= low) & (frequencies <= high)\n",
    "        band_powers[band] = np.trapz(power[band_mask], frequencies[band_mask])\n",
    "    \n",
    "    return frequencies, power, band_powers\n",
    "\n",
    "# Test our function\n",
    "test_signal = np.random.randn(1000) + 2*np.sin(2*np.pi*10*np.linspace(0, 4, 1000))\n",
    "freqs, psd, bands = calculate_power_spectrum(test_signal)\n",
    "\n",
    "print(\"Band powers:\")\n",
    "for band, power in bands.items():\n",
    "    print(f\"  {band}: {power:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes: Organizing Complex Analysis Pipelines 🏗️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGProcessor:\n",
    "    \"\"\"A simple EEG processing class to demonstrate object-oriented concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_rate=250, channels=None):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.channels = channels or ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4']\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        \"\"\"Load EEG data (channels × time_points).\"\"\"\n",
    "        self.raw_data = np.array(data)\n",
    "        print(f\"Data loaded: {self.raw_data.shape} (channels × time_points)\")\n",
    "        \n",
    "    def apply_bandpass_filter(self, low_freq=1, high_freq=40):\n",
    "        \"\"\"Apply a bandpass filter to remove noise.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded! Use load_data() first.\")\n",
    "            \n",
    "        # Design butterworth filter\n",
    "        sos = signal.butter(4, [low_freq, high_freq], \n",
    "                           btype='bandpass', fs=self.sampling_rate, output='sos')\n",
    "        \n",
    "        # Apply filter to each channel\n",
    "        self.processed_data = np.zeros_like(self.raw_data)\n",
    "        for i, channel_data in enumerate(self.raw_data):\n",
    "            self.processed_data[i] = signal.sosfilt(sos, channel_data)\n",
    "            \n",
    "        print(f\"✅ Bandpass filter applied ({low_freq}-{high_freq} Hz)\")\n",
    "        \n",
    "    def plot_channel(self, channel_idx=0, duration=2.0):\n",
    "        \"\"\"Plot raw vs processed data for a specific channel.\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"No data loaded!\")\n",
    "            \n",
    "        # Time axis\n",
    "        n_samples = int(duration * self.sampling_rate)\n",
    "        time_axis = np.linspace(0, duration, n_samples)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot raw data\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(time_axis, self.raw_data[channel_idx, :n_samples], 'b-', alpha=0.7)\n",
    "        plt.title(f'Raw EEG - Channel {self.channels[channel_idx]}')\n",
    "        plt.ylabel('Amplitude (μV)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot processed data (if available)\n",
    "        if self.processed_data is not None:\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(time_axis, self.processed_data[channel_idx, :n_samples], 'r-', alpha=0.7)\n",
    "            plt.title(f'Filtered EEG - Channel {self.channels[channel_idx]}')\n",
    "            plt.ylabel('Amplitude (μV)')\n",
    "            plt.xlabel('Time (seconds)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demo the class\n",
    "processor = EEGProcessor(sampling_rate=250)\n",
    "\n",
    "# Simulate some EEG data with noise\n",
    "time_points = np.linspace(0, 10, 2500)  # 10 seconds at 250 Hz\n",
    "n_channels = len(processor.channels)\n",
    "\n",
    "# Create realistic EEG-like signals\n",
    "fake_eeg = np.zeros((n_channels, len(time_points)))\n",
    "for i in range(n_channels):\n",
    "    # Alpha waves (10 Hz) + noise + some artifacts\n",
    "    alpha_wave = 2 * np.sin(2 * np.pi * 10 * time_points + i * 0.5)\n",
    "    theta_wave = 1 * np.sin(2 * np.pi * 6 * time_points + i * 0.3)\n",
    "    noise = 0.5 * np.random.randn(len(time_points))\n",
    "    artifacts = 10 * np.sin(2 * np.pi * 60 * time_points)  # 60 Hz line noise\n",
    "    \n",
    "    fake_eeg[i] = alpha_wave + theta_wave + noise + artifacts\n",
    "\n",
    "# Process the data\n",
    "processor.load_data(fake_eeg)\n",
    "processor.apply_bandpass_filter(low_freq=1, high_freq=40)\n",
    "processor.plot_channel(channel_idx=0, duration=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔢 NumPy Fundamentals: The Heart of Scientific Computing\n",
    "\n",
    "NumPy is the foundation of scientific computing in Python. In neuroscience, you'll use it for everything from basic array operations to complex signal processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Creation and Basic Operations 🎯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Different ways to create arrays (common in neuroscience)\n\n# 1. Time axis for signals\nsampling_rate = 250  # Hz\nduration = 2.0  # seconds\ntime_axis = np.linspace(0, duration, int(sampling_rate * duration))\nprint(f\"Time axis: {time_axis[:5]}...{time_axis[-5:]}\")\nprint(f\"Shape: {time_axis.shape}\")\n\n# 2. Initialize arrays for data storage\nn_channels = 64\nn_timepoints = 1000\nn_trials = 100\n\n# Empty array for EEG data (channels × time × trials)\neeg_data = np.zeros((n_channels, n_timepoints, n_trials))\nprint(f\"\\nEEG data array shape: {eeg_data.shape}\")\n\n# Random data (useful for testing)\nrandom_signals = np.random.randn(n_channels, n_timepoints)\nprint(f\"Random signals shape: {random_signals.shape}\")\n\n# 3. Structured arrays for experiments\ntrial_conditions = np.array(['rest', 'task', 'rest', 'task'] * 25)\nprint(f\"\\nTrial conditions (first 10): {trial_conditions[:10]}\")\n\n# 4. Frequency arrays for spectral analysis\nfrequencies = np.fft.fftfreq(n_timepoints, 1/sampling_rate)\npositive_freqs = frequencies[:n_timepoints//2]\nprint(f\"\\nFrequency range: {positive_freqs[0]:.1f} to {positive_freqs[-1]:.1f} Hz\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Indexing and Slicing: Extracting What You Need 🎯\n",
    "\n",
    "In neuroscience, you'll constantly need to extract specific time windows, channels, or trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample EEG data (channels × time × trials)\nnp.random.seed(42)  # For reproducible results\nn_channels, n_timepoints, n_trials = 10, 1000, 50\neeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n\n# Add some structure to make it more realistic\ntime_axis = np.linspace(0, 4, n_timepoints)  # 4 seconds\nfor ch in range(n_channels):\n    for trial in range(n_trials):\n        # Add alpha waves (10 Hz) with some variability\n        alpha_freq = 10 + np.random.normal(0, 1)\n        eeg_data[ch, :, trial] += 2 * np.sin(2 * np.pi * alpha_freq * time_axis)\n\nprint(f\"EEG data shape: {eeg_data.shape}\")\nprint(f\"Data type: {eeg_data.dtype}\")\nprint(f\"Memory usage: {eeg_data.nbytes / 1024:.1f} KB\")\n\n# Common indexing operations\nprint(\"\\n=== Common Indexing Operations ===\")\n\n# 1. Select specific channels\nfrontal_channels = [0, 1, 2]  # Assuming these are frontal\nfrontal_data = eeg_data[frontal_channels, :, :]\nprint(f\"1. Frontal channels data shape: {frontal_data.shape}\")\n\n# 2. Select time window (e.g., 1-3 seconds)\nstart_time, end_time = 1.0, 3.0\nstart_idx = int(start_time * (n_timepoints / 4))  # 4 seconds total\nend_idx = int(end_time * (n_timepoints / 4))\ntime_window = eeg_data[:, start_idx:end_idx, :]\nprint(f\"2. Time window (1-3s) shape: {time_window.shape}\")\n\n# 3. Select specific trials\ntask_trials = [1, 3, 5, 7, 9]  # Assuming these are task trials\ntask_data = eeg_data[:, :, task_trials]\nprint(f\"3. Task trials data shape: {task_data.shape}\")\n\n# 4. Complex indexing: frontal channels, specific time window, task trials\n# First select channels, then time window, then trials\nsubset = eeg_data[np.ix_(frontal_channels, range(start_idx, end_idx), task_trials)]\nprint(f\"4. Complex subset shape: {subset.shape}\")\n\n# Alternative approach using step-by-step indexing\n# subset_step1 = eeg_data[frontal_channels, :, :]  # Select channels\n# subset_step2 = subset_step1[:, start_idx:end_idx, :]  # Select time window\n# subset_final = subset_step2[:, :, task_trials]  # Select trials\n# print(f\"4. Complex subset shape (alternative): {subset_final.shape}\")\n\n# 5. Boolean indexing (very powerful!)\n# Find time points where signal is above threshold\nchannel_0_trial_0 = eeg_data[0, :, 0]\nhigh_amplitude_mask = np.abs(channel_0_trial_0) > 2\nhigh_amplitude_points = channel_0_trial_0[high_amplitude_mask]\nprint(f\"5. High amplitude points: {len(high_amplitude_points)} out of {n_timepoints}\")\n\n# 6. Advanced: condition-based selection\n# Get trials where the mean amplitude is above median\ntrial_means = np.mean(eeg_data, axis=(0, 1))  # Mean across channels and time\nhigh_activity_trials = trial_means > np.median(trial_means)\nhigh_activity_data = eeg_data[:, :, high_activity_trials]\nprint(f\"6. High activity trials: {high_activity_data.shape[2]} out of {n_trials}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Operations: The Power of Vectorization ⚡\n",
    "\n",
    "Vectorized operations are much faster than Python loops. This is crucial when processing large neuroscience datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🚀 Performance & Optimization: Critical for Large Neuroscience Datasets\n\nimport time\nimport numpy as np\nfrom functools import wraps\n\nprint(\"⚡ Performance Optimization for Neuroscience Computing\")\nprint(\"=\" * 55)\n\n# Performance testing decorator\ndef time_it(func):\n    \"\"\"Decorator to time function execution\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"⏱️ {func.__name__}: {end_time - start_time:.4f} seconds\")\n        return result\n    return wrapper\n\n# Memory usage tracking\ndef track_memory(func):\n    \"\"\"Decorator to track memory usage\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            import psutil\n            import os\n            process = psutil.Process(os.getpid())\n            mem_before = process.memory_info().rss / 1024**2  # MB\n            \n            result = func(*args, **kwargs)\n            \n            mem_after = process.memory_info().rss / 1024**2  # MB\n            print(f\"💾 {func.__name__}: {mem_after - mem_before:.1f} MB memory increase\")\n            return result\n        except ImportError:\n            return func(*args, **kwargs)\n    return wrapper\n\nprint(\"\\n1. 🐌 LOOP VS ⚡ VECTORIZED OPERATIONS\")\nprint(\"=\" * 45)\n\n# Generate realistic EEG-like data\nn_samples = 100000\neeg_signal = np.random.randn(n_samples) * 50  # 50 μV typical EEG amplitude\n\nprint(f\"Testing with {n_samples:,} EEG samples...\")\n\n# Method 1: Python loop (slow)\n@time_it\ndef apply_filter_loop(signal):\n    \"\"\"Apply a simple moving average filter using Python loops\"\"\"\n    filtered = []\n    window_size = 5\n    \n    for i in range(len(signal)):\n        if i < window_size:\n            filtered.append(signal[i])\n        else:\n            # Moving average\n            avg = sum(signal[i-window_size:i]) / window_size\n            filtered.append(avg)\n    \n    return np.array(filtered)\n\n# Method 2: NumPy vectorized (fast)\n@time_it\ndef apply_filter_vectorized(signal):\n    \"\"\"Apply moving average filter using NumPy convolution\"\"\"\n    window_size = 5\n    kernel = np.ones(window_size) / window_size\n    # Use 'same' mode to maintain original length\n    filtered = np.convolve(signal, kernel, mode='same')\n    return filtered\n\n# Method 3: SciPy optimized (fastest)\n@time_it\ndef apply_filter_scipy(signal):\n    \"\"\"Apply moving average using SciPy's optimized functions\"\"\"\n    from scipy import ndimage\n    filtered = ndimage.uniform_filter1d(signal, size=5, mode='constant')\n    return filtered\n\nprint(\"\\n🏁 Speed Comparison:\")\nresult_loop = apply_filter_loop(eeg_signal)\nresult_vectorized = apply_filter_vectorized(eeg_signal)\nresult_scipy = apply_filter_scipy(eeg_signal)\n\n# Verify results are similar\nprint(f\"\\n🔍 Results validation:\")\nprint(f\"   Loop vs Vectorized max difference: {np.max(np.abs(result_loop - result_vectorized)):.6f}\")\nprint(f\"   Vectorized vs SciPy max difference: {np.max(np.abs(result_vectorized - result_scipy)):.6f}\")\n\nprint(\"\\n\\n2. 💾 MEMORY OPTIMIZATION\")\nprint(\"=\" * 30)\n\n@track_memory\n@time_it\ndef memory_inefficient_analysis(n_channels=64, n_timepoints=10000, n_trials=100):\n    \"\"\"Demonstrate memory-inefficient approach\"\"\"\n    \n    # Create large dataset\n    eeg_data = np.random.randn(n_channels, n_timepoints, n_trials) * 50\n    \n    # Inefficient: Create many intermediate arrays\n    step1 = eeg_data ** 2  # Power\n    step2 = np.sqrt(step1)  # Back to amplitude\n    step3 = step1 + step2  # Combine\n    step4 = np.mean(step3, axis=1)  # Average over time\n    step5 = np.std(step4, axis=1)   # Standard deviation over trials\n    \n    return step5\n\n@track_memory\n@time_it\ndef memory_efficient_analysis(n_channels=64, n_timepoints=10000, n_trials=100):\n    \"\"\"Demonstrate memory-efficient approach\"\"\"\n    \n    # Create dataset\n    eeg_data = np.random.randn(n_channels, n_timepoints, n_trials) * 50\n    \n    # Efficient: Chain operations without intermediate storage\n    result = np.std(np.mean(eeg_data**2 + np.abs(eeg_data), axis=1), axis=1)\n    \n    return result\n\nprint(\"\\n🧪 Memory Usage Comparison:\")\nprint(\"Inefficient approach:\")\nresult1 = memory_inefficient_analysis(32, 5000, 50)  # Smaller for demo\n\nprint(\"\\nEfficient approach:\")\nresult2 = memory_efficient_analysis(32, 5000, 50)\n\nprint(f\"\\nResults are similar: {np.allclose(result1, result2)}\")\n\nprint(\"\\n\\n3. 🔄 PARALLEL PROCESSING\")\nprint(\"=\" * 25)\n\ndef process_single_channel(channel_data):\n    \"\"\"Process a single EEG channel (example: power spectral density)\"\"\"\n    from scipy.signal import welch\n    frequencies, psd = welch(channel_data, fs=250, nperseg=256)\n    # Return alpha power (8-13 Hz)\n    alpha_mask = (frequencies >= 8) & (frequencies <= 13)\n    alpha_power = np.trapz(psd[alpha_mask], frequencies[alpha_mask])\n    return alpha_power\n\n@time_it\ndef sequential_processing(eeg_data):\n    \"\"\"Process channels sequentially\"\"\"\n    results = []\n    for channel in eeg_data:\n        alpha_power = process_single_channel(channel)\n        results.append(alpha_power)\n    return np.array(results)\n\n@time_it\ndef parallel_processing(eeg_data):\n    \"\"\"Process channels in parallel\"\"\"\n    try:\n        from multiprocessing import Pool\n        import os\n        \n        # Use half the available cores to avoid overloading\n        n_cores = max(1, os.cpu_count() // 2)\n        \n        with Pool(processes=n_cores) as pool:\n            results = pool.map(process_single_channel, eeg_data)\n        \n        return np.array(results)\n        \n    except Exception as e:\n        print(f\"   ⚠️ Parallel processing failed: {e}\")\n        print(\"   📝 Falling back to sequential processing\")\n        return sequential_processing(eeg_data)\n\n# Test with moderate-sized data\ntest_eeg = np.random.randn(32, 2500) * 50  # 32 channels, 10 seconds at 250 Hz\n\nprint(\"\\n🏁 Processing Speed Comparison:\")\nprint(\"Sequential processing:\")\nseq_result = sequential_processing(test_eeg)\n\nprint(\"Parallel processing:\")\npar_result = parallel_processing(test_eeg)\n\nprint(f\"\\nResults match: {np.allclose(seq_result, par_result)}\")\n\nprint(\"\\n\\n4. 📊 PROFILING YOUR CODE\")\nprint(\"=\" * 30)\n\ndef profile_function(func, *args, **kwargs):\n    \"\"\"Simple profiling function\"\"\"\n    import cProfile\n    import pstats\n    import io\n    \n    # Create a profiler\n    profiler = cProfile.Profile()\n    \n    # Profile the function\n    profiler.enable()\n    result = func(*args, **kwargs)\n    profiler.disable()\n    \n    # Get stats\n    stats_stream = io.StringIO()\n    ps = pstats.Stats(profiler, stream=stats_stream)\n    ps.sort_stats('cumulative')\n    ps.print_stats(10)  # Top 10 functions\n    \n    print(\"📈 Top time-consuming functions:\")\n    print(stats_stream.getvalue())\n    \n    return result\n\ndef complex_eeg_analysis(eeg_data):\n    \"\"\"A more complex analysis function to profile\"\"\"\n    \n    # Bandpass filtering simulation\n    filtered = np.zeros_like(eeg_data)\n    for i, channel in enumerate(eeg_data):\n        # Simple high-pass filter (remove DC)\n        filtered[i] = channel - np.mean(channel)\n    \n    # Compute power in different bands\n    from scipy.signal import welch\n    \n    power_bands = {}\n    bands = {'alpha': (8, 13), 'beta': (13, 30), 'theta': (4, 8)}\n    \n    for band_name, (low, high) in bands.items():\n        band_powers = []\n        for channel in filtered:\n            freqs, psd = welch(channel, fs=250, nperseg=256)\n            mask = (freqs >= low) & (freqs <= high)\n            power = np.trapz(psd[mask], freqs[mask])\n            band_powers.append(power)\n        power_bands[band_name] = np.array(band_powers)\n    \n    # Correlation analysis\n    correlation_matrix = np.corrcoef(filtered)\n    \n    # Feature extraction\n    features = np.concatenate([\n        power_bands['alpha'],\n        power_bands['beta'],\n        power_bands['theta'],\n        np.diag(correlation_matrix)\n    ])\n    \n    return features\n\n# Profile the complex analysis\nprint(\"🔍 Profiling complex EEG analysis...\")\nsample_data = np.random.randn(16, 1250) * 50  # 16 channels, 5 seconds\nprofiled_result = profile_function(complex_eeg_analysis, sample_data)\n\nprint(\"\\n\\n💡 OPTIMIZATION STRATEGIES\")\nprint(\"=\" * 35)\n\noptimization_tips = [\n    \"✅ Use NumPy/SciPy built-in functions instead of pure Python loops\",\n    \"✅ Minimize intermediate array creation with chained operations\", \n    \"✅ Choose appropriate data types (float32 vs float64)\",\n    \"✅ Use in-place operations when possible (+=, *=, etc.)\",\n    \"✅ Leverage parallel processing for embarrassingly parallel tasks\",\n    \"✅ Profile your code to identify actual bottlenecks\",\n    \"✅ Consider using numba for JIT compilation of critical functions\",\n    \"✅ Use memory mapping for very large datasets\",\n    \"✅ Batch operations to reduce function call overhead\",\n    \"✅ Cache expensive computations when reused\"\n]\n\nfor tip in optimization_tips:\n    print(f\"   {tip}\")\n\nprint(\"\\n📚 Performance Benchmarks Saved:\")\nprint(\"   - Loop vs vectorized speedup factor\")\nprint(\"   - Memory usage differences\")  \nprint(\"   - Parallel processing overhead\")\nprint(\"   - Function profiling results\")\n\nprint(\"\\n🎯 Key Takeaway: Always measure before optimizing!\")\nprint(\"   Use vectorized operations for 10-100x speedups in neuroscience computing.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Operations for Neuroscience 🧮\n",
    "\n",
    "Let's explore the mathematical operations you'll use most frequently in brain signal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample multi-channel EEG data\nnp.random.seed(123)\nn_channels, n_timepoints, n_trials = 8, 500, 20\neeg_data = np.random.randn(n_channels, n_timepoints, n_trials)\n\n# Add realistic signal structure\ntime_axis = np.linspace(0, 2, n_timepoints)  # 2 seconds\nfor ch in range(n_channels):\n    for trial in range(n_trials):\n        # Add alpha rhythm (10 Hz) and some theta (6 Hz)\n        alpha = 3 * np.sin(2 * np.pi * 10 * time_axis + ch * 0.2)\n        theta = 1.5 * np.sin(2 * np.pi * 6 * time_axis + ch * 0.1)\n        eeg_data[ch, :, trial] += alpha + theta\n\nprint(f\"Sample EEG data shape: {eeg_data.shape}\")\nprint(f\"Data range: {eeg_data.min():.2f} to {eeg_data.max():.2f}\")\n\n# === Statistical Operations ===\nprint(\"\\n=== Statistical Operations ===\")\n\n# 1. Basic statistics across different axes\ntrial_averages = np.mean(eeg_data, axis=2)  # Average across trials\nchannel_averages = np.mean(eeg_data, axis=0)  # Average across channels\ntime_averages = np.mean(eeg_data, axis=1)  # Average across time\n\nprint(f\"Trial averages shape: {trial_averages.shape}\")\nprint(f\"Channel averages shape: {channel_averages.shape}\")\nprint(f\"Time averages shape: {time_averages.shape}\")\n\n# 2. Standard deviation and variance\ntrial_std = np.std(eeg_data, axis=2)\nprint(f\"\\nStandard deviation across trials: {trial_std.mean():.3f}\")\n\n# 3. Percentiles (useful for outlier detection)\npercentiles = np.percentile(eeg_data, [5, 25, 50, 75, 95])\nprint(f\"Data percentiles: {percentiles}\")\n\n# === Signal Processing Operations ===\nprint(\"\\n=== Signal Processing Operations ===\")\n\n# 1. Z-score normalization (common preprocessing step)\neeg_normalized = (eeg_data - np.mean(eeg_data, axis=1, keepdims=True)) / np.std(eeg_data, axis=1, keepdims=True)\nprint(f\"Normalized data mean: {np.mean(eeg_normalized):.6f}\")\nprint(f\"Normalized data std: {np.std(eeg_normalized):.6f}\")\n\n# 2. Baseline correction (subtract pre-stimulus period)\nbaseline_period = slice(0, 50)  # First 50 time points\nbaseline_mean = np.mean(eeg_data[:, baseline_period, :], axis=1, keepdims=True)\neeg_baseline_corrected = eeg_data - baseline_mean\nprint(f\"Baseline corrected data range: {eeg_baseline_corrected.min():.2f} to {eeg_baseline_corrected.max():.2f}\")\n\n# 3. Root Mean Square (RMS) - measure of signal power\nrms_values = np.sqrt(np.mean(eeg_data**2, axis=1))\nprint(f\"RMS values shape: {rms_values.shape}\")\nprint(f\"Average RMS across channels: {np.mean(rms_values):.3f}\")\n\n# === Advanced Operations ===\nprint(\"\\n=== Advanced Operations ===\")\n\n# 1. Correlation between channels\n# Take first trial, correlate channels across time\nchannel_correlations = np.corrcoef(eeg_data[:, :, 0])\nprint(f\"Channel correlation matrix shape: {channel_correlations.shape}\")\nprint(f\"Average correlation: {np.mean(channel_correlations[np.triu_indices(n_channels, k=1)]):.3f}\")\n\n# 2. Covariance matrix\ncov_matrix = np.cov(eeg_data[:, :, 0])\nprint(f\"Covariance matrix shape: {cov_matrix.shape}\")\n\n# 3. Principal Component Analysis (PCA) preparation\n# Center the data\ndata_centered = eeg_data - np.mean(eeg_data, axis=1, keepdims=True)\n# Compute covariance matrix\ncov_temporal = np.cov(data_centered[:, :, 0].T)  # Time x Time covariance\neigenvalues, eigenvectors = np.linalg.eig(cov_temporal)\nprint(f\"Top 5 eigenvalues: {np.sort(eigenvalues)[-5:][::-1]}\")\n\n# 4. Sliding window analysis\nwindow_size = 50\nstep_size = 10\nwindows = []\nfor start in range(0, n_timepoints - window_size, step_size):\n    window_data = eeg_data[:, start:start+window_size, :]\n    window_mean = np.mean(window_data, axis=1)  # Average across time in window\n    windows.append(window_mean)\n\nwindowed_analysis = np.array(windows)\nprint(f\"Windowed analysis shape: {windowed_analysis.shape}\")\nprint(f\"Number of windows: {len(windows)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra for Neural Networks 🧠\n",
    "\n",
    "Understanding matrix operations is crucial for neural networks and many signal processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Matrix Operations Fundamental to Neural Networks ===\n",
    "print(\"=== Matrix Operations for Neural Networks ===\")\n",
    "\n",
    "# 1. Basic matrix multiplication (the heart of neural networks)\n",
    "# Simulate a simple neural network layer\n",
    "input_features = 64  # e.g., 64 EEG channels\n",
    "hidden_units = 32\n",
    "batch_size = 10\n",
    "\n",
    "# Input data (batch_size × input_features)\n",
    "X = np.random.randn(batch_size, input_features)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "\n",
    "# Weight matrix (input_features × hidden_units)\n",
    "W = np.random.randn(input_features, hidden_units) * 0.1  # Small random weights\n",
    "print(f\"Weight W shape: {W.shape}\")\n",
    "\n",
    "# Bias vector (hidden_units,)\n",
    "b = np.zeros(hidden_units)\n",
    "print(f\"Bias b shape: {b.shape}\")\n",
    "\n",
    "# Forward pass: linear transformation\n",
    "z = np.dot(X, W) + b  # or X @ W + b\n",
    "print(f\"Linear output z shape: {z.shape}\")\n",
    "\n",
    "# Activation function (ReLU)\n",
    "a = np.maximum(0, z)\n",
    "print(f\"Activated output a shape: {a.shape}\")\n",
    "print(f\"Activated units: {np.sum(a > 0)} out of {a.size}\")\n",
    "\n",
    "# 2. Batch operations (processing multiple samples simultaneously)\n",
    "print(\"\\n=== Batch Operations ===\")\n",
    "\n",
    "# Simulate processing multiple EEG epochs\n",
    "n_epochs = 100\n",
    "n_channels = 10\n",
    "n_timepoints = 250\n",
    "\n",
    "# Create batch of EEG data\n",
    "eeg_batch = np.random.randn(n_epochs, n_channels, n_timepoints)\n",
    "print(f\"EEG batch shape: {eeg_batch.shape}\")\n",
    "\n",
    "# Flatten each epoch for neural network input\n",
    "eeg_flattened = eeg_batch.reshape(n_epochs, -1)\n",
    "print(f\"Flattened EEG shape: {eeg_flattened.shape}\")\n",
    "\n",
    "# Apply transformation to entire batch\n",
    "feature_weights = np.random.randn(n_channels * n_timepoints, 50)\n",
    "features = np.dot(eeg_flattened, feature_weights)\n",
    "print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "# 3. Covariance and correlation matrices (important for connectivity analysis)\n",
    "print(\"\\n=== Covariance and Correlation ===\")\n",
    "\n",
    "# Simulate multi-channel EEG data\n",
    "n_channels = 5\n",
    "n_timepoints = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create correlated channels (simulate brain connectivity)\n",
    "base_signal = np.random.randn(n_timepoints)\n",
    "channels = np.zeros((n_channels, n_timepoints))\n",
    "\n",
    "for i in range(n_channels):\n",
    "    # Each channel is base signal + independent noise\n",
    "    channels[i] = 0.7 * base_signal + 0.3 * np.random.randn(n_timepoints)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = np.corrcoef(channels)\n",
    "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
    "print(f\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Compute covariance matrix\n",
    "covariance_matrix = np.cov(channels)\n",
    "print(f\"\\nCovariance matrix shape: {covariance_matrix.shape}\")\n",
    "\n",
    "# 4. Eigendecomposition (used in PCA, ICA, etc.)\n",
    "print(\"\\n=== Eigendecomposition ===\")\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# Sort by eigenvalue magnitude\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"Sorted eigenvalues: {eigenvalues_sorted}\")\n",
    "print(f\"Explained variance ratio: {eigenvalues_sorted / np.sum(eigenvalues_sorted)}\")\n",
    "\n",
    "# 5. Matrix norms (useful for regularization)\n",
    "print(\"\\n=== Matrix Norms ===\")\n",
    "\n",
    "sample_matrix = np.random.randn(5, 5)\n",
    "print(f\"Frobenius norm: {np.linalg.norm(sample_matrix, 'fro'):.3f}\")\n",
    "print(f\"Nuclear norm: {np.linalg.norm(sample_matrix, 'nuc'):.3f}\")\n",
    "print(f\"Spectral norm: {np.linalg.norm(sample_matrix, 2):.3f}\")\n",
    "\n",
    "# 6. Solving linear systems (used in many algorithms)\n",
    "print(\"\\n=== Solving Linear Systems ===\")\n",
    "\n",
    "# Example: least squares solution\n",
    "A = np.random.randn(100, 10)  # Design matrix\n",
    "x_true = np.random.randn(10)  # True parameters\n",
    "y = A @ x_true + 0.1 * np.random.randn(100)  # Noisy observations\n",
    "\n",
    "# Solve least squares: x = (A^T A)^(-1) A^T y\n",
    "x_estimated = np.linalg.solve(A.T @ A, A.T @ y)\n",
    "print(f\"True parameters: {x_true[:5]}\")\n",
    "print(f\"Estimated parameters: {x_estimated[:5]}\")\n",
    "print(f\"Estimation error: {np.linalg.norm(x_true - x_estimated):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧬 Real-World Example: EEG Signal Analysis\n",
    "\n",
    "Let's put everything together with a realistic neuroscience example: analyzing EEG signals to detect different brain states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🏥 Real-World Clinical Applications & Case Studies\n\nprint(\"🌍 Real-World Applications of Python & NumPy in Neuroscience\")\nprint(\"=\" * 60)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nimport time\n\nprint(\"🎯 Learning Objective: Connect programming skills to actual clinical practice\")\nprint(\"📊 See how the techniques you've learned solve real neuroscience problems\")\n\n# Case Study 1: Seizure Detection Algorithm\nprint(\"\\n\\n🚨 CASE STUDY 1: Epileptic Seizure Detection\")\nprint(\"=\" * 50)\n\ndef simulate_seizure_detection():\n    \"\"\"Simulate a basic seizure detection algorithm using NumPy\"\"\"\n    \n    print(\"🏥 Clinical Context:\")\n    print(\"   - Epilepsy affects 65 million people worldwide\")\n    print(\"   - Early seizure detection can trigger interventions\")\n    print(\"   - Automated detection reduces monitoring burden\")\n    print(\"   - Real-time processing requires efficient algorithms\")\n    \n    # Simulate EEG data with seizure activity\n    sampling_rate = 250  # Hz\n    duration = 60  # seconds (1 minute recording)\n    n_channels = 19  # Standard 10-20 EEG montage\n    \n    time_axis = np.linspace(0, duration, sampling_rate * duration)\n    \n    # Normal background EEG (alpha + noise)\n    normal_eeg = np.zeros((n_channels, len(time_axis)))\n    for ch in range(n_channels):\n        # Alpha waves with some variation\n        alpha = 2 * np.sin(2 * np.pi * (10 + np.random.normal(0, 0.5)) * time_axis)\n        noise = 1.5 * np.random.randn(len(time_axis))\n        normal_eeg[ch] = alpha + noise\n    \n    # Add seizure activity (high amplitude, synchronized oscillations)\n    seizure_start = 30  # seconds\n    seizure_duration = 15  # seconds\n    seizure_start_idx = int(seizure_start * sampling_rate)\n    seizure_end_idx = int((seizure_start + seizure_duration) * sampling_rate)\n    \n    seizure_eeg = normal_eeg.copy()\n    \n    # Create seizure pattern - high frequency, high amplitude, synchronized\n    seizure_time = time_axis[seizure_start_idx:seizure_end_idx]\n    seizure_freq = 15  # Hz (typical seizure frequency)\n    \n    for ch in range(n_channels):\n        # Synchronized seizure activity across channels\n        phase_shift = ch * 0.1  # Small phase differences\n        seizure_pattern = 8 * np.sin(2 * np.pi * seizure_freq * seizure_time + phase_shift)\n        \n        # Add harmonics for realism\n        seizure_pattern += 4 * np.sin(2 * np.pi * 2 * seizure_freq * seizure_time + phase_shift)\n        \n        seizure_eeg[ch, seizure_start_idx:seizure_end_idx] += seizure_pattern\n    \n    return seizure_eeg, time_axis, (seizure_start, seizure_start + seizure_duration)\n\ndef seizure_detection_algorithm(eeg_data, sampling_rate=250, window_size=5):\n    \"\"\"\n    Simple seizure detection based on:\n    1. High amplitude detection\n    2. Spectral power analysis  \n    3. Cross-channel synchronization\n    \"\"\"\n    \n    n_channels, n_timepoints = eeg_data.shape\n    window_samples = int(window_size * sampling_rate)\n    n_windows = n_timepoints // window_samples\n    \n    detection_scores = []\n    \n    for w in range(n_windows):\n        start_idx = w * window_samples\n        end_idx = start_idx + window_samples\n        window_data = eeg_data[:, start_idx:end_idx]\n        \n        # Feature 1: High amplitude detection\n        amplitude_score = np.mean(np.abs(window_data))\n        \n        # Feature 2: Spectral power in seizure band (10-25 Hz)\n        freqs, psd = signal.welch(window_data.mean(axis=0), fs=sampling_rate, nperseg=128)\n        seizure_band_mask = (freqs >= 10) & (freqs <= 25)\n        spectral_score = np.trapz(psd[seizure_band_mask], freqs[seizure_band_mask])\n        \n        # Feature 3: Cross-channel synchronization\n        correlations = np.corrcoef(window_data)\n        sync_score = np.mean(correlations[np.triu_indices_from(correlations, k=1)])\n        \n        # Combine features (simple weighted sum)\n        combined_score = (amplitude_score * 0.4 + \n                         spectral_score * 0.4 + \n                         sync_score * 0.2)\n        \n        detection_scores.append(combined_score)\n    \n    return np.array(detection_scores)\n\n# Run seizure detection simulation\nseizure_eeg, time_axis, seizure_period = simulate_seizure_detection()\ndetection_scores = seizure_detection_algorithm(seizure_eeg)\n\n# Visualize results\nfig, axes = plt.subplots(3, 1, figsize=(15, 10))\n\n# Plot raw EEG (selected channels)\nselected_channels = [0, 5, 10]\nfor i, ch in enumerate(selected_channels):\n    axes[0].plot(time_axis, seizure_eeg[ch] + i*20, label=f'Channel {ch}', alpha=0.8)\n\naxes[0].axvspan(seizure_period[0], seizure_period[1], alpha=0.3, color='red', label='Actual Seizure')\naxes[0].set_ylabel('Amplitude (μV)')\naxes[0].set_title('Simulated EEG with Seizure Activity')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot detection scores\nwindow_times = np.linspace(0, 60, len(detection_scores))\naxes[1].plot(window_times, detection_scores, 'b-', linewidth=2, label='Detection Score')\naxes[1].axhline(y=np.mean(detection_scores) + 2*np.std(detection_scores), \n                color='red', linestyle='--', label='Detection Threshold')\naxes[1].axvspan(seizure_period[0], seizure_period[1], alpha=0.3, color='red')\naxes[1].set_ylabel('Detection Score')\naxes[1].set_title('Seizure Detection Algorithm Output')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Plot spectral analysis\nfreqs, psd_normal = signal.welch(seizure_eeg[:, :7500].mean(axis=0), fs=250, nperseg=512)\nfreqs, psd_seizure = signal.welch(seizure_eeg[:, 7500:11250].mean(axis=0), fs=250, nperseg=512)\n\naxes[2].semilogy(freqs, psd_normal, label='Normal Period', alpha=0.8)\naxes[2].semilogy(freqs, psd_seizure, label='Seizure Period', alpha=0.8)\naxes[2].axvspan(10, 25, alpha=0.2, color='red', label='Seizure Band')\naxes[2].set_xlim(0, 40)\naxes[2].set_xlabel('Frequency (Hz)')\naxes[2].set_ylabel('Power Spectral Density')\naxes[2].set_title('Spectral Analysis: Normal vs Seizure')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate detection performance\nthreshold = np.mean(detection_scores) + 2*np.std(detection_scores)\ndetected_windows = detection_scores > threshold\nseizure_windows = (window_times >= seizure_period[0]) & (window_times <= seizure_period[1])\n\ntrue_positives = np.sum(detected_windows & seizure_windows)\nfalse_positives = np.sum(detected_windows & ~seizure_windows)\ntrue_negatives = np.sum(~detected_windows & ~seizure_windows)\nfalse_negatives = np.sum(~detected_windows & seizure_windows)\n\nsensitivity = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\nspecificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n\nprint(f\"\\n📊 Detection Performance:\")\nprint(f\"   Sensitivity (True Positive Rate): {sensitivity:.2f}\")\nprint(f\"   Specificity (True Negative Rate): {specificity:.2f}\")\nprint(f\"   True Positives: {true_positives}, False Positives: {false_positives}\")\nprint(f\"   True Negatives: {true_negatives}, False Negatives: {false_negatives}\")\n\nprint(\"\\n🏥 Clinical Impact:\")\nprint(\"   ✅ Automated seizure detection enables:\")\nprint(\"      • 24/7 monitoring without constant human oversight\")\nprint(\"      • Immediate alerts for medical intervention\")\nprint(\"      • Objective seizure counting for treatment evaluation\")\nprint(\"      • Early warning systems for seizure prediction\")\n\nsimulate_seizure_detection()\n\n# Case Study 2: Sleep Stage Classification\nprint(\"\\n\\n😴 CASE STUDY 2: Sleep Stage Classification\")\nprint(\"=\" * 50)\n\ndef simulate_sleep_analysis():\n    \"\"\"Simulate sleep stage classification using EEG features\"\"\"\n    \n    print(\"🏥 Clinical Context:\")\n    print(\"   - Sleep disorders affect 50-70 million US adults\")\n    print(\"   - Polysomnography requires manual scoring (expensive, time-consuming)\")\n    print(\"   - Automated staging improves diagnosis accessibility\")\n    print(\"   - Different sleep stages have distinct EEG signatures\")\n    \n    # Sleep stages and their characteristics\n    sleep_stages = {\n        'Wake': {'freq_bands': {'beta': 2.0, 'alpha': 1.5, 'theta': 0.5}, 'amplitude': 30},\n        'N1': {'freq_bands': {'theta': 2.0, 'alpha': 1.0, 'beta': 0.5}, 'amplitude': 40},\n        'N2': {'freq_bands': {'theta': 1.5, 'sigma': 2.5, 'delta': 1.0}, 'amplitude': 50},  # Sleep spindles\n        'N3': {'freq_bands': {'delta': 4.0, 'theta': 0.5}, 'amplitude': 80},  # Deep sleep\n        'REM': {'freq_bands': {'theta': 2.0, 'alpha': 1.0, 'beta': 1.5}, 'amplitude': 35}  # Similar to wake\n    }\n    \n    frequency_bands = {\n        'delta': 2,    # 0.5-4 Hz\n        'theta': 6,    # 4-8 Hz  \n        'alpha': 10,   # 8-13 Hz\n        'sigma': 12,   # 11-15 Hz (sleep spindles)\n        'beta': 20     # 13-30 Hz\n    }\n    \n    # Simulate 8 hours of sleep data\n    sampling_rate = 100  # Hz (lower for sleep studies)\n    epoch_duration = 30  # seconds per epoch (standard)\n    n_epochs = 8 * 60 * 60 // epoch_duration  # 960 epochs for 8 hours\n    epoch_samples = sampling_rate * epoch_duration\n    \n    # Realistic sleep progression\n    stage_sequence = (\n        ['Wake'] * 10 +           # 5 minutes wake\n        ['N1'] * 4 +              # 2 minutes N1\n        ['N2'] * 20 +             # 10 minutes N2\n        ['N3'] * 40 +             # 20 minutes N3 (deep sleep)\n        ['N2'] * 15 +             # Back to N2\n        ['REM'] * 15 +            # First REM period\n        (['N2'] * 30 + ['N3'] * 20 + ['N2'] * 10 + ['REM'] * 20) * 6  # Sleep cycles\n    )\n    \n    # Pad or truncate to match n_epochs\n    if len(stage_sequence) < n_epochs:\n        stage_sequence.extend(['N2'] * (n_epochs - len(stage_sequence)))\n    else:\n        stage_sequence = stage_sequence[:n_epochs]\n    \n    # Generate EEG data for each epoch\n    simulated_eeg = []\n    true_stages = []\n    \n    for epoch_idx, stage in enumerate(stage_sequence):\n        stage_info = sleep_stages[stage]\n        \n        # Generate epoch data\n        epoch_data = np.zeros(epoch_samples)\n        \n        # Add frequency components\n        time_epoch = np.linspace(0, epoch_duration, epoch_samples)\n        \n        for band, amplitude in stage_info['freq_bands'].items():\n            freq = frequency_bands[band]\n            epoch_data += amplitude * np.sin(2 * np.pi * freq * time_epoch + \n                                           np.random.uniform(0, 2*np.pi))\n        \n        # Add noise\n        noise_level = stage_info['amplitude'] * 0.3\n        epoch_data += noise_level * np.random.randn(epoch_samples)\n        \n        simulated_eeg.append(epoch_data)\n        true_stages.append(stage)\n    \n    return np.array(simulated_eeg), true_stages\n\ndef extract_sleep_features(eeg_epochs, sampling_rate=100):\n    \"\"\"Extract features for sleep stage classification\"\"\"\n    \n    features = []\n    \n    for epoch in eeg_epochs:\n        # Compute power spectral density\n        freqs, psd = signal.welch(epoch, fs=sampling_rate, nperseg=sampling_rate*4)\n        \n        # Extract band powers\n        band_powers = {}\n        bands = {\n            'delta': (0.5, 4),\n            'theta': (4, 8),\n            'alpha': (8, 13),\n            'sigma': (11, 15),  # Sleep spindles\n            'beta': (13, 30)\n        }\n        \n        for band_name, (low, high) in bands.items():\n            mask = (freqs >= low) & (freqs <= high)\n            band_powers[band_name] = np.trapz(psd[mask], freqs[mask])\n        \n        # Additional features\n        total_power = sum(band_powers.values())\n        relative_powers = {band: power/total_power for band, power in band_powers.items()}\n        \n        # Combine features\n        epoch_features = [\n            relative_powers['delta'],\n            relative_powers['theta'], \n            relative_powers['alpha'],\n            relative_powers['sigma'],\n            relative_powers['beta'],\n            band_powers['delta'] / band_powers['alpha'],  # Delta/Alpha ratio\n            np.var(epoch),  # Signal variance\n            np.mean(np.abs(epoch))  # Mean absolute amplitude\n        ]\n        \n        features.append(epoch_features)\n    \n    return np.array(features)\n\ndef classify_sleep_stages(features, true_stages):\n    \"\"\"Simple rule-based sleep stage classification\"\"\"\n    \n    predicted_stages = []\n    \n    for feature_vector in features:\n        delta_rel, theta_rel, alpha_rel, sigma_rel, beta_rel, delta_alpha_ratio, variance, amplitude = feature_vector\n        \n        # Simple decision rules (in practice, use machine learning)\n        if delta_rel > 0.5 and amplitude > 50:\n            stage = 'N3'  # Deep sleep: high delta\n        elif sigma_rel > 0.15 and delta_alpha_ratio > 2:\n            stage = 'N2'  # Sleep spindles present\n        elif theta_rel > 0.3 and alpha_rel < 0.2:\n            stage = 'N1'  # Transitional sleep\n        elif beta_rel > 0.2 and alpha_rel > 0.2 and amplitude < 40:\n            if theta_rel > 0.25:\n                stage = 'REM'  # REM: mixed frequencies, low amplitude\n            else:\n                stage = 'Wake'  # Awake: beta and alpha present\n        else:\n            stage = 'N2'  # Default to N2\n        \n        predicted_stages.append(stage)\n    \n    return predicted_stages\n\n# Run sleep analysis simulation\nprint(\"🔄 Generating 8 hours of simulated sleep EEG...\")\nsleep_eeg, true_stages = simulate_sleep_analysis()\n\nprint(\"🔄 Extracting sleep features...\")\nsleep_features = extract_sleep_features(sleep_eeg)\n\nprint(\"🔄 Classifying sleep stages...\")\npredicted_stages = classify_sleep_stages(sleep_features, true_stages)\n\n# Calculate accuracy\nstage_names = ['Wake', 'N1', 'N2', 'N3', 'REM']\naccuracy = np.mean([true == pred for true, pred in zip(true_stages, predicted_stages)])\n\nprint(f\"\\n📊 Classification Results:\")\nprint(f\"   Overall Accuracy: {accuracy:.2f}\")\n\n# Confusion matrix\nfrom collections import defaultdict\nconfusion_matrix = defaultdict(lambda: defaultdict(int))\n\nfor true, pred in zip(true_stages, predicted_stages):\n    confusion_matrix[true][pred] += 1\n\nprint(f\"\\n📋 Confusion Matrix:\")\nprint(f\"{'True\\\\Pred':<8}\", end=\"\")\nfor stage in stage_names:\n    print(f\"{stage:>6}\", end=\"\")\nprint()\n\nfor true_stage in stage_names:\n    print(f\"{true_stage:<8}\", end=\"\")\n    for pred_stage in stage_names:\n        count = confusion_matrix[true_stage][pred_stage]\n        print(f\"{count:>6}\", end=\"\")\n    print()\n\n# Visualize hypnogram\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8))\n\n# True hypnogram\nstage_to_num = {'Wake': 4, 'REM': 3, 'N1': 2, 'N2': 1, 'N3': 0}\ntrue_numeric = [stage_to_num[stage] for stage in true_stages]\npred_numeric = [stage_to_num[stage] for stage in predicted_stages]\n\ntime_hours = np.arange(len(true_stages)) * 0.5 / 60  # Convert epochs to hours\n\nax1.plot(time_hours, true_numeric, 'b-', linewidth=2, label='True Stages')\nax1.set_ylabel('Sleep Stage')\nax1.set_title('True Sleep Hypnogram')\nax1.set_yticks(list(stage_to_num.values()))\nax1.set_yticklabels(list(stage_to_num.keys()))\nax1.grid(True, alpha=0.3)\nax1.set_xlim(0, 8)\n\nax2.plot(time_hours, pred_numeric, 'r-', linewidth=2, label='Predicted Stages')\nax2.set_xlabel('Time (hours)')\nax2.set_ylabel('Sleep Stage')\nax2.set_title('Predicted Sleep Hypnogram')\nax2.set_yticks(list(stage_to_num.values()))\nax2.set_yticklabels(list(stage_to_num.keys()))\nax2.grid(True, alpha=0.3)\nax2.set_xlim(0, 8)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n🏥 Clinical Impact:\")\nprint(f\"   ✅ Automated sleep staging enables:\")\nprint(f\"      • Faster sleep study analysis (minutes vs hours)\")\nprint(f\"      • Consistent scoring across different technicians\")\nprint(f\"      • Large-scale sleep research studies\")\nprint(f\"      • Home sleep monitoring devices\")\nprint(f\"      • Early detection of sleep disorders\")\n\nsimulate_sleep_analysis()\n\nprint(\"\\n\\n🧠 CASE STUDY 3: Brain-Computer Interface (BCI)\")\nprint(\"=\" * 55)\n\ndef simulate_motor_imagery_bci():\n    \"\"\"Simulate a motor imagery BCI using NumPy for feature extraction\"\"\"\n    \n    print(\"🏥 Clinical Context:\")\n    print(\"   - BCIs help paralyzed patients control devices with thoughts\")\n    print(\"   - Motor imagery creates detectable EEG patterns\")\n    print(\"   - Real-time classification enables device control\")\n    print(\"   - NumPy enables fast feature computation for real-time use\")\n    \n    # Simulate motor imagery experiment\n    sampling_rate = 250  # Hz\n    trial_duration = 4    # seconds\n    n_trials_per_class = 40\n    n_channels = 22  # Motor cortex area\n    \n    # Motor imagery affects specific frequency bands\n    mu_band = (8, 12)      # Mu rhythm suppression\n    beta_band = (18, 26)   # Beta rhythm changes\n    \n    trials_left = []\n    trials_right = []\n    \n    # Generate left hand imagery trials\n    for trial in range(n_trials_per_class):\n        time_axis = np.linspace(0, trial_duration, sampling_rate * trial_duration)\n        trial_data = np.zeros((n_channels, len(time_axis)))\n        \n        for ch in range(n_channels):\n            # Base EEG\n            base_signal = 2 * np.random.randn(len(time_axis))\n            \n            # Motor imagery affects contralateral channels more\n            if ch < n_channels // 2:  # Right hemisphere (left hand)\n                # Mu rhythm suppression (Event-Related Desynchronization)\n                mu_suppression = -1.5 * np.sin(2 * np.pi * 10 * time_axis)\n                beta_enhancement = 1.0 * np.sin(2 * np.pi * 22 * time_axis)\n            else:  # Left hemisphere\n                mu_suppression = -0.5 * np.sin(2 * np.pi * 10 * time_axis)\n                beta_enhancement = 0.3 * np.sin(2 * np.pi * 22 * time_axis)\n            \n            trial_data[ch] = base_signal + mu_suppression + beta_enhancement\n        \n        trials_left.append(trial_data)\n    \n    # Generate right hand imagery trials\n    for trial in range(n_trials_per_class):\n        time_axis = np.linspace(0, trial_duration, sampling_rate * trial_duration)\n        trial_data = np.zeros((n_channels, len(time_axis)))\n        \n        for ch in range(n_channels):\n            base_signal = 2 * np.random.randn(len(time_axis))\n            \n            if ch >= n_channels // 2:  # Left hemisphere (right hand)\n                mu_suppression = -1.5 * np.sin(2 * np.pi * 10 * time_axis)\n                beta_enhancement = 1.0 * np.sin(2 * np.pi * 22 * time_axis)\n            else:  # Right hemisphere\n                mu_suppression = -0.5 * np.sin(2 * np.pi * 10 * time_axis)\n                beta_enhancement = 0.3 * np.sin(2 * np.pi * 22 * time_axis)\n            \n            trial_data[ch] = base_signal + mu_suppression + beta_enhancement\n        \n        trials_right.append(trial_data)\n    \n    return trials_left, trials_right\n\ndef extract_bci_features(trials, sampling_rate=250):\n    \"\"\"Extract Common Spatial Patterns (CSP) features for BCI\"\"\"\n    \n    features = []\n    \n    for trial in trials:\n        # Simple power spectral features (in practice, use CSP)\n        trial_features = []\n        \n        for ch in range(trial.shape[0]):\n            # Compute power in mu and beta bands\n            freqs, psd = signal.welch(trial[ch], fs=sampling_rate, nperseg=256)\n            \n            # Mu band power\n            mu_mask = (freqs >= 8) & (freqs <= 12)\n            mu_power = np.trapz(psd[mu_mask], freqs[mu_mask])\n            \n            # Beta band power  \n            beta_mask = (freqs >= 18) & (freqs <= 26)\n            beta_power = np.trapz(psd[beta_mask], freqs[beta_mask])\n            \n            trial_features.extend([mu_power, beta_power])\n        \n        features.append(trial_features)\n    \n    return np.array(features)\n\ndef classify_motor_imagery(features_left, features_right):\n    \"\"\"Simple classification using feature differences\"\"\"\n    \n    # Combine data\n    X = np.vstack([features_left, features_right])\n    y = np.hstack([np.zeros(len(features_left)), np.ones(len(features_right))])\n    \n    # Simple classification: compare hemisphere differences\n    left_hemisphere_features = X[:, :X.shape[1]//2]\n    right_hemisphere_features = X[:, X.shape[1]//2:]\n    \n    # Laterality index\n    hemisphere_diff = np.mean(left_hemisphere_features, axis=1) - np.mean(right_hemisphere_features, axis=1)\n    \n    # Threshold-based classification\n    threshold = np.median(hemisphere_diff)\n    predictions = (hemisphere_diff > threshold).astype(int)\n    \n    accuracy = np.mean(predictions == y)\n    \n    return predictions, accuracy, hemisphere_diff\n\n# Run BCI simulation\nprint(\"🔄 Simulating motor imagery BCI experiment...\")\ntrials_left, trials_right = simulate_motor_imagery_bci()\n\nprint(\"🔄 Extracting BCI features...\")\nfeatures_left = extract_bci_features(trials_left)\nfeatures_right = extract_bci_features(trials_right)\n\nprint(\"🔄 Training classifier...\")\npredictions, accuracy, laterality_scores = classify_motor_imagery(features_left, features_right)\n\nprint(f\"\\n📊 BCI Performance:\")\nprint(f\"   Classification Accuracy: {accuracy:.2f}\")\nprint(f\"   Chance Level: 0.50\")\nprint(f\"   Performance Above Chance: {accuracy > 0.6}\")\n\n# Visualize laterality patterns\nplt.figure(figsize=(12, 6))\n\n# Plot laterality scores\nall_labels = np.hstack([np.zeros(len(features_left)), np.ones(len(features_right))])\ncolors = ['blue' if label == 0 else 'red' for label in all_labels]\nlabels = ['Left Hand' if label == 0 else 'Right Hand' for label in all_labels]\n\nplt.scatter(range(len(laterality_scores)), laterality_scores, c=colors, alpha=0.6)\nplt.axhline(y=np.median(laterality_scores), color='black', linestyle='--', label='Decision Threshold')\nplt.xlabel('Trial Number')\nplt.ylabel('Laterality Score (L-R Hemisphere Difference)')\nplt.title('Motor Imagery BCI: Hemispheric Lateralization')\nplt.legend(['Left Hand Imagery', 'Right Hand Imagery', 'Decision Threshold'])\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\n🏥 Clinical Impact:\")\nprint(f\"   ✅ Motor imagery BCIs enable:\")\nprint(f\"      • Wheelchair control for paralyzed patients\")\nprint(f\"      • Computer cursor control via thoughts\")\nprint(f\"      • Robotic arm control for amputees\")\nprint(f\"      • Communication devices for locked-in syndrome\")\nprint(f\"      • Stroke rehabilitation through neurofeedback\")\n\nsimulate_motor_imagery_bci()\n\nprint(\"\\n\\n🌟 KEY TAKEAWAYS: NumPy in Clinical Practice\")\nprint(\"=\" * 55)\n\nkey_takeaways = [\n    \"🔬 Array operations enable real-time signal processing\",\n    \"📊 Vectorized computations make clinical algorithms feasible\", \n    \"🏥 Simple statistical methods solve complex medical problems\",\n    \"⚡ Performance optimization is critical for real-time applications\",\n    \"🧠 Mathematical concepts translate directly to patient care\",\n    \"📈 Feature extraction from signals enables automated diagnosis\",\n    \"🤖 Automated analysis improves accuracy and reduces costs\",\n    \"🎯 Pattern recognition helps detect subtle neurological changes\"\n]\n\nfor takeaway in key_takeaways:\n    print(f\"   {takeaway}\")\n\nprint(f\"\\n💡 Professional Development:\")\nprint(f\"   • These skills are directly applicable in clinical research\")\nprint(f\"   • Understanding the math makes you a better collaborator\")\nprint(f\"   • Performance optimization skills are valued in medical device companies\")\nprint(f\"   • Signal processing knowledge applies to many biosignals (ECG, EMG, fMRI)\")\n\nprint(f\"\\n🚀 Next Steps:\")\nprint(f\"   • Learn machine learning for more sophisticated classification\")\nprint(f\"   • Study digital signal processing for advanced filtering\")\nprint(f\"   • Explore deep learning for automatic feature discovery\")\nprint(f\"   • Practice with real datasets from public repositories\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Exploratory Data Analysis ===\nprint(\"=== Exploratory Data Analysis ===\")\n\n# 1. Visualize sample trials\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Plot sample eyes-closed trial\naxes[0, 0].plot(time_axis[:1000], eeg_data[6, :1000, 0])  # P3 channel, first 4 seconds\naxes[0, 0].set_title('Eyes Closed - P3 Channel')\naxes[0, 0].set_xlabel('Time (s)')\naxes[0, 0].set_ylabel('Amplitude (μV)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot sample eyes-open trial\naxes[0, 1].plot(time_axis[:1000], eeg_data[6, :1000, 35])  # P3 channel, eyes-open trial\naxes[0, 1].set_title('Eyes Open - P3 Channel')\naxes[0, 1].set_xlabel('Time (s)')\naxes[0, 1].set_ylabel('Amplitude (μV)')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 2. Power spectral density comparison\nfrom scipy.signal import welch\n\n# Compute PSD for both conditions\nfreqs, psd_closed = welch(eeg_data[6, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\nfreqs, psd_open = welch(eeg_data[6, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n\naxes[1, 0].semilogy(freqs, psd_closed, label='Eyes Closed', alpha=0.8)\naxes[1, 0].semilogy(freqs, psd_open, label='Eyes Open', alpha=0.8)\naxes[1, 0].set_xlim(0, 40)\naxes[1, 0].set_xlabel('Frequency (Hz)')\naxes[1, 0].set_ylabel('PSD (μV²/Hz)')\naxes[1, 0].set_title('Power Spectral Density - P3 Channel')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 3. Topographic map of alpha power\nalpha_band = (8, 12)\nalpha_indices = (freqs >= alpha_band[0]) & (freqs <= alpha_band[1])\n\nalpha_power_closed = np.zeros(n_channels)\nalpha_power_open = np.zeros(n_channels)\n\nfor ch in range(n_channels):\n    # Eyes closed\n    freqs_ch, psd_ch = welch(eeg_data[ch, :, :30].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n    alpha_power_closed[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n    \n    # Eyes open\n    freqs_ch, psd_ch = welch(eeg_data[ch, :, 30:].mean(axis=1), sampling_rate, nperseg=sampling_rate*2)\n    alpha_power_open[ch] = np.trapz(psd_ch[alpha_indices], freqs_ch[alpha_indices])\n\n# Bar plot of alpha power by channel\nx = np.arange(n_channels)\nwidth = 0.35\n\naxes[1, 1].bar(x - width/2, alpha_power_closed, width, label='Eyes Closed', alpha=0.8)\naxes[1, 1].bar(x + width/2, alpha_power_open, width, label='Eyes Open', alpha=0.8)\naxes[1, 1].set_xlabel('Channel')\naxes[1, 1].set_ylabel('Alpha Power (μV²)')\naxes[1, 1].set_title('Alpha Power by Channel (8-12 Hz)')\naxes[1, 1].set_xticks(x)\naxes[1, 1].set_xticklabels(channel_names, rotation=45)\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# === Statistical Analysis ===\nprint(\"\\n=== Statistical Analysis ===\")\n\n# Compare alpha power between conditions\nprint(\"Alpha power comparison:\")\nprint(f\"Eyes Closed - Mean: {alpha_power_closed.mean():.3f}, Std: {alpha_power_closed.std():.3f}\")\nprint(f\"Eyes Open - Mean: {alpha_power_open.mean():.3f}, Std: {alpha_power_open.std():.3f}\")\n\n# Effect size (Cohen's d)\npooled_std = np.sqrt(((alpha_power_closed.std()**2) + (alpha_power_open.std()**2)) / 2)\ncohens_d = (alpha_power_closed.mean() - alpha_power_open.mean()) / pooled_std\nprint(f\"Effect size (Cohen's d): {cohens_d:.3f}\")\n\n# Find channels with largest differences\nalpha_diff = alpha_power_closed - alpha_power_open\nmax_diff_channel = np.argmax(alpha_diff)\nprint(f\"\\nLargest alpha difference in channel: {channel_names[max_diff_channel]} ({alpha_diff[max_diff_channel]:.3f} μV²)\")\n\n# Correlation between channels\nprint(\"\\n=== Channel Connectivity Analysis ===\")\n\n# Compute average correlation matrices for each condition\ncorr_closed = np.zeros((n_channels, n_channels))\ncorr_open = np.zeros((n_channels, n_channels))\n\nfor trial in range(30):\n    corr_closed += np.corrcoef(eeg_data[:, :, trial])\n    corr_open += np.corrcoef(eeg_data[:, :, trial + 30])\n\ncorr_closed /= 30\ncorr_open /= 30\n\n# Plot correlation matrices\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nim1 = axes[0].imshow(corr_closed, cmap='coolwarm', vmin=-1, vmax=1)\naxes[0].set_title('Eyes Closed - Correlation Matrix')\naxes[0].set_xticks(range(n_channels))\naxes[0].set_yticks(range(n_channels))\naxes[0].set_xticklabels(channel_names, rotation=45)\naxes[0].set_yticklabels(channel_names)\nplt.colorbar(im1, ax=axes[0])\n\nim2 = axes[1].imshow(corr_open, cmap='coolwarm', vmin=-1, vmax=1)\naxes[1].set_title('Eyes Open - Correlation Matrix')\naxes[1].set_xticks(range(n_channels))\naxes[1].set_yticks(range(n_channels))\naxes[1].set_xticklabels(channel_names, rotation=45)\naxes[1].set_yticklabels(channel_names)\nplt.colorbar(im2, ax=axes[1])\n\n# Difference matrix\ncorr_diff = corr_closed - corr_open\nim3 = axes[2].imshow(corr_diff, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\naxes[2].set_title('Correlation Difference (Closed - Open)')\naxes[2].set_xticks(range(n_channels))\naxes[2].set_yticks(range(n_channels))\naxes[2].set_xticklabels(channel_names, rotation=45)\naxes[2].set_yticklabels(channel_names)\nplt.colorbar(im3, ax=axes[2])\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\navg_corr_closed = np.mean(corr_closed[np.triu_indices(n_channels, k=1)])\navg_corr_open = np.mean(corr_open[np.triu_indices(n_channels, k=1)])\n\nprint(f\"Average correlation - Eyes Closed: {avg_corr_closed:.3f}\")\nprint(f\"Average correlation - Eyes Open: {avg_corr_open:.3f}\")\nprint(f\"Difference: {avg_corr_closed - avg_corr_open:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🧠 Knowledge Checkpoints & Interactive Assessments\n\nprint(\"🎯 Learning Checkpoint: Test Your Understanding!\")\nprint(\"=\" * 50)\n\nimport numpy as np\nfrom IPython.display import display, HTML\n\ndef create_checkpoint(question, options, correct_answer, explanation):\n    \"\"\"Create an interactive checkpoint question\"\"\"\n    \n    print(f\"\\n❓ {question}\")\n    print(\"-\" * len(question))\n    \n    for i, option in enumerate(options, 1):\n        print(f\"{i}. {option}\")\n    \n    # Simple text-based interaction\n    print(f\"\\n💡 Think about your answer, then run the next cell to see the solution!\")\n    \n    return correct_answer, explanation\n\ndef reveal_answer(correct_answer, explanation):\n    \"\"\"Reveal the answer with explanation\"\"\"\n    print(f\"✅ Correct Answer: {correct_answer}\")\n    print(f\"📚 Explanation: {explanation}\")\n\n# Checkpoint 1: Array Indexing\nq1_correct, q1_explanation = create_checkpoint(\n    \"You have EEG data with shape (64, 1000, 100) representing (channels, time, trials). You want to select channels [0,1,2], all time points, and trials [10,20,30]. Which approach is correct?\",\n    [\n        \"eeg_data[[0,1,2], :, [10,20,30]]\",\n        \"eeg_data[np.ix_([0,1,2], range(1000), [10,20,30])]\",\n        \"eeg_data[0:3, :, 10:31:10]\",\n        \"eeg_data[(0,1,2), :, (10,20,30)]\"\n    ],\n    \"2. eeg_data[np.ix_([0,1,2], range(1000), [10,20,30])]\",\n    \"Using np.ix_ creates the proper mesh grid for advanced indexing with different list lengths on different axes. Option 1 would cause a broadcasting error.\"\n)\n\nprint(\"\\n\" + \"=\"*60)\n\n# Checkpoint 2: Performance \nq2_correct, q2_explanation = create_checkpoint(\n    \"Which operation will be fastest for computing the mean of a 1000×1000 array?\",\n    [\n        \"Using nested for loops in Python\",\n        \"Using np.mean() with the entire array\", \n        \"Using list comprehension with np.mean on each row\",\n        \"Using pandas DataFrame.mean()\"\n    ],\n    \"2. Using np.mean() with the entire array\",\n    \"NumPy's vectorized operations are optimized with compiled C code and SIMD instructions, making them 10-100x faster than pure Python loops.\"\n)\n\nprint(\"\\n\" + \"=\"*60)\n\n# Checkpoint 3: Neuroscience Application\nq3_correct, q3_explanation = create_checkpoint(\n    \"You're analyzing alpha waves (8-13 Hz) in EEG data sampled at 250 Hz. After computing the FFT, which frequency indices correspond to the alpha band?\",\n    [\n        \"indices 8 to 13\",\n        \"indices 16 to 26 (approximately)\", \n        \"indices 2 to 3.25\",\n        \"indices 32 to 52\"\n    ],\n    \"2. indices 16 to 26 (approximately)\",\n    \"For FFT with N samples at sampling rate fs, frequency resolution is fs/N. For alpha band 8-13 Hz at 250 Hz sampling: 8/(250/N) to 13/(250/N). With typical N=1024, this gives approximately indices 33-54, but with shorter windows, indices 16-26 is reasonable.\"\n)\n\nprint(\"\\n\" + \"=\"*60)\n\nprint(\"🏆 PRACTICAL CODING CHALLENGES\")\nprint(\"=\" * 35)\n\ndef challenge_1():\n    \"\"\"Challenge: Create and manipulate realistic EEG data\"\"\"\n    \n    print(\"\\n🥇 CHALLENGE 1: EEG Data Simulation\")\n    print(\"-\" * 35)\n    print(\"Task: Create realistic EEG data and extract features\")\n    print(\"\\nRequirements:\")\n    print(\"1. Generate 32 channels × 2500 timepoints × 20 trials\")\n    print(\"2. Add alpha waves (10 Hz) with amplitude 50 μV\")\n    print(\"3. Add noise with standard deviation 25 μV\") \n    print(\"4. Extract alpha power for each channel\")\n    print(\"5. Find the channel with highest alpha power\")\n    \n    print(\"\\n💻 Try to solve this yourself, then check the solution below!\")\n    \n    # Solution (students should try first)\n    print(\"\\n🔍 SOLUTION:\")\n    \n    # 1. Generate data structure\n    n_channels, n_timepoints, n_trials = 32, 2500, 20\n    sampling_rate = 250  # Hz\n    duration = n_timepoints / sampling_rate  # 10 seconds\n    \n    eeg_data = np.zeros((n_channels, n_timepoints, n_trials))\n    time_axis = np.linspace(0, duration, n_timepoints)\n    \n    # 2. Add alpha waves and noise\n    for trial in range(n_trials):\n        for channel in range(n_channels):\n            # Alpha wave with slight frequency variation\n            alpha_freq = 10 + np.random.normal(0, 0.5)\n            alpha_signal = 50 * np.sin(2 * np.pi * alpha_freq * time_axis)\n            \n            # Add noise\n            noise = 25 * np.random.randn(n_timepoints)\n            \n            eeg_data[channel, :, trial] = alpha_signal + noise\n    \n    # 3. Extract alpha power using Welch's method\n    from scipy.signal import welch\n    \n    alpha_powers = []\n    for channel in range(n_channels):\n        channel_powers = []\n        for trial in range(n_trials):\n            freqs, psd = welch(eeg_data[channel, :, trial], fs=sampling_rate, nperseg=512)\n            alpha_mask = (freqs >= 8) & (freqs <= 13)\n            alpha_power = np.trapz(psd[alpha_mask], freqs[alpha_mask])\n            channel_powers.append(alpha_power)\n        \n        alpha_powers.append(np.mean(channel_powers))\n    \n    alpha_powers = np.array(alpha_powers)\n    \n    # 4. Find channel with highest alpha power\n    max_alpha_channel = np.argmax(alpha_powers)\n    \n    print(f\"✅ Generated EEG data shape: {eeg_data.shape}\")\n    print(f\"✅ Alpha powers computed for {n_channels} channels\")\n    print(f\"✅ Channel {max_alpha_channel} has highest alpha power: {alpha_powers[max_alpha_channel]:.2f}\")\n    print(f\"✅ Average alpha power across channels: {np.mean(alpha_powers):.2f} ± {np.std(alpha_powers):.2f}\")\n    \n    return eeg_data, alpha_powers\n\ndef challenge_2():\n    \"\"\"Challenge: Error debugging\"\"\"\n    \n    print(\"\\n🥈 CHALLENGE 2: Debug the Buggy Code\")\n    print(\"-\" * 40)\n    print(\"The following code has several bugs. Can you spot and fix them?\")\n    \n    buggy_code = '''\n# Buggy EEG analysis code\ndef analyze_eeg_buggy(eeg_data):\n    # Bug 1: Wrong axis for time averaging\n    time_avg = np.mean(eeg_data, axis=0)  # Should be axis=1\n    \n    # Bug 2: Division by zero potential\n    normalized = eeg_data / np.std(eeg_data, axis=1)  # Missing keepdims=True\n    \n    # Bug 3: Wrong indexing\n    channels = [0, 1, 2]\n    trials = [5, 10, 15, 20]\n    subset = eeg_data[channels, :, trials]  # Broadcasting error\n    \n    # Bug 4: Inefficient memory usage\n    result1 = eeg_data ** 2\n    result2 = np.sqrt(result1)\n    result3 = result1 + result2\n    final = np.mean(result3)\n    \n    return final\n'''\n    \n    print(\"🐛 Buggy Code:\")\n    print(buggy_code)\n    \n    print(\"\\n🔧 CORRECTED VERSION:\")\n    \n    def analyze_eeg_fixed(eeg_data):\n        \"\"\"Fixed version of the EEG analysis\"\"\"\n        \n        # Fix 1: Correct axis for time averaging (channels × time × trials)\n        time_avg = np.mean(eeg_data, axis=1)  # Average over time (axis=1)\n        \n        # Fix 2: Add keepdims to prevent broadcasting issues\n        normalized = eeg_data / np.std(eeg_data, axis=1, keepdims=True)\n        \n        # Fix 3: Use np.ix_ for proper advanced indexing\n        channels = [0, 1, 2]\n        trials = [5, 10, 15, 20]\n        subset = eeg_data[np.ix_(channels, range(eeg_data.shape[1]), trials)]\n        \n        # Fix 4: Chain operations to save memory\n        final = np.mean(eeg_data**2 + np.sqrt(eeg_data**2))\n        \n        return final, time_avg, normalized, subset\n    \n    # Test with sample data\n    test_data = np.random.randn(32, 1000, 50) * 50\n    \n    try:\n        result, time_avg, normalized, subset = analyze_eeg_fixed(test_data)\n        print(\"✅ Fixed code runs successfully!\")\n        print(f\"   Final result: {result:.3f}\")\n        print(f\"   Time average shape: {time_avg.shape}\")\n        print(f\"   Subset shape: {subset.shape}\")\n        print(f\"   Normalized data std: {np.std(normalized):.3f}\")\n    except Exception as e:\n        print(f\"❌ Error in fixed code: {e}\")\n\ndef challenge_3():\n    \"\"\"Challenge: Performance optimization\"\"\"\n    \n    print(\"\\n🥉 CHALLENGE 3: Optimize This Function\")\n    print(\"-\" * 40)\n    print(\"Make this slow function faster:\")\n    \n    def slow_correlation_analysis(eeg_data):\n        \"\"\"Slow version - compute correlation between all channel pairs\"\"\"\n        n_channels = eeg_data.shape[0]\n        correlations = np.zeros((n_channels, n_channels))\n        \n        for i in range(n_channels):\n            for j in range(n_channels):\n                # Compute correlation coefficient manually (slow!)\n                mean_i = np.mean(eeg_data[i])\n                mean_j = np.mean(eeg_data[j])\n                \n                numerator = np.sum((eeg_data[i] - mean_i) * (eeg_data[j] - mean_j))\n                denom_i = np.sqrt(np.sum((eeg_data[i] - mean_i)**2))\n                denom_j = np.sqrt(np.sum((eeg_data[j] - mean_j)**2))\n                \n                correlations[i, j] = numerator / (denom_i * denom_j)\n        \n        return correlations\n    \n    def fast_correlation_analysis(eeg_data):\n        \"\"\"Optimized version using NumPy's built-in function\"\"\"\n        # One line vs nested loops!\n        return np.corrcoef(eeg_data)\n    \n    # Performance comparison\n    test_data = np.random.randn(16, 2500) * 50  # Smaller for demo\n    \n    print(\"🐌 Timing slow version...\")\n    start_time = time.time()\n    slow_result = slow_correlation_analysis(test_data)\n    slow_time = time.time() - start_time\n    \n    print(\"⚡ Timing fast version...\")\n    start_time = time.time()\n    fast_result = fast_correlation_analysis(test_data)\n    fast_time = time.time() - start_time\n    \n    print(f\"\\n📊 Performance Results:\")\n    print(f\"   Slow version: {slow_time:.4f} seconds\")\n    print(f\"   Fast version: {fast_time:.4f} seconds\")\n    print(f\"   Speedup: {slow_time/fast_time:.1f}x faster!\")\n    print(f\"   Results match: {np.allclose(slow_result, fast_result)}\")\n\nprint(\"📝 SELF-ASSESSMENT CHECKLIST\")\nprint(\"=\" * 30)\n\nchecklist_items = [\n    \"✅ I can create and manipulate multi-dimensional NumPy arrays\",\n    \"✅ I understand array indexing and slicing for neuroscience data\",\n    \"✅ I can use vectorized operations instead of loops\",\n    \"✅ I know how to debug common array shape and indexing errors\",\n    \"✅ I can compute basic statistics along different axes\",\n    \"✅ I understand the importance of performance optimization\",\n    \"✅ I can validate and troubleshoot EEG data\",\n    \"✅ I'm comfortable with linear algebra operations in NumPy\"\n]\n\nprint(\"\\nCheck off the items you feel confident about:\")\nfor item in checklist_items:\n    print(f\"   {item}\")\n\nprint(\"\\n💡 If you checked less than 6 items, review the corresponding sections!\")\n\n# Run the challenges\nchallenge_1()\nchallenge_2() \nchallenge_3()\n\nprint(\"\\n\\n🎓 CONGRATULATIONS!\")\nprint(\"You've completed the Python & NumPy fundamentals for neuroscience!\")\nprint(\"🚀 You're ready to move on to machine learning concepts!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Event-Related Potential (ERP) Analysis 🎯\n",
    "\n",
    "**Task**: Analyze event-related potentials by extracting epochs around stimulus events and computing averaged responses.\n",
    "\n",
    "**Background**: In EEG experiments, we often want to see how the brain responds to specific stimuli (like seeing a face or hearing a tone). We extract small time windows around each stimulus and average across trials to reveal the typical brain response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise 1: Event-Related Potential Analysis ===\n",
    "print(\"=== Exercise 1: ERP Analysis ===\")\n",
    "\n",
    "# Simulate continuous EEG data\n",
    "sampling_rate = 250  # Hz\n",
    "duration = 120  # seconds (2 minutes)\n",
    "n_channels = 4\n",
    "channel_names = ['Fz', 'Cz', 'Pz', 'Oz']\n",
    "\n",
    "# Generate continuous EEG\n",
    "np.random.seed(123)\n",
    "time_continuous = np.linspace(0, duration, int(sampling_rate * duration))\n",
    "continuous_eeg = np.random.randn(n_channels, len(time_continuous)) * 2\n",
    "\n",
    "# Add some background alpha rhythm\n",
    "for ch in range(n_channels):\n",
    "    continuous_eeg[ch] += 3 * np.sin(2 * np.pi * 10 * time_continuous + ch * 0.5)\n",
    "\n",
    "# Generate stimulus events (random times)\n",
    "n_events = 50\n",
    "event_times = np.sort(np.random.uniform(5, duration-5, n_events))  # Avoid edges\n",
    "event_samples = (event_times * sampling_rate).astype(int)\n",
    "\n",
    "# Add stimulus-locked responses to the data\n",
    "for event_sample in event_samples:\n",
    "    # Create a stereotypical ERP response\n",
    "    erp_time = np.linspace(0, 1, 250)  # 1 second response\n",
    "    \n",
    "    # N100 component (negative deflection at 100ms)\n",
    "    n100 = -5 * np.exp(-((erp_time - 0.1)**2) / (2 * 0.02**2))\n",
    "    \n",
    "    # P300 component (positive deflection at 300ms)\n",
    "    p300 = 8 * np.exp(-((erp_time - 0.3)**2) / (2 * 0.05**2))\n",
    "    \n",
    "    erp_response = n100 + p300\n",
    "    \n",
    "    # Add to each channel (with some variability)\n",
    "    for ch in range(n_channels):\n",
    "        if event_sample + 250 < continuous_eeg.shape[1]:  # Make sure we don't go out of bounds\n",
    "            # Different channels have different sensitivities\n",
    "            sensitivity = [0.5, 1.0, 1.2, 0.8][ch]  # Cz and Pz are most sensitive\n",
    "            continuous_eeg[ch, event_sample:event_sample+250] += sensitivity * erp_response\n",
    "\n",
    "print(f\"Continuous EEG shape: {continuous_eeg.shape}\")\n",
    "print(f\"Number of events: {n_events}\")\n",
    "print(f\"Event times (first 10): {event_times[:10]}\")\n",
    "\n",
    "# YOUR TASK: Complete the following functions\n",
    "\n",
    "def extract_epochs(continuous_data, event_samples, pre_stim=0.2, post_stim=0.8, sampling_rate=250):\n",
    "    \"\"\"\n",
    "    Extract epochs around stimulus events.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    continuous_data : array, shape (n_channels, n_timepoints)\n",
    "        Continuous EEG data\n",
    "    event_samples : array\n",
    "        Sample indices of stimulus events\n",
    "    pre_stim : float\n",
    "        Time before stimulus (seconds)\n",
    "    post_stim : float\n",
    "        Time after stimulus (seconds)\n",
    "    sampling_rate : int\n",
    "        Sampling frequency\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs : array, shape (n_channels, n_timepoints_epoch, n_events)\n",
    "        Extracted epochs\n",
    "    epoch_times : array\n",
    "        Time axis for epochs (relative to stimulus)\n",
    "    \"\"\"\n",
    "    # TODO: Implement epoch extraction\n",
    "    pre_samples = int(pre_stim * sampling_rate)\n",
    "    post_samples = int(post_stim * sampling_rate)\n",
    "    epoch_length = pre_samples + post_samples\n",
    "    \n",
    "    n_channels = continuous_data.shape[0]\n",
    "    valid_events = []\n",
    "    epochs_list = []\n",
    "    \n",
    "    for event_sample in event_samples:\n",
    "        start_sample = event_sample - pre_samples\n",
    "        end_sample = event_sample + post_samples\n",
    "        \n",
    "        # Check if epoch is within data bounds\n",
    "        if start_sample >= 0 and end_sample < continuous_data.shape[1]:\n",
    "            epoch = continuous_data[:, start_sample:end_sample]\n",
    "            epochs_list.append(epoch)\n",
    "            valid_events.append(event_sample)\n",
    "    \n",
    "    epochs = np.stack(epochs_list, axis=2)\n",
    "    epoch_times = np.linspace(-pre_stim, post_stim, epoch_length)\n",
    "    \n",
    "    return epochs, epoch_times\n",
    "\n",
    "def baseline_correct_epochs(epochs, baseline_period, epoch_times):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "    baseline_period : tuple\n",
    "        (start_time, end_time) for baseline period\n",
    "    epoch_times : array\n",
    "        Time axis for epochs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_corrected : array\n",
    "        Baseline-corrected epochs\n",
    "    \"\"\"\n",
    "    # TODO: Implement baseline correction\n",
    "    baseline_mask = (epoch_times >= baseline_period[0]) & (epoch_times <= baseline_period[1])\n",
    "    baseline_mean = np.mean(epochs[:, baseline_mask, :], axis=1, keepdims=True)\n",
    "    epochs_corrected = epochs - baseline_mean\n",
    "    \n",
    "    return epochs_corrected\n",
    "\n",
    "def compute_erp(epochs):\n",
    "    \"\"\"\n",
    "    Compute event-related potential by averaging across trials.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : array, shape (n_channels, n_timepoints, n_events)\n",
    "        Epoch data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    erp : array, shape (n_channels, n_timepoints)\n",
    "        Averaged ERP\n",
    "    erp_std : array, shape (n_channels, n_timepoints)\n",
    "        Standard deviation across trials\n",
    "    \"\"\"\n",
    "    # TODO: Implement ERP computation\n",
    "    erp = np.mean(epochs, axis=2)\n",
    "    erp_std = np.std(epochs, axis=2)\n",
    "    \n",
    "    return erp, erp_std\n",
    "\n",
    "# Test your implementation\n",
    "epochs, epoch_times = extract_epochs(continuous_eeg, event_samples, pre_stim=0.2, post_stim=0.8)\n",
    "epochs_corrected = baseline_correct_epochs(epochs, baseline_period=(-0.2, 0), epoch_times=epoch_times)\n",
    "erp, erp_std = compute_erp(epochs_corrected)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Epochs shape: {epochs.shape}\")\n",
    "print(f\"ERP shape: {erp.shape}\")\n",
    "print(f\"Epoch time range: {epoch_times[0]:.3f} to {epoch_times[-1]:.3f} seconds\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    plt.subplot(2, 2, ch + 1)\n",
    "    plt.plot(epoch_times, erp[ch], 'b-', linewidth=2, label='ERP')\n",
    "    plt.fill_between(epoch_times, erp[ch] - erp_std[ch], erp[ch] + erp_std[ch], \n",
    "                     alpha=0.3, color='blue', label='±1 SD')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7, label='Stimulus')\n",
    "    plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude (μV)')\n",
    "    plt.title(f'ERP - Channel {channel_names[ch]}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if ch == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis questions for you to think about:\n",
    "print(\"\\n=== Analysis Questions ===\")\n",
    "print(\"1. Which channel shows the strongest P300 response?\")\n",
    "print(\"2. At what time does the N100 component peak?\")\n",
    "print(\"3. How does the signal-to-noise ratio compare across channels?\")\n",
    "\n",
    "# Find P300 peak\n",
    "p300_window = (epoch_times >= 0.2) & (epoch_times <= 0.4)\n",
    "p300_peaks = np.argmax(erp[:, p300_window], axis=1)\n",
    "p300_peak_times = epoch_times[p300_window][p300_peaks]\n",
    "\n",
    "print(f\"\\nP300 peak times by channel:\")\n",
    "for ch in range(n_channels):\n",
    "    print(f\"  {channel_names[ch]}: {p300_peak_times[ch]:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Spectral Analysis and Connectivity 🌊\n",
    "\n",
    "**Task**: Analyze the frequency content of neural signals and compute connectivity between brain regions.\n",
    "\n",
    "**Background**: The brain operates at different frequency bands (delta, theta, alpha, beta, gamma). Understanding how these frequencies change and how they're synchronized between brain regions gives us insights into brain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Exercise 2: Spectral Analysis and Connectivity ===\nprint(\"=== Exercise 2: Spectral Analysis and Connectivity ===\")\n\n# Generate sample data with known connectivity\nnp.random.seed(456)\nsampling_rate = 500  # Hz\nduration = 60  # seconds\nn_channels = 6\nchannel_names = ['F3', 'F4', 'C3', 'C4', 'P3', 'P4']\n\ntime_axis = np.linspace(0, duration, int(sampling_rate * duration))\nn_timepoints = len(time_axis)\n\n# Create signals with known connectivity patterns\nsignals = np.zeros((n_channels, n_timepoints))\n\n# Generate base oscillations\nalpha_source = np.sin(2 * np.pi * 10 * time_axis)  # 10 Hz alpha\nbeta_source = np.sin(2 * np.pi * 20 * time_axis)   # 20 Hz beta\ngamma_source = np.sin(2 * np.pi * 40 * time_axis)  # 40 Hz gamma\n\n# Add noise and connectivity patterns\nfor ch in range(n_channels):\n    # Base noise\n    signals[ch] = 0.5 * np.random.randn(n_timepoints)\n    \n    # Add frequency-specific connectivity\n    if 'F' in channel_names[ch]:  # Frontal channels\n        signals[ch] += 2 * beta_source + 0.5 * np.random.randn(n_timepoints)\n    elif 'C' in channel_names[ch]:  # Central channels\n        signals[ch] += 1.5 * alpha_source + 1 * beta_source + 0.5 * np.random.randn(n_timepoints)\n    elif 'P' in channel_names[ch]:  # Posterior channels\n        signals[ch] += 3 * alpha_source + 0.5 * np.random.randn(n_timepoints)\n    \n    # Add some gamma coupling\n    if ch % 2 == 0:  # Left hemisphere\n        signals[ch] += 0.8 * gamma_source\n    else:  # Right hemisphere\n        # Delayed gamma (simulate inter-hemispheric delay)\n        delay_samples = int(0.01 * sampling_rate)  # 10ms delay\n        delayed_gamma = np.zeros_like(gamma_source)\n        delayed_gamma[delay_samples:] = gamma_source[:-delay_samples]\n        signals[ch] += 0.8 * delayed_gamma\n\nprint(f\"Generated signals shape: {signals.shape}\")\nprint(f\"Duration: {duration} seconds\")\nprint(f\"Sampling rate: {sampling_rate} Hz\")\n\n# YOUR TASK: Implement the following functions\n\ndef compute_power_spectrum(data, sampling_rate, nperseg=None):\n    \"\"\"\n    Compute power spectral density for multi-channel data.\n    \n    Parameters:\n    -----------\n    data : array, shape (n_channels, n_timepoints)\n        Multi-channel signal data\n    sampling_rate : int\n        Sampling frequency\n    nperseg : int, optional\n        Length of each segment for Welch's method\n        \n    Returns:\n    --------\n    frequencies : array\n        Frequency values\n    psd : array, shape (n_channels, n_frequencies)\n        Power spectral density for each channel\n    \"\"\"\n    # TODO: Implement power spectrum computation\n    if nperseg is None:\n        nperseg = sampling_rate * 4  # 4 second windows\n    \n    n_channels = data.shape[0]\n    \n    # Compute PSD for first channel to get frequency array\n    frequencies, psd_ch = signal.welch(data[0], sampling_rate, nperseg=nperseg)\n    \n    # Initialize PSD array\n    psd = np.zeros((n_channels, len(frequencies)))\n    \n    # Compute PSD for each channel\n    for ch in range(n_channels):\n        frequencies, psd[ch] = signal.welch(data[ch], sampling_rate, nperseg=nperseg)\n    \n    return frequencies, psd\n\ndef compute_band_power(psd, frequencies, freq_bands):\n    \"\"\"\n    Compute power in specific frequency bands.\n    \n    Parameters:\n    -----------\n    psd : array, shape (n_channels, n_frequencies)\n        Power spectral density\n    frequencies : array\n        Frequency values\n    freq_bands : dict\n        Dictionary of frequency bands {band_name: (low_freq, high_freq)}\n        \n    Returns:\n    --------\n    band_powers : dict\n        Dictionary of band powers {band_name: array of powers per channel}\n    \"\"\"\n    # TODO: Implement band power computation\n    band_powers = {}\n    \n    for band_name, (low_freq, high_freq) in freq_bands.items():\n        # Create frequency mask\n        freq_mask = (frequencies >= low_freq) & (frequencies <= high_freq)\n        \n        # Compute power in band for each channel\n        band_power = np.zeros(psd.shape[0])\n        for ch in range(psd.shape[0]):\n            band_power[ch] = np.trapz(psd[ch, freq_mask], frequencies[freq_mask])\n        \n        band_powers[band_name] = band_power\n    \n    return band_powers\n\ndef compute_coherence(data, sampling_rate, nperseg=None):\n    \"\"\"\n    Compute coherence between all pairs of channels.\n    \n    Parameters:\n    -----------\n    data : array, shape (n_channels, n_timepoints)\n        Multi-channel signal data\n    sampling_rate : int\n        Sampling frequency\n    nperseg : int, optional\n        Length of each segment\n        \n    Returns:\n    --------\n    frequencies : array\n        Frequency values\n    coherence : array, shape (n_channels, n_channels, n_frequencies)\n        Coherence matrix\n    \"\"\"\n    # TODO: Implement coherence computation\n    if nperseg is None:\n        nperseg = sampling_rate * 4\n    \n    n_channels = data.shape[0]\n    \n    # Compute coherence for first pair to get frequency array\n    frequencies, coh_temp = signal.coherence(data[0], data[1], sampling_rate, nperseg=nperseg)\n    \n    # Initialize coherence array\n    coherence = np.zeros((n_channels, n_channels, len(frequencies)))\n    \n    # Compute coherence for all pairs\n    for i in range(n_channels):\n        for j in range(n_channels):\n            if i == j:\n                coherence[i, j, :] = 1.0  # Perfect coherence with self\n            else:\n                frequencies, coherence[i, j, :] = signal.coherence(data[i], data[j], sampling_rate, nperseg=nperseg)\n    \n    return frequencies, coherence\n\n# Test your implementation\nfrequencies, psd = compute_power_spectrum(signals, sampling_rate)\n\nfreq_bands = {\n    'delta': (1, 4),\n    'theta': (4, 8),\n    'alpha': (8, 13),\n    'beta': (13, 30),\n    'gamma': (30, 80)\n}\n\nband_powers = compute_band_power(psd, frequencies, freq_bands)\ncoh_frequencies, coherence = compute_coherence(signals, sampling_rate)\n\nprint(f\"\\nResults:\")\nprint(f\"PSD shape: {psd.shape}\")\nprint(f\"Frequency range: {frequencies[0]:.1f} to {frequencies[-1]:.1f} Hz\")\nprint(f\"Coherence shape: {coherence.shape}\")\n\n# Plot results\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Power spectra\nfor ch in range(n_channels):\n    axes[0, 0].semilogy(frequencies, psd[ch], label=channel_names[ch], alpha=0.8)\naxes[0, 0].set_xlim(0, 60)\naxes[0, 0].set_xlabel('Frequency (Hz)')\naxes[0, 0].set_ylabel('PSD (μV²/Hz)')\naxes[0, 0].set_title('Power Spectral Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Band powers\nband_names = list(freq_bands.keys())\nx_pos = np.arange(len(band_names))\nbar_width = 0.1\n\nfor ch in range(n_channels):\n    powers = [band_powers[band][ch] for band in band_names]\n    axes[0, 1].bar(x_pos + ch * bar_width, powers, bar_width, \n                   label=channel_names[ch], alpha=0.8)\n\naxes[0, 1].set_xlabel('Frequency Band')\naxes[0, 1].set_ylabel('Power (μV²)')\naxes[0, 1].set_title('Band Powers')\naxes[0, 1].set_xticks(x_pos + bar_width * 2.5)\naxes[0, 1].set_xticklabels(band_names)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Coherence heatmap (average across frequencies)\navg_coherence = np.mean(coherence, axis=2)\nim = axes[0, 2].imshow(avg_coherence, cmap='viridis', vmin=0, vmax=1)\naxes[0, 2].set_title('Average Coherence Matrix')\naxes[0, 2].set_xticks(range(n_channels))\naxes[0, 2].set_yticks(range(n_channels))\naxes[0, 2].set_xticklabels(channel_names)\naxes[0, 2].set_yticklabels(channel_names)\nplt.colorbar(im, ax=axes[0, 2])\n\n# 4. Alpha coherence network\nalpha_idx = (coh_frequencies >= 8) & (coh_frequencies <= 13)\nalpha_coherence = np.mean(coherence[:, :, alpha_idx], axis=2)\nim2 = axes[1, 0].imshow(alpha_coherence, cmap='viridis', vmin=0, vmax=1)\naxes[1, 0].set_title('Alpha Band Coherence (8-13 Hz)')\naxes[1, 0].set_xticks(range(n_channels))\naxes[1, 0].set_yticks(range(n_channels))\naxes[1, 0].set_xticklabels(channel_names)\naxes[1, 0].set_yticklabels(channel_names)\nplt.colorbar(im2, ax=axes[1, 0])\n\n# 5. Beta coherence network\nbeta_idx = (coh_frequencies >= 13) & (coh_frequencies <= 30)\nbeta_coherence = np.mean(coherence[:, :, beta_idx], axis=2)\nim3 = axes[1, 1].imshow(beta_coherence, cmap='viridis', vmin=0, vmax=1)\naxes[1, 1].set_title('Beta Band Coherence (13-30 Hz)')\naxes[1, 1].set_xticks(range(n_channels))\naxes[1, 1].set_yticks(range(n_channels))\naxes[1, 1].set_xticklabels(channel_names)\naxes[1, 1].set_yticklabels(channel_names)\nplt.colorbar(im3, ax=axes[1, 1])\n\n# 6. Coherence spectrum between F3 and F4\nf3_idx = channel_names.index('F3')\nf4_idx = channel_names.index('F4')\naxes[1, 2].plot(coh_frequencies, coherence[f3_idx, f4_idx, :], 'b-', linewidth=2)\naxes[1, 2].set_xlim(0, 60)\naxes[1, 2].set_ylim(0, 1)\naxes[1, 2].set_xlabel('Frequency (Hz)')\naxes[1, 2].set_ylabel('Coherence')\naxes[1, 2].set_title('F3-F4 Coherence Spectrum')\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Analysis summary\nprint(\"\\n=== Analysis Summary ===\")\nprint(\"Band power analysis:\")\nfor band in band_names:\n    max_channel = np.argmax(band_powers[band])\n    print(f\"  {band}: strongest in {channel_names[max_channel]} ({band_powers[band][max_channel]:.3f} μV²)\")\n\nprint(\"\\nCoherence analysis:\")\n# Find strongest coherence pairs (excluding self-connections)\ncoherence_no_diag = avg_coherence.copy()\nnp.fill_diagonal(coherence_no_diag, 0)\nmax_coherence_idx = np.unravel_index(np.argmax(coherence_no_diag), coherence_no_diag.shape)\nprint(f\"  Strongest coherence: {channel_names[max_coherence_idx[0]]}-{channel_names[max_coherence_idx[1]]} ({coherence_no_diag[max_coherence_idx]:.3f})\")\n\n# Interhemispheric coherence\nleft_channels = [i for i, ch in enumerate(channel_names) if '3' in ch]\nright_channels = [i for i, ch in enumerate(channel_names) if '4' in ch]\ninterhemispheric_coherence = []\n\nfor l_ch in left_channels:\n    for r_ch in right_channels:\n        if channel_names[l_ch][0] == channel_names[r_ch][0]:  # Same region\n            coh_val = avg_coherence[l_ch, r_ch]\n            interhemispheric_coherence.append(coh_val)\n            print(f\"  {channel_names[l_ch]}-{channel_names[r_ch]} coherence: {coh_val:.3f}\")\n\nprint(f\"  Average interhemispheric coherence: {np.mean(interhemispheric_coherence):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Key Takeaways: What You've Learned\n",
    "\n",
    "Congratulations! You've just completed a comprehensive review of Python and NumPy fundamentals for neuroscience. Here's what you've mastered:\n",
    "\n",
    "## 🐍 Python Skills\n",
    "- **List comprehensions** for efficient data processing\n",
    "- **Functions** for building reusable analysis tools\n",
    "- **Classes** for organizing complex analysis pipelines\n",
    "- **Best practices** for scientific computing\n",
    "\n",
    "## 🔢 NumPy Mastery\n",
    "- **Array creation and manipulation** for multi-dimensional brain data\n",
    "- **Indexing and slicing** to extract specific channels, time windows, and trials\n",
    "- **Vectorized operations** for lightning-fast computations\n",
    "- **Mathematical operations** essential for signal processing\n",
    "- **Linear algebra** foundations for neural networks\n",
    "\n",
    "## 🧠 Neuroscience Applications\n",
    "- **EEG data processing** with realistic multi-channel signals\n",
    "- **Event-related potential (ERP) analysis** for studying brain responses\n",
    "- **Spectral analysis** to understand frequency content\n",
    "- **Connectivity analysis** using coherence measures\n",
    "- **Statistical analysis** of neural data\n",
    "\n",
    "## 💡 Performance Tips You've Learned\n",
    "\n",
    "1. **Always use vectorized operations** instead of Python loops\n",
    "2. **Specify axis parameters** carefully when working with multi-dimensional data\n",
    "3. **Use keepdims=True** when you need to preserve array dimensions\n",
    "4. **Leverage NumPy's broadcasting** for efficient operations\n",
    "5. **Choose appropriate data types** to save memory\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What's Next?\n",
    "\n",
    "In the next notebook (`02_ml_foundations.ipynb`), you'll build on these fundamentals to explore:\n",
    "\n",
    "- **Machine learning concepts** relevant to neuroscience\n",
    "- **Classification and regression** for brain state detection\n",
    "- **Cross-validation** and model evaluation\n",
    "- **Feature engineering** for neural signals\n",
    "- **Preparing for deep learning** with PyTorch\n",
    "\n",
    "You're well-equipped for this journey! The NumPy skills you've practiced here will be your foundation for everything that comes next.\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue your deep learning journey? See you in the next notebook! 🧠⚡**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}